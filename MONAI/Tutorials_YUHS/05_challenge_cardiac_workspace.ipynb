{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "\n",
    "# Segmentation Exercise  [workspace]\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>\n",
    "\n",
    "\n",
    "In this exercise we will segment the left ventricle of the heart in relatively small images using neural networks. \n",
    "Below is the code for setting up a segmentation network and training it. The network isn't very good, **so the exercise is to improve the quality of the segmentation by improving the network and/or the training scheme including data loading efficiency and data augmentation**. \n",
    "\n",
    "The data being used here is derived from the [Sunnybrook Cardiac Dataset](https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/) of cardiac MR images, filtered to contain only left ventricular myocardium segmentations and reduced in the XY dimensions.\n",
    "\n",
    "<img src=\"https://www.cardiacatlas.org/wp-content/uploads/2015/09/scd-mri.png\" width=400>\n",
    "\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "### 1. Improve Data Pipeline\n",
    "\n",
    "The pipeline currently has a very basic set of transforms. We'd want to add new transforms which will add regularization to our training process, specifically modifying the image and segmentations to make the learning problem a little harder. \n",
    "\n",
    "The dataset being used is `ArrayDataset` but we have in MONAI `NPZDictItemDataset` for loading data from Numpy's NPZ file format. Change the code to use this class instead. You'll need a different way of getting `caseIndices` and splitting the dataset using it.\n",
    "\n",
    "### 2. Improve/Replace Network\n",
    "\n",
    "As you can see we're not getting good results from our network. The training loss values are jumping around and not decreasing much anymore. The validation score has topped out at 0.25, which is really poor. \n",
    "\n",
    "It's now up to you to improve the results of our segmentation task. The things to consider changing include the network itself, how data is loaded, how batches might be composed, and what transforms we want to use from MONAI. \n",
    "\n",
    "### 3. Replace The Training Loop\n",
    "\n",
    "This notebook uses a simple training loop with validation done explicitly. Replace this with a use of the `SupervisedTrainer` class and `SupervisedEvaluator` to do the evaluation throughout the training process. The graph plotting is done simply by recording values at each iteration through the loop, you'll want to use some other mechanism to do the same thing such as using a `MetricLogger` handler object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU Support\n",
    "\n",
    "Running  `!nvidia-smi` in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
    "\n",
    "if GPU Memory Usage is no `0 MiB` shutdown all kernels and restart current kernel.\n",
    "- step1. shutdown kernel with following <b>Menu</b> > <b>Kernel</b> > <b>Shut Down All kernels </b>\n",
    "- step2. restart kernelw with following <b>Menu</b> > <b>Kernel</b> > <b>Restart Kernel</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "2783c3a4-855a-4b2b-86c0-297b7b31b9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  7 14:24:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                   On |\n",
      "| N/A   35C    P0    61W / 400W |     39MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    3   0   0  |     13MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      0MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX4DkxZygKNP"
   },
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNnKTuzBgKZm",
    "outputId": "8b7c3607-768e-4222-d4c0-82806f5a1264"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import monai\n",
    "from monai.transforms import Compose, AddChannel, ScaleIntensity, ToTensor\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.data import ArrayDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.utils import first, progress_bar\n",
    "from monai.networks import one_hot\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "set_determinism(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "directory = \"temp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNZjqg3Lq-1"
   },
   "source": [
    "## download dataset \n",
    "\n",
    "We will use preprocessed Sunnybrook Cardiac MRI dataset (2MB) at  [VPH Summer SChool 2019](https://www.vph-institute.org/events/2019-vph-summer-school.html)\n",
    "\n",
    "You can check for further research\n",
    "- Sunnybrook Cardiac MRI dataset(30MB) in [kaggle](https://www.kaggle.com/datasets/salikhussaini49/sunnybrook-cardiac-mri)\n",
    "- Sunnybrook Cardiac Data DICOM file(2.6GB) at [Cardiac Atlas Project](https://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "DATA_NPZ = \"https://github.com/ericspod/VPHSummerSchool2019/raw/master/scd_lvsegs.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data from the remote source and visualize a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 ms, sys: 4.66 ms, total: 39.2 ms\n",
      "Wall time: 744 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remote_file = urlopen(DATA_NPZ)\n",
    "npz = BytesIO(remote_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 64, 64) (420, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f63482a7910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load(npz)  # load all the data from the archive\n",
    "\n",
    "images = data[\"images\"]  # images in BHW array order\n",
    "segs = data[\"segs\"]  # segmentations in BHW array order\n",
    "case_indices = data[\"caseIndices\"]  # the indices in `images` for each case\n",
    "\n",
    "images = images.astype(np.float32) / images.max()  # normalize images\n",
    "\n",
    "print(images.shape, segs.shape)\n",
    "plt.imshow(images[13] + segs[13] * 0.25, cmap=\"gray\")  # show image 13 with segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our data into a training and validation set by keeping the last 6 cases as the latter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline [start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "valid_index = case_indices[-6, 0]  # keep the last 6 cases for testing\n",
    "\n",
    "# divide the images, segmentations, and categories into train/test sets\n",
    "train_images, train_segs = images[:valid_index], segs[:valid_index]\n",
    "valid_images, valid_segs = images[valid_index:], segs[valid_index:]\n",
    "\n",
    "batch_size = 50\n",
    "num_workers = 2\n",
    "\n",
    "image_trans = Compose(\n",
    "    [\n",
    "        ScaleIntensity(),  # rescale image data to range [0,1]\n",
    "        AddChannel(),  # add 1-size channel dimension\n",
    "        ToTensor(),  # convert to tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "seg_trans = Compose([AddChannel(), ToTensor()])\n",
    "\n",
    "ds = ArrayDataset(train_images, image_trans, train_segs, seg_trans)\n",
    "loader = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_ds = ArrayDataset(valid_images, image_trans, valid_segs, seg_trans)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "im, seg = first(loader)\n",
    "print(im.shape, im.min(), im.max(), seg.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a MONAI data loading object to compose batches during training, and another for validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline [end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Improve Data Pipeline\n",
    "\n",
    "The pipeline currently has a very basic set of transforms. We'd want to add new transforms which will add regularization to our training process, specifically modifying the image and segmentations to make the learning problem a little harder. \n",
    "\n",
    "The dataset being used is `ArrayDataset` but we have in MONAI `NPZDictItemDataset` for loading data from Numpy's NPZ file format. Change the code to use this class instead. You'll need a different way of getting `caseIndices` and splitting the dataset using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imports\n",
    "from monai.data import  NPZDictItemDataset #TODO  #ArrayDataset  to NPZDictItemDataset\n",
    "\n",
    "############\n",
    "batch_size = 6\n",
    "num_workers = 10\n",
    "aug_prob = 0.5\n",
    "############\n",
    "from monai.transforms import (\n",
    "    Activationsd, \n",
    "    AsDiscreted,\n",
    "    AddChanneld,\n",
    "    ScaleIntensityd,\n",
    "    CastToTyped,\n",
    "    EnsureTyped,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    Rand2DElasticd,\n",
    "    RandAffined,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline, We split our data into a training and validation set by keeping the last 6 cases as the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# use these when interpolating binary segmentations to ensure values are 0 or 1 only\n",
    "zoom_mode = monai.utils.enums.InterpolateMode.NEAREST\n",
    "elast_mode = monai.utils.enums.GridSampleMode.BILINEAR, monai.utils.enums.GridSampleMode.NEAREST\n",
    "\n",
    "#########\n",
    "# for keys \n",
    "from monai.utils.enums import CommonKeys\n",
    "both_keys = (CommonKeys.IMAGE, CommonKeys.LABEL) #DONE\n",
    "image_only = CommonKeys.IMAGE #DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans = Compose(\n",
    "    [\n",
    "        CastToTyped(both_keys, (np.float32, np.int32)),\n",
    "        ScaleIntensityd(image_only),\n",
    "        AddChanneld(both_keys),\n",
    "        RandRotate90d(keys=both_keys, prob=aug_prob),\n",
    "        RandFlipd(keys=both_keys, prob=aug_prob),\n",
    "        #RandZoomd(keys=both_keys, prob=aug_prob, mode=zoom_mode),\n",
    "        Rand2DElasticd(keys=both_keys, prob=aug_prob, spacing=10, magnitude_range=(-2, 2), mode=elast_mode),\n",
    "        #RandAffined(keys=both_keys, prob=aug_prob, rotate_range=1, translate_range=16, mode=elast_mode),        \n",
    "        EnsureTyped(both_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_trans = Compose(\n",
    "    [\n",
    "        CastToTyped(both_keys, (np.float32, np.int32)),\n",
    "        ScaleIntensityd(image_only),\n",
    "        AddChanneld(both_keys),        \n",
    "        EnsureTyped(both_keys),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a MONAI data loading object to compose batches during training, and another for validation: However, we need to replace `ArrayDataset` `ArrayDataset(train_images, image_trans, train_segs, seg_trans)` with `NPZDictItemDataset`\n",
    "\n",
    "check MONAI document for [NPZDictItemDataset](https://docs.monai.io/en/stable/data.html?highlight=NPZDictItemDataset#npzdictitemdataset)\n",
    "\n",
    "class monai.data.<b>NPZDictItemDataset</b>(`npzfile, keys, transform=None, other_keys=()`)\n",
    "\n",
    "Represents a dataset from a loaded NPZ file. The members of the file to load are named in the keys of keys and stored under the keyed name. All loaded arrays must have the same 0-dimension (batch) size. Items are always dicts mapping names to an item extracted from the loaded arrays. If passing slicing indices, will return a PyTorch Subset, for example: data: Subset = dataset[1:4], for more details, please check: https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "\n",
    "Parameters\n",
    "- <b>npzfile</b> (`Union[str, IO]`) â€“ Path to .npz file or stream containing .npz file data\n",
    "- <b>keys</b> (`Dict[str, str]`) â€“ Maps keys to load from file to name to store in dataset\n",
    "- <b>transform</b> (`Optional[Callable[â€¦, Dict[str, Any]]]`) â€“ Transform to apply to batch dict\n",
    "- <b>other_keys</b> (`Optional[Sequence[str]]`) â€“ secondary data to load from file and store in dict other_keys, not returned by __getitem__\n",
    "- <b>data</b> â€“ input data to load and transform to generate dataset for model.\n",
    "- <b>transform</b> â€“ a callable data transform on input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create training and validation datasets from the whole set of images, these will be resized below based on case indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for keys \n",
    "from monai.utils.enums import CommonKeys\n",
    "\n",
    "# create training and validation datasets from the whole set of images, these will be resized below based on case indices\n",
    "\n",
    "keys_train = {\"images\": CommonKeys.IMAGE, \"segs\": CommonKeys.LABEL} #TODO\n",
    "keys_val =  {\"images\": CommonKeys.IMAGE, \"segs\": CommonKeys.LABEL} # TODO\n",
    "\n",
    "train_dat = NPZDictItemDataset( npz, keys_train , train_trans  , (\"caseIndices\",) ) #TODO #ArrayDataset  to NPZDictItemDataset\n",
    "val_dat   = NPZDictItemDataset( npz, keys_val , val_trans  ) #TODO  #ArrayDataset  to NPZDictItemDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure  train_loader and val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# extract the case indices array\n",
    "case_indices = train_dat.other_keys[\"caseIndices\"] # TODO data[\"caseIndices\"]   to train_dat\n",
    "val_index = case_indices[-6, 0]  # keep the last 6 cases for testing\n",
    "\n",
    "\n",
    "train_dat = train_dat[:val_index]\n",
    "val_dat = val_dat[val_index:]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dat,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dat,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 52\n",
      "62 9\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dat), len(val_dat))\n",
    "print(len(train_loader), len(val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define out simple network. This doesn't do a good job so consider how to improve it by adding layers or other elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monai Network\n",
    "MONAI provides predefined networks. we can easily import it. \n",
    "\n",
    "- [Layers](https://docs.monai.io/en/stable/networks.html#layers) : Act, Conv, Norm, Dropout, Flatten, Reshape, Pad, Pool, SkipConnection\n",
    "- [Blocks](https://docs.monai.io/en/stable/networks.html#module-monai.networks.blocks) : ADN, Convolution, Synamic UnetBlock, FCN, GCN, Squeeze-andExcitation, ResNeXt, SABlock, Transformer Block, \n",
    "- [Nets](https://docs.monai.io/en/stable/networks.html#nets)  : DenseNet121, EfficientNet, SegResNet, ResNet, SENet154, DyUNet, UNet, AutoEncoder, VarAutoEncoder, ViT, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define out simple network. This doesn't do a good job so consider how to improve it by adding layers or other elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLine Model \n",
    "\n",
    "SegNet : \n",
    " - Input \n",
    " - Conv2D\n",
    " - MaxPool2D\n",
    " - Conv3D\n",
    " - ConvT2D\n",
    " - Conv2d\n",
    " - Output \n",
    "\n",
    "<p><img align='left' src=\"https://miro.medium.com/max/1400/1*nGFy96r63GwSE_EsJDLMDw.png\" width=600>  </p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # layer 1: convolution, normalization, downsampling\n",
    "            nn.Conv2d(1, 2, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1),\n",
    "            # layer 2\n",
    "            nn.Conv2d(2, 4, 3, 1, 1),\n",
    "            # layer 3\n",
    "            nn.ConvTranspose2d(4, 2, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            # layer 4: output\n",
    "            nn.Conv2d(2, 1, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "net = SegNet()\n",
    "net = net.to(device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "\n",
    "### 2. Improve/Replace Network\n",
    "\n",
    "As you can see we're not getting good results from our network. The training loss values are jumping around and not decreasing much anymore. The validation score has topped out at 0.25, which is really poor. \n",
    "\n",
    "It's now up to you to improve the results of our segmentation task. The things to consider changing include the network itself, how data is loaded, how batches might be composed, and what transforms we want to use from MONAI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model \n",
    "\n",
    "### UNet\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "U Shaped Network was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. \n",
    "It is Fully Convolutional Network Model for the segmentation task with two paths(encoder and decoder) with 1x1 convolution skip connection similar as residual. \n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\" width=800>\n",
    "\n",
    "#### UNet in monai.networks\n",
    "class monai.networks.nets.<b>UNet </b> (`spatial_dims, in_channels, out_channels, channels, strides, kernel_size=3, up_kernel_size=3, num_res_units=0, act='PRELU', norm='INSTANCE', dropout=0.0, bias=True, dimensions=None` )\n",
    "Enhanced version of <br>UNet</b> which has residual units implemented with the `ResidualUnit` class. The residual part uses a convolution to change the input dimensions to match the output dimensions if this is necessary but will use `nn.Identity` if not. Refer to: [Link](https://link.springer.com/chapter/10.1007/978-3-030-12029-0_40).\n",
    "\n",
    "Each layer of the network has a encode and decode path with a skip connection between them. Data in the encode path is downsampled using strided convolutions (if strides is given values greater than 1) and in the decode path upsampled using strided transpose convolutions. These down or up sampling operations occur at the beginning of each block rather than afterwards as is typical in <b>UNet</b> implementations.\n",
    "\n",
    "To further explain this consider the first example network given below. This network has 3 layers with strides of 2 for each of the middle layers (the last layer is the bottom connection which does not down/up sample). Input data to this network is immediately reduced in the spatial dimensions by a factor of 2 by the first convolution of the residual unit defining the first layer of the encode part. The last layer of the decode part will upsample its input (data from the previous layer concatenated with data from the skip connection) in the first convolution. this ensures the final output of the network has the same shape as the input.\n",
    "\n",
    "Padding values for the convolutions are chosen to ensure output sizes are even divisors/multiples of the input sizes if the strides value for a layer is a factor of the input sizes. A typical case is to use strides values of 2 and inputs that are multiples of powers of 2. An input can thus be downsampled evenly however many times its dimensions can be divided by 2, so for the example network inputs would have to have dimensions that are multiples of 4. In the second example network given below the input to the bottom layer will have shape `(1, 64, 15, 15)` for an input of shape `(1, 1, 240, 240)` demonstrating the input being reduced in size spatially by 2**4.\n",
    "\n",
    "##### Parameters\n",
    " - <b>spatial_dims </b>(`int`)  â€“ number of spatial dimensions.\n",
    " - <b>in_channels </b>(`int`) â€“ number of input channels.\n",
    " - <b>out_channels </b>(`int`) â€“ number of output channels.\n",
    " - <b>channels</b> (`Sequence[int]`) â€“ sequence of channels. Top block first. The length of channels should be no less than 2.\n",
    " - <b>strides</b> (`Sequence[int]`) â€“ sequence of convolution strides. The length of stride should equal to len(channels) - 1.\n",
    " - <b>kernel_size</b> (`Union[Sequence[int], int]`) â€“ convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>up_kernel_size</b> (`Union[Sequence[int], int]`) â€“ upsampling convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>num_res_units</b>  (`int`)â€“ number of residual units. Defaults to 0.\n",
    " - <b>act</b> (`Union[Tuple, str]`) â€“ activation type and arguments. Defaults to PReLU.\n",
    " - <b>norm</b> (`Union[Tuple, str]`) â€“ feature normalization type and arguments. Defaults to instance norm.\n",
    " - <b>dropout</b> (`float`) â€“ dropout ratio. Defaults to no dropout.\n",
    " - <b>bias (`bool`)</b> â€“ whether to have a bias term in convolution blocks. Defaults to True. According to Performance Tuning Guide, if a conv layer is directly followed by a batch norm layer, bias should be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from monai.networks.nets import AutoEncoder\n",
    "#from monai.networks.nets import UNet\n",
    "from monai.networks.nets import AttentionUnet\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "#net = AutoEncoder( spatial_dims =2, in_channels=1, out_channels=1, channels=(4, 8, 16, 32),  strides=(2, 2, 2, 2), ) \n",
    "#net = AutoEncoder( spatial_dims =2, in_channels=1, out_channels=1, channels=(8, 16, 32, 64), strides=(2, 2, 2, 2), )\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[16, 32, 64], strides=[2, 2], num_res_units=2, dropout=0.2) # 3 layers \n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16], strides=[2, 2], num_res_units=2, dropout=0.2) # 3 layers \n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16, 32, 64], strides=[2,2, 2,2], num_res_units=3, dropout=0.4) # 5 layers\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[16, 32, 64, 128, 256], strides=[2, 2, 2, 2], num_res_units=4, dropout=0.2) # 5 layers\n",
    "#net = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=[4, 8, 16, 32, 64, 128, 256], strides=[2, 2, 2, 2, 2, 2], num_res_units=6, dropout=0.2) # 7 layers\n",
    "net = AttentionUnet(spatial_dims=2, in_channels=1, out_channels=1, channels=[16, 32, 64, 128, 256, 512, 1024], strides=[2, 2, 2, 2, 2, 2], dropout=0.5) # 7 layers\n",
    "\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "loss = DiceLoss(sigmoid=True)\n",
    "metric = DiceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "opt = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train loop [Baseline]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "step_losses = []\n",
    "epoch_metrics = []\n",
    "total_step = 0\n",
    "\n",
    "print(\"start train\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "\n",
    "    # train network with training images\n",
    "    for bimages, bsegs in loader:\n",
    "        bimages = bimages.to(device)\n",
    "        bsegs = bsegs.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        prediction = net(bimages)\n",
    "        loss_val = loss(torch.sigmoid(prediction), bsegs)\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        step_losses.append((total_step, loss_val.item()))\n",
    "        total_step += 1\n",
    "\n",
    "    net.eval()\n",
    "    metric_vals = []\n",
    "\n",
    "    # test our network using the validation dataset\n",
    "    with torch.no_grad():\n",
    "        for bimages, bsegs in val_loader:\n",
    "            bimages = bimages.to(device)\n",
    "            bsegs = bsegs.to(device)\n",
    "\n",
    "            #prediction = net(bimages)\n",
    "\n",
    "            mvals = metric(y_pred=torch.sigmoid(prediction) > 0.5, y=bsegs)\n",
    "            metric_vals += mvals.cpu().data.numpy().flatten().tolist()\n",
    "\n",
    "    epoch_metrics.append((total_step, np.average(metric_vals)))\n",
    "\n",
    "    progress_bar(epoch + 1, num_epochs, f\"Validation Metric: {epoch_metrics[-1][1]:7.3}\")\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now graph the results from our training and find the results are not very good:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ Challenge: Improve Results and Implementation ðŸ‘ˆ\n",
    "\n",
    "### 3. Replace The Training Loop\n",
    "\n",
    "This notebook uses a simple training loop with validation done explicitly. Replace this with a use of the `SupervisedTrainer` class and `SupervisedEvaluator` to do the evaluation throughout the training process. The graph plotting is done simply by recording values at each iteration through the loop, you'll want to use some other mechanism to do the same thing such as using a `MetricLogger` handler object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ignite\n",
    "in the end-to-end pipeline notebook,  we already use   `SupervisedTrainer` and `SupervisedEvaluator`. \n",
    "use MedNIST pipeline 03 and 04 for reference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_losses = []\n",
    "epoch_metrics = []\n",
    "metric_values = []\n",
    "iter_losses=[]\n",
    "batch_sizes=[]\n",
    "epoch_loss_values =[]\n",
    "total_step = 0\n",
    "max_epochs = 100\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, Activationsd, AsDiscreted\n",
    "\n",
    "post_transform = Compose(\n",
    "    [Activationsd(keys=\"pred\", sigmoid=True), AsDiscreted(keys=[\"pred\", \"label\"], threshold_values=True,),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.handlers import StatsHandler, MeanDice, from_engine\n",
    "from monai.engines import SupervisedEvaluator\n",
    "\n",
    "evaluator = SupervisedEvaluator(\n",
    "    device= device, #TODO\n",
    "    val_data_loader= val_loader, #TODO\n",
    "    network=net , #TODO\n",
    "    postprocessing= post_transform, #TODO\n",
    "    key_val_metric={\"val_mean_dice\": MeanDice(include_background=True, output_transform=from_engine([\"pred\", \"label\"]))},\n",
    "    val_handlers=[StatsHandler(output_transform=lambda x: None)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 6 62\n"
     ]
    }
   ],
   "source": [
    "from monai.handlers import MetricLogger, ValidationHandler\n",
    "from monai.engines import SupervisedTrainer\n",
    "\n",
    "logger = MetricLogger(evaluator=evaluator)\n",
    "\n",
    "trainer = SupervisedTrainer(\n",
    "    device= device, #TODO\n",
    "    max_epochs= max_epochs,  #TODO\n",
    "    train_data_loader= train_loader,  #TODO\n",
    "    network= net,  #TODO\n",
    "    optimizer= opt,  #TODO\n",
    "    loss_function= loss,  #TODO\n",
    "    train_handlers=[logger, ValidationHandler(1, evaluator)],\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(train_dat) // train_loader.batch_size\n",
    "if len(train_dat) % train_loader.batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "print(len(train_dat) , train_loader.batch_size, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def _end_iter(engine: Engine):\n",
    "    global step\n",
    "    loss = np.average([o[\"loss\"] for o in engine.state.output])\n",
    "    batch_len = len(engine.state.batch[0])\n",
    "    epoch = engine.state.epoch\n",
    "    epoch_len = engine.state.max_epochs\n",
    "    step_total = engine.state.iteration  \n",
    "    iter_losses.append(loss)\n",
    "    batch_sizes.append(batch_len)\n",
    "\n",
    "    print(f\"\\nepoch {epoch}/{epoch_len}, step {step}/{steps_per_epoch},  total step {step_total}/{steps_per_epoch*epoch_len}, training_loss = {loss:.4f}\", end='')\n",
    "    step += 1\n",
    "    \n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def _end_epoch(engine: Engine):\n",
    "    global step\n",
    "    overall_average_loss = np.average(iter_losses, weights=batch_sizes)\n",
    "    epoch_loss_values.append(overall_average_loss)    \n",
    "    # clear the contents of iter_losses and batch_sizes for the next epoch\n",
    "    del iter_losses[:]\n",
    "    del batch_sizes[:]\n",
    "    \n",
    "    dice = evaluator.state.metrics[\"val_mean_dice\"]   \n",
    "    metric_values.append(dice)\n",
    "    #progress_bar(engine.state.epoch, num_epochs, f\"Validation Metric: {dice:7.3}\")\n",
    "    print(f\" | avg loss: {overall_average_loss:.4f} Dice Metric: {dice:7.3}\", end='')\n",
    "    step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1/100, step 1/62,  total step 1/6200, training_loss = 0.8994\n",
      "epoch 1/100, step 2/62,  total step 2/6200, training_loss = 0.9145\n",
      "epoch 1/100, step 3/62,  total step 3/6200, training_loss = 0.9094\n",
      "epoch 1/100, step 4/62,  total step 4/6200, training_loss = 0.9181\n",
      "epoch 1/100, step 5/62,  total step 5/6200, training_loss = 0.8364\n",
      "epoch 1/100, step 6/62,  total step 6/6200, training_loss = 0.8896\n",
      "epoch 1/100, step 7/62,  total step 7/6200, training_loss = 0.8388\n",
      "epoch 1/100, step 8/62,  total step 8/6200, training_loss = 0.8618\n",
      "epoch 1/100, step 9/62,  total step 9/6200, training_loss = 0.8190\n",
      "epoch 1/100, step 10/62,  total step 10/6200, training_loss = 0.8226\n",
      "epoch 1/100, step 11/62,  total step 11/6200, training_loss = 0.7831\n",
      "epoch 1/100, step 12/62,  total step 12/6200, training_loss = 0.8080\n",
      "epoch 1/100, step 13/62,  total step 13/6200, training_loss = 0.8857\n",
      "epoch 1/100, step 14/62,  total step 14/6200, training_loss = 0.8983\n",
      "epoch 1/100, step 15/62,  total step 15/6200, training_loss = 0.9100\n",
      "epoch 1/100, step 16/62,  total step 16/6200, training_loss = 0.9007\n",
      "epoch 1/100, step 17/62,  total step 17/6200, training_loss = 0.8397\n",
      "epoch 1/100, step 18/62,  total step 18/6200, training_loss = 0.8306\n",
      "epoch 1/100, step 19/62,  total step 19/6200, training_loss = 0.8632\n",
      "epoch 1/100, step 20/62,  total step 20/6200, training_loss = 0.7952\n",
      "epoch 1/100, step 21/62,  total step 21/6200, training_loss = 0.8454\n",
      "epoch 1/100, step 22/62,  total step 22/6200, training_loss = 0.8681\n",
      "epoch 1/100, step 23/62,  total step 23/6200, training_loss = 0.8603\n",
      "epoch 1/100, step 24/62,  total step 24/6200, training_loss = 0.8104\n",
      "epoch 1/100, step 25/62,  total step 25/6200, training_loss = 0.8002\n",
      "epoch 1/100, step 26/62,  total step 26/6200, training_loss = 0.7650\n",
      "epoch 1/100, step 27/62,  total step 27/6200, training_loss = 0.7773\n",
      "epoch 1/100, step 28/62,  total step 28/6200, training_loss = 0.8423\n",
      "epoch 1/100, step 29/62,  total step 29/6200, training_loss = 0.8566\n",
      "epoch 1/100, step 30/62,  total step 30/6200, training_loss = 0.8503\n",
      "epoch 1/100, step 31/62,  total step 31/6200, training_loss = 0.7906\n",
      "epoch 1/100, step 32/62,  total step 32/6200, training_loss = 0.7794\n",
      "epoch 1/100, step 33/62,  total step 33/6200, training_loss = 0.8780\n",
      "epoch 1/100, step 34/62,  total step 34/6200, training_loss = 0.7847\n",
      "epoch 1/100, step 35/62,  total step 35/6200, training_loss = 0.8035\n",
      "epoch 1/100, step 36/62,  total step 36/6200, training_loss = 0.8630\n",
      "epoch 1/100, step 37/62,  total step 37/6200, training_loss = 0.7758\n",
      "epoch 1/100, step 38/62,  total step 38/6200, training_loss = 0.8486\n",
      "epoch 1/100, step 39/62,  total step 39/6200, training_loss = 0.8607\n",
      "epoch 1/100, step 40/62,  total step 40/6200, training_loss = 0.7830\n",
      "epoch 1/100, step 41/62,  total step 41/6200, training_loss = 0.7775\n",
      "epoch 1/100, step 42/62,  total step 42/6200, training_loss = 0.8241\n",
      "epoch 1/100, step 43/62,  total step 43/6200, training_loss = 0.7333\n",
      "epoch 1/100, step 44/62,  total step 44/6200, training_loss = 0.7588\n",
      "epoch 1/100, step 45/62,  total step 45/6200, training_loss = 0.7863\n",
      "epoch 1/100, step 46/62,  total step 46/6200, training_loss = 0.7318\n",
      "epoch 1/100, step 47/62,  total step 47/6200, training_loss = 0.7806\n",
      "epoch 1/100, step 48/62,  total step 48/6200, training_loss = 0.6900\n",
      "epoch 1/100, step 49/62,  total step 49/6200, training_loss = 0.7523\n",
      "epoch 1/100, step 50/62,  total step 50/6200, training_loss = 0.7872\n",
      "epoch 1/100, step 51/62,  total step 51/6200, training_loss = 0.7712\n",
      "epoch 1/100, step 52/62,  total step 52/6200, training_loss = 0.7623\n",
      "epoch 1/100, step 53/62,  total step 53/6200, training_loss = 0.6432\n",
      "epoch 1/100, step 54/62,  total step 54/6200, training_loss = 0.6650\n",
      "epoch 1/100, step 55/62,  total step 55/6200, training_loss = 0.7254\n",
      "epoch 1/100, step 56/62,  total step 56/6200, training_loss = 0.7940\n",
      "epoch 1/100, step 57/62,  total step 57/6200, training_loss = 0.7944\n",
      "epoch 1/100, step 58/62,  total step 58/6200, training_loss = 0.8314\n",
      "epoch 1/100, step 59/62,  total step 59/6200, training_loss = 0.6442\n",
      "epoch 1/100, step 60/62,  total step 60/6200, training_loss = 0.7644\n",
      "epoch 1/100, step 61/62,  total step 61/6200, training_loss = 0.7030\n",
      "epoch 1/100, step 62/62,  total step 62/6200, training_loss = 0.8159 | avg loss: 0.8097 Dice Metric:   0.347\n",
      "epoch 2/100, step 1/62,  total step 63/6200, training_loss = 0.7133\n",
      "epoch 2/100, step 2/62,  total step 64/6200, training_loss = 0.7539\n",
      "epoch 2/100, step 3/62,  total step 65/6200, training_loss = 0.7722\n",
      "epoch 2/100, step 4/62,  total step 66/6200, training_loss = 0.8230\n",
      "epoch 2/100, step 5/62,  total step 67/6200, training_loss = 0.6846\n",
      "epoch 2/100, step 6/62,  total step 68/6200, training_loss = 0.7693\n",
      "epoch 2/100, step 7/62,  total step 69/6200, training_loss = 0.6395\n",
      "epoch 2/100, step 8/62,  total step 70/6200, training_loss = 0.7433\n",
      "epoch 2/100, step 9/62,  total step 71/6200, training_loss = 0.7213\n",
      "epoch 2/100, step 10/62,  total step 72/6200, training_loss = 0.6710\n",
      "epoch 2/100, step 11/62,  total step 73/6200, training_loss = 0.6216\n",
      "epoch 2/100, step 12/62,  total step 74/6200, training_loss = 0.6746\n",
      "epoch 2/100, step 13/62,  total step 75/6200, training_loss = 0.7929\n",
      "epoch 2/100, step 14/62,  total step 76/6200, training_loss = 0.8211\n",
      "epoch 2/100, step 15/62,  total step 77/6200, training_loss = 0.8478\n",
      "epoch 2/100, step 16/62,  total step 78/6200, training_loss = 0.8357\n",
      "epoch 2/100, step 17/62,  total step 79/6200, training_loss = 0.7454\n",
      "epoch 2/100, step 18/62,  total step 80/6200, training_loss = 0.7374\n",
      "epoch 2/100, step 19/62,  total step 81/6200, training_loss = 0.7844\n",
      "epoch 2/100, step 20/62,  total step 82/6200, training_loss = 0.6826\n",
      "epoch 2/100, step 21/62,  total step 83/6200, training_loss = 0.7596\n",
      "epoch 2/100, step 22/62,  total step 84/6200, training_loss = 0.7979\n",
      "epoch 2/100, step 23/62,  total step 85/6200, training_loss = 0.7889\n",
      "epoch 2/100, step 24/62,  total step 86/6200, training_loss = 0.7140\n",
      "epoch 2/100, step 25/62,  total step 87/6200, training_loss = 0.7025\n",
      "epoch 2/100, step 26/62,  total step 88/6200, training_loss = 0.6436\n",
      "epoch 2/100, step 27/62,  total step 89/6200, training_loss = 0.6638\n",
      "epoch 2/100, step 28/62,  total step 90/6200, training_loss = 0.7684\n",
      "epoch 2/100, step 29/62,  total step 91/6200, training_loss = 0.7900\n",
      "epoch 2/100, step 30/62,  total step 92/6200, training_loss = 0.7757\n",
      "epoch 2/100, step 31/62,  total step 93/6200, training_loss = 0.6898\n",
      "epoch 2/100, step 32/62,  total step 94/6200, training_loss = 0.6761\n",
      "epoch 2/100, step 33/62,  total step 95/6200, training_loss = 0.8181\n",
      "epoch 2/100, step 34/62,  total step 96/6200, training_loss = 0.6804\n",
      "epoch 2/100, step 35/62,  total step 97/6200, training_loss = 0.7101\n",
      "epoch 2/100, step 36/62,  total step 98/6200, training_loss = 0.7976\n",
      "epoch 2/100, step 37/62,  total step 99/6200, training_loss = 0.6734\n",
      "epoch 2/100, step 38/62,  total step 100/6200, training_loss = 0.7768\n",
      "epoch 2/100, step 39/62,  total step 101/6200, training_loss = 0.7935\n",
      "epoch 2/100, step 40/62,  total step 102/6200, training_loss = 0.6895\n",
      "epoch 2/100, step 41/62,  total step 103/6200, training_loss = 0.6819\n",
      "epoch 2/100, step 42/62,  total step 104/6200, training_loss = 0.7473\n",
      "epoch 2/100, step 43/62,  total step 105/6200, training_loss = 0.6314\n",
      "epoch 2/100, step 44/62,  total step 106/6200, training_loss = 0.6567\n",
      "epoch 2/100, step 45/62,  total step 107/6200, training_loss = 0.6927\n",
      "epoch 2/100, step 46/62,  total step 108/6200, training_loss = 0.6230\n",
      "epoch 2/100, step 47/62,  total step 109/6200, training_loss = 0.6882\n",
      "epoch 2/100, step 48/62,  total step 110/6200, training_loss = 0.5662\n",
      "epoch 2/100, step 49/62,  total step 111/6200, training_loss = 0.6549\n",
      "epoch 2/100, step 50/62,  total step 112/6200, training_loss = 0.6955\n",
      "epoch 2/100, step 51/62,  total step 113/6200, training_loss = 0.6780\n",
      "epoch 2/100, step 52/62,  total step 114/6200, training_loss = 0.6659\n",
      "epoch 2/100, step 53/62,  total step 115/6200, training_loss = 0.5152\n",
      "epoch 2/100, step 54/62,  total step 116/6200, training_loss = 0.5507\n",
      "epoch 2/100, step 55/62,  total step 117/6200, training_loss = 0.6240\n",
      "epoch 2/100, step 56/62,  total step 118/6200, training_loss = 0.7084\n",
      "epoch 2/100, step 57/62,  total step 119/6200, training_loss = 0.7099\n",
      "epoch 2/100, step 58/62,  total step 120/6200, training_loss = 0.7648\n",
      "epoch 2/100, step 59/62,  total step 121/6200, training_loss = 0.5540\n",
      "epoch 2/100, step 60/62,  total step 122/6200, training_loss = 0.6722\n",
      "epoch 2/100, step 61/62,  total step 123/6200, training_loss = 0.5917\n",
      "epoch 2/100, step 62/62,  total step 124/6200, training_loss = 0.7349 | avg loss: 0.7089 Dice Metric:   0.428\n",
      "epoch 3/100, step 1/62,  total step 125/6200, training_loss = 0.6375\n",
      "epoch 3/100, step 2/62,  total step 126/6200, training_loss = 0.6642\n",
      "epoch 3/100, step 3/62,  total step 127/6200, training_loss = 0.7039\n",
      "epoch 3/100, step 4/62,  total step 128/6200, training_loss = 0.7689\n",
      "epoch 3/100, step 5/62,  total step 129/6200, training_loss = 0.5737\n",
      "epoch 3/100, step 6/62,  total step 130/6200, training_loss = 0.6848\n",
      "epoch 3/100, step 7/62,  total step 131/6200, training_loss = 0.5374\n",
      "epoch 3/100, step 8/62,  total step 132/6200, training_loss = 0.6599\n",
      "epoch 3/100, step 9/62,  total step 133/6200, training_loss = 0.6157\n",
      "epoch 3/100, step 10/62,  total step 134/6200, training_loss = 0.5788\n",
      "epoch 3/100, step 11/62,  total step 135/6200, training_loss = 0.5140\n",
      "epoch 3/100, step 12/62,  total step 136/6200, training_loss = 0.5711\n",
      "epoch 3/100, step 13/62,  total step 137/6200, training_loss = 0.7240\n",
      "epoch 3/100, step 14/62,  total step 138/6200, training_loss = 0.7595\n",
      "epoch 3/100, step 15/62,  total step 139/6200, training_loss = 0.7929\n",
      "epoch 3/100, step 16/62,  total step 140/6200, training_loss = 0.7762\n",
      "epoch 3/100, step 17/62,  total step 141/6200, training_loss = 0.6551\n",
      "epoch 3/100, step 18/62,  total step 142/6200, training_loss = 0.6618\n",
      "epoch 3/100, step 19/62,  total step 143/6200, training_loss = 0.7070\n",
      "epoch 3/100, step 20/62,  total step 144/6200, training_loss = 0.6110\n",
      "epoch 3/100, step 21/62,  total step 145/6200, training_loss = 0.6770\n",
      "epoch 3/100, step 22/62,  total step 146/6200, training_loss = 0.7375\n",
      "epoch 3/100, step 23/62,  total step 147/6200, training_loss = 0.7172\n",
      "epoch 3/100, step 24/62,  total step 148/6200, training_loss = 0.6360\n",
      "epoch 3/100, step 25/62,  total step 149/6200, training_loss = 0.6296\n",
      "epoch 3/100, step 26/62,  total step 150/6200, training_loss = 0.5374\n",
      "epoch 3/100, step 27/62,  total step 151/6200, training_loss = 0.5814\n",
      "epoch 3/100, step 28/62,  total step 152/6200, training_loss = 0.6944\n",
      "epoch 3/100, step 29/62,  total step 153/6200, training_loss = 0.7201\n",
      "epoch 3/100, step 30/62,  total step 154/6200, training_loss = 0.7054\n",
      "epoch 3/100, step 31/62,  total step 155/6200, training_loss = 0.5961\n",
      "epoch 3/100, step 32/62,  total step 156/6200, training_loss = 0.5890\n",
      "epoch 3/100, step 33/62,  total step 157/6200, training_loss = 0.7610\n",
      "epoch 3/100, step 34/62,  total step 158/6200, training_loss = 0.5876\n",
      "epoch 3/100, step 35/62,  total step 159/6200, training_loss = 0.6342\n",
      "epoch 3/100, step 36/62,  total step 160/6200, training_loss = 0.7393\n",
      "epoch 3/100, step 37/62,  total step 161/6200, training_loss = 0.6297\n",
      "epoch 3/100, step 38/62,  total step 162/6200, training_loss = 0.7106\n",
      "epoch 3/100, step 39/62,  total step 163/6200, training_loss = 0.7276\n",
      "epoch 3/100, step 40/62,  total step 164/6200, training_loss = 0.6037\n",
      "epoch 3/100, step 41/62,  total step 165/6200, training_loss = 0.6294\n",
      "epoch 3/100, step 42/62,  total step 166/6200, training_loss = 0.7021\n",
      "epoch 3/100, step 43/62,  total step 167/6200, training_loss = 0.5443\n",
      "epoch 3/100, step 44/62,  total step 168/6200, training_loss = 0.5738\n",
      "epoch 3/100, step 45/62,  total step 169/6200, training_loss = 0.6093\n",
      "epoch 3/100, step 46/62,  total step 170/6200, training_loss = 0.5297\n",
      "epoch 3/100, step 47/62,  total step 171/6200, training_loss = 0.6077\n",
      "epoch 3/100, step 48/62,  total step 172/6200, training_loss = 0.4655\n",
      "epoch 3/100, step 49/62,  total step 173/6200, training_loss = 0.5624\n",
      "epoch 3/100, step 50/62,  total step 174/6200, training_loss = 0.6099\n",
      "epoch 3/100, step 51/62,  total step 175/6200, training_loss = 0.5860\n",
      "epoch 3/100, step 52/62,  total step 176/6200, training_loss = 0.5771\n",
      "epoch 3/100, step 53/62,  total step 177/6200, training_loss = 0.4320\n",
      "epoch 3/100, step 54/62,  total step 178/6200, training_loss = 0.4524\n",
      "epoch 3/100, step 55/62,  total step 179/6200, training_loss = 0.5315\n",
      "epoch 3/100, step 56/62,  total step 180/6200, training_loss = 0.6249\n",
      "epoch 3/100, step 57/62,  total step 181/6200, training_loss = 0.6285\n",
      "epoch 3/100, step 58/62,  total step 182/6200, training_loss = 0.6999\n",
      "epoch 3/100, step 59/62,  total step 183/6200, training_loss = 0.4824\n",
      "epoch 3/100, step 60/62,  total step 184/6200, training_loss = 0.6025\n",
      "epoch 3/100, step 61/62,  total step 185/6200, training_loss = 0.4975\n",
      "epoch 3/100, step 62/62,  total step 186/6200, training_loss = 0.6721 | avg loss: 0.6296 Dice Metric:   0.492\n",
      "epoch 4/100, step 1/62,  total step 187/6200, training_loss = 0.5902\n",
      "epoch 4/100, step 2/62,  total step 188/6200, training_loss = 0.5856\n",
      "epoch 4/100, step 3/62,  total step 189/6200, training_loss = 0.6288\n",
      "epoch 4/100, step 4/62,  total step 190/6200, training_loss = 0.6982\n",
      "epoch 4/100, step 5/62,  total step 191/6200, training_loss = 0.4707\n",
      "epoch 4/100, step 6/62,  total step 192/6200, training_loss = 0.6072\n",
      "epoch 4/100, step 7/62,  total step 193/6200, training_loss = 0.4821\n",
      "epoch 4/100, step 8/62,  total step 194/6200, training_loss = 0.5836\n",
      "epoch 4/100, step 9/62,  total step 195/6200, training_loss = 0.5522\n",
      "epoch 4/100, step 10/62,  total step 196/6200, training_loss = 0.5177\n",
      "epoch 4/100, step 11/62,  total step 197/6200, training_loss = 0.4122\n",
      "epoch 4/100, step 12/62,  total step 198/6200, training_loss = 0.4931\n",
      "epoch 4/100, step 13/62,  total step 199/6200, training_loss = 0.6593\n",
      "epoch 4/100, step 14/62,  total step 200/6200, training_loss = 0.6930\n",
      "epoch 4/100, step 15/62,  total step 201/6200, training_loss = 0.7373\n",
      "epoch 4/100, step 16/62,  total step 202/6200, training_loss = 0.7232\n",
      "epoch 4/100, step 17/62,  total step 203/6200, training_loss = 0.5919\n",
      "epoch 4/100, step 18/62,  total step 204/6200, training_loss = 0.5671\n",
      "epoch 4/100, step 19/62,  total step 205/6200, training_loss = 0.6286\n",
      "epoch 4/100, step 20/62,  total step 206/6200, training_loss = 0.5711\n",
      "epoch 4/100, step 21/62,  total step 207/6200, training_loss = 0.5900\n",
      "epoch 4/100, step 22/62,  total step 208/6200, training_loss = 0.6647\n",
      "epoch 4/100, step 23/62,  total step 209/6200, training_loss = 0.6447\n",
      "epoch 4/100, step 24/62,  total step 210/6200, training_loss = 0.5583\n",
      "epoch 4/100, step 25/62,  total step 211/6200, training_loss = 0.5659\n",
      "epoch 4/100, step 26/62,  total step 212/6200, training_loss = 0.4645\n",
      "epoch 4/100, step 27/62,  total step 213/6200, training_loss = 0.5064\n",
      "epoch 4/100, step 28/62,  total step 214/6200, training_loss = 0.6430\n",
      "epoch 4/100, step 29/62,  total step 215/6200, training_loss = 0.6578\n",
      "epoch 4/100, step 30/62,  total step 216/6200, training_loss = 0.6297\n",
      "epoch 4/100, step 31/62,  total step 217/6200, training_loss = 0.5059\n",
      "epoch 4/100, step 32/62,  total step 218/6200, training_loss = 0.5157\n",
      "epoch 4/100, step 33/62,  total step 219/6200, training_loss = 0.7102\n",
      "epoch 4/100, step 34/62,  total step 220/6200, training_loss = 0.4971\n",
      "epoch 4/100, step 35/62,  total step 221/6200, training_loss = 0.5639\n",
      "epoch 4/100, step 36/62,  total step 222/6200, training_loss = 0.6730\n",
      "epoch 4/100, step 37/62,  total step 223/6200, training_loss = 0.5402\n",
      "epoch 4/100, step 38/62,  total step 224/6200, training_loss = 0.6359\n",
      "epoch 4/100, step 39/62,  total step 225/6200, training_loss = 0.6669\n",
      "epoch 4/100, step 40/62,  total step 226/6200, training_loss = 0.5428\n",
      "epoch 4/100, step 41/62,  total step 227/6200, training_loss = 0.5225\n",
      "epoch 4/100, step 42/62,  total step 228/6200, training_loss = 0.5863\n",
      "epoch 4/100, step 43/62,  total step 229/6200, training_loss = 0.4719\n",
      "epoch 4/100, step 44/62,  total step 230/6200, training_loss = 0.5071\n",
      "epoch 4/100, step 45/62,  total step 231/6200, training_loss = 0.5142\n",
      "epoch 4/100, step 46/62,  total step 232/6200, training_loss = 0.4322\n",
      "epoch 4/100, step 47/62,  total step 233/6200, training_loss = 0.5424\n",
      "epoch 4/100, step 48/62,  total step 234/6200, training_loss = 0.3842\n",
      "epoch 4/100, step 49/62,  total step 235/6200, training_loss = 0.4686\n",
      "epoch 4/100, step 50/62,  total step 236/6200, training_loss = 0.5253\n",
      "epoch 4/100, step 51/62,  total step 237/6200, training_loss = 0.5172\n",
      "epoch 4/100, step 52/62,  total step 238/6200, training_loss = 0.4924\n",
      "epoch 4/100, step 53/62,  total step 239/6200, training_loss = 0.3377\n",
      "epoch 4/100, step 54/62,  total step 240/6200, training_loss = 0.3798\n",
      "epoch 4/100, step 55/62,  total step 241/6200, training_loss = 0.4519\n",
      "epoch 4/100, step 56/62,  total step 242/6200, training_loss = 0.5537\n",
      "epoch 4/100, step 57/62,  total step 243/6200, training_loss = 0.5550\n",
      "epoch 4/100, step 58/62,  total step 244/6200, training_loss = 0.6427\n",
      "epoch 4/100, step 59/62,  total step 245/6200, training_loss = 0.4202\n",
      "epoch 4/100, step 60/62,  total step 246/6200, training_loss = 0.5461\n",
      "epoch 4/100, step 61/62,  total step 247/6200, training_loss = 0.4161\n",
      "epoch 4/100, step 62/62,  total step 248/6200, training_loss = 0.5961 | avg loss: 0.5554 Dice Metric:   0.532\n",
      "epoch 5/100, step 1/62,  total step 249/6200, training_loss = 0.5558\n",
      "epoch 5/100, step 2/62,  total step 250/6200, training_loss = 0.5142\n",
      "epoch 5/100, step 3/62,  total step 251/6200, training_loss = 0.5598\n",
      "epoch 5/100, step 4/62,  total step 252/6200, training_loss = 0.6522\n",
      "epoch 5/100, step 5/62,  total step 253/6200, training_loss = 0.3851\n",
      "epoch 5/100, step 6/62,  total step 254/6200, training_loss = 0.5369\n",
      "epoch 5/100, step 7/62,  total step 255/6200, training_loss = 0.4066\n",
      "epoch 5/100, step 8/62,  total step 256/6200, training_loss = 0.5159\n",
      "epoch 5/100, step 9/62,  total step 257/6200, training_loss = 0.4881\n",
      "epoch 5/100, step 10/62,  total step 258/6200, training_loss = 0.4539\n",
      "epoch 5/100, step 11/62,  total step 259/6200, training_loss = 0.3385\n",
      "epoch 5/100, step 12/62,  total step 260/6200, training_loss = 0.4063\n",
      "epoch 5/100, step 13/62,  total step 261/6200, training_loss = 0.5785\n",
      "epoch 5/100, step 14/62,  total step 262/6200, training_loss = 0.6178\n",
      "epoch 5/100, step 15/62,  total step 263/6200, training_loss = 0.6790\n",
      "epoch 5/100, step 16/62,  total step 264/6200, training_loss = 0.6624\n",
      "epoch 5/100, step 17/62,  total step 265/6200, training_loss = 0.5128\n",
      "epoch 5/100, step 18/62,  total step 266/6200, training_loss = 0.4939\n",
      "epoch 5/100, step 19/62,  total step 267/6200, training_loss = 0.5610\n",
      "epoch 5/100, step 20/62,  total step 268/6200, training_loss = 0.5060\n",
      "epoch 5/100, step 21/62,  total step 269/6200, training_loss = 0.5100\n",
      "epoch 5/100, step 22/62,  total step 270/6200, training_loss = 0.6104\n",
      "epoch 5/100, step 23/62,  total step 271/6200, training_loss = 0.5498\n",
      "epoch 5/100, step 24/62,  total step 272/6200, training_loss = 0.4734\n",
      "epoch 5/100, step 25/62,  total step 273/6200, training_loss = 0.4845\n",
      "epoch 5/100, step 26/62,  total step 274/6200, training_loss = 0.4094\n",
      "epoch 5/100, step 27/62,  total step 275/6200, training_loss = 0.4542\n",
      "epoch 5/100, step 28/62,  total step 276/6200, training_loss = 0.5911\n",
      "epoch 5/100, step 29/62,  total step 277/6200, training_loss = 0.5765\n",
      "epoch 5/100, step 30/62,  total step 278/6200, training_loss = 0.5498\n",
      "epoch 5/100, step 31/62,  total step 279/6200, training_loss = 0.4320\n",
      "epoch 5/100, step 32/62,  total step 280/6200, training_loss = 0.4434\n",
      "epoch 5/100, step 33/62,  total step 281/6200, training_loss = 0.6322\n",
      "epoch 5/100, step 34/62,  total step 282/6200, training_loss = 0.4205\n",
      "epoch 5/100, step 35/62,  total step 283/6200, training_loss = 0.4884\n",
      "epoch 5/100, step 36/62,  total step 284/6200, training_loss = 0.6018\n",
      "epoch 5/100, step 37/62,  total step 285/6200, training_loss = 0.5587\n",
      "epoch 5/100, step 38/62,  total step 286/6200, training_loss = 0.5648\n",
      "epoch 5/100, step 39/62,  total step 287/6200, training_loss = 0.5899\n",
      "epoch 5/100, step 40/62,  total step 288/6200, training_loss = 0.4893\n",
      "epoch 5/100, step 41/62,  total step 289/6200, training_loss = 0.4767\n",
      "epoch 5/100, step 42/62,  total step 290/6200, training_loss = 0.5590\n",
      "epoch 5/100, step 43/62,  total step 291/6200, training_loss = 0.4320\n",
      "epoch 5/100, step 44/62,  total step 292/6200, training_loss = 0.4461\n",
      "epoch 5/100, step 45/62,  total step 293/6200, training_loss = 0.4269\n",
      "epoch 5/100, step 46/62,  total step 294/6200, training_loss = 0.3633\n",
      "epoch 5/100, step 47/62,  total step 295/6200, training_loss = 0.4772\n",
      "epoch 5/100, step 48/62,  total step 296/6200, training_loss = 0.3555\n",
      "epoch 5/100, step 49/62,  total step 297/6200, training_loss = 0.4149\n",
      "epoch 5/100, step 50/62,  total step 298/6200, training_loss = 0.4632\n",
      "epoch 5/100, step 51/62,  total step 299/6200, training_loss = 0.4417\n",
      "epoch 5/100, step 52/62,  total step 300/6200, training_loss = 0.4221\n",
      "epoch 5/100, step 53/62,  total step 301/6200, training_loss = 0.2943\n",
      "epoch 5/100, step 54/62,  total step 302/6200, training_loss = 0.3377\n",
      "epoch 5/100, step 55/62,  total step 303/6200, training_loss = 0.3843\n",
      "epoch 5/100, step 56/62,  total step 304/6200, training_loss = 0.4833\n",
      "epoch 5/100, step 57/62,  total step 305/6200, training_loss = 0.4982\n",
      "epoch 5/100, step 58/62,  total step 306/6200, training_loss = 0.5981\n",
      "epoch 5/100, step 59/62,  total step 307/6200, training_loss = 0.3445\n",
      "epoch 5/100, step 60/62,  total step 308/6200, training_loss = 0.5199\n",
      "epoch 5/100, step 61/62,  total step 309/6200, training_loss = 0.3477\n",
      "epoch 5/100, step 62/62,  total step 310/6200, training_loss = 0.5652 | avg loss: 0.4920 Dice Metric:   0.585\n",
      "epoch 6/100, step 1/62,  total step 311/6200, training_loss = 0.5090\n",
      "epoch 6/100, step 2/62,  total step 312/6200, training_loss = 0.4662\n",
      "epoch 6/100, step 3/62,  total step 313/6200, training_loss = 0.5138\n",
      "epoch 6/100, step 4/62,  total step 314/6200, training_loss = 0.6110\n",
      "epoch 6/100, step 5/62,  total step 315/6200, training_loss = 0.3127\n",
      "epoch 6/100, step 6/62,  total step 316/6200, training_loss = 0.4699\n",
      "epoch 6/100, step 7/62,  total step 317/6200, training_loss = 0.3849\n",
      "epoch 6/100, step 8/62,  total step 318/6200, training_loss = 0.4836\n",
      "epoch 6/100, step 9/62,  total step 319/6200, training_loss = 0.4627\n",
      "epoch 6/100, step 10/62,  total step 320/6200, training_loss = 0.4208\n",
      "epoch 6/100, step 11/62,  total step 321/6200, training_loss = 0.3002\n",
      "epoch 6/100, step 12/62,  total step 322/6200, training_loss = 0.3685\n",
      "epoch 6/100, step 13/62,  total step 323/6200, training_loss = 0.5233\n",
      "epoch 6/100, step 14/62,  total step 324/6200, training_loss = 0.5468\n",
      "epoch 6/100, step 15/62,  total step 325/6200, training_loss = 0.6457\n",
      "epoch 6/100, step 16/62,  total step 326/6200, training_loss = 0.6344\n",
      "epoch 6/100, step 17/62,  total step 327/6200, training_loss = 0.4756\n",
      "epoch 6/100, step 18/62,  total step 328/6200, training_loss = 0.4423\n",
      "epoch 6/100, step 19/62,  total step 329/6200, training_loss = 0.5196\n",
      "epoch 6/100, step 20/62,  total step 330/6200, training_loss = 0.5246\n",
      "epoch 6/100, step 21/62,  total step 331/6200, training_loss = 0.4726\n",
      "epoch 6/100, step 22/62,  total step 332/6200, training_loss = 0.5463\n",
      "epoch 6/100, step 23/62,  total step 333/6200, training_loss = 0.4939\n",
      "epoch 6/100, step 24/62,  total step 334/6200, training_loss = 0.4322\n",
      "epoch 6/100, step 25/62,  total step 335/6200, training_loss = 0.4270\n",
      "epoch 6/100, step 26/62,  total step 336/6200, training_loss = 0.3412\n",
      "epoch 6/100, step 27/62,  total step 337/6200, training_loss = 0.3901\n",
      "epoch 6/100, step 28/62,  total step 338/6200, training_loss = 0.5673\n",
      "epoch 6/100, step 29/62,  total step 339/6200, training_loss = 0.5152\n",
      "epoch 6/100, step 30/62,  total step 340/6200, training_loss = 0.4854\n",
      "epoch 6/100, step 31/62,  total step 341/6200, training_loss = 0.3721\n",
      "epoch 6/100, step 32/62,  total step 342/6200, training_loss = 0.3992\n",
      "epoch 6/100, step 33/62,  total step 343/6200, training_loss = 0.5899\n",
      "epoch 6/100, step 34/62,  total step 344/6200, training_loss = 0.3744\n",
      "epoch 6/100, step 35/62,  total step 345/6200, training_loss = 0.4263\n",
      "epoch 6/100, step 36/62,  total step 346/6200, training_loss = 0.5465\n",
      "epoch 6/100, step 37/62,  total step 347/6200, training_loss = 0.5382\n",
      "epoch 6/100, step 38/62,  total step 348/6200, training_loss = 0.5091\n",
      "epoch 6/100, step 39/62,  total step 349/6200, training_loss = 0.5311\n",
      "epoch 6/100, step 40/62,  total step 350/6200, training_loss = 0.4585\n",
      "epoch 6/100, step 41/62,  total step 351/6200, training_loss = 0.4341\n",
      "epoch 6/100, step 42/62,  total step 352/6200, training_loss = 0.4838\n",
      "epoch 6/100, step 43/62,  total step 353/6200, training_loss = 0.3981\n",
      "epoch 6/100, step 44/62,  total step 354/6200, training_loss = 0.4336\n",
      "epoch 6/100, step 45/62,  total step 355/6200, training_loss = 0.3709\n",
      "epoch 6/100, step 46/62,  total step 356/6200, training_loss = 0.3307\n",
      "epoch 6/100, step 47/62,  total step 357/6200, training_loss = 0.4462\n",
      "epoch 6/100, step 48/62,  total step 358/6200, training_loss = 0.3211\n",
      "epoch 6/100, step 49/62,  total step 359/6200, training_loss = 0.3672\n",
      "epoch 6/100, step 50/62,  total step 360/6200, training_loss = 0.4249\n",
      "epoch 6/100, step 51/62,  total step 361/6200, training_loss = 0.3992\n",
      "epoch 6/100, step 52/62,  total step 362/6200, training_loss = 0.3760\n",
      "epoch 6/100, step 53/62,  total step 363/6200, training_loss = 0.2720\n",
      "epoch 6/100, step 54/62,  total step 364/6200, training_loss = 0.3272\n",
      "epoch 6/100, step 55/62,  total step 365/6200, training_loss = 0.3477\n",
      "epoch 6/100, step 56/62,  total step 366/6200, training_loss = 0.4436\n",
      "epoch 6/100, step 57/62,  total step 367/6200, training_loss = 0.4676\n",
      "epoch 6/100, step 58/62,  total step 368/6200, training_loss = 0.5500\n",
      "epoch 6/100, step 59/62,  total step 369/6200, training_loss = 0.2943\n",
      "epoch 6/100, step 60/62,  total step 370/6200, training_loss = 0.4844\n",
      "epoch 6/100, step 61/62,  total step 371/6200, training_loss = 0.3092\n",
      "epoch 6/100, step 62/62,  total step 372/6200, training_loss = 0.5035 | avg loss: 0.4487 Dice Metric:   0.596\n",
      "epoch 7/100, step 1/62,  total step 373/6200, training_loss = 0.4303\n",
      "epoch 7/100, step 2/62,  total step 374/6200, training_loss = 0.4151\n",
      "epoch 7/100, step 3/62,  total step 375/6200, training_loss = 0.4864\n",
      "epoch 7/100, step 4/62,  total step 376/6200, training_loss = 0.5749\n",
      "epoch 7/100, step 5/62,  total step 377/6200, training_loss = 0.2645\n",
      "epoch 7/100, step 6/62,  total step 378/6200, training_loss = 0.4065\n",
      "epoch 7/100, step 7/62,  total step 379/6200, training_loss = 0.3341\n",
      "epoch 7/100, step 8/62,  total step 380/6200, training_loss = 0.4309\n",
      "epoch 7/100, step 9/62,  total step 381/6200, training_loss = 0.3726\n",
      "epoch 7/100, step 10/62,  total step 382/6200, training_loss = 0.3958\n",
      "epoch 7/100, step 11/62,  total step 383/6200, training_loss = 0.2620\n",
      "epoch 7/100, step 12/62,  total step 384/6200, training_loss = 0.3433\n",
      "epoch 7/100, step 13/62,  total step 385/6200, training_loss = 0.4554\n",
      "epoch 7/100, step 14/62,  total step 386/6200, training_loss = 0.4916\n",
      "epoch 7/100, step 15/62,  total step 387/6200, training_loss = 0.6009\n",
      "epoch 7/100, step 16/62,  total step 388/6200, training_loss = 0.5665\n",
      "epoch 7/100, step 17/62,  total step 389/6200, training_loss = 0.4419\n",
      "epoch 7/100, step 18/62,  total step 390/6200, training_loss = 0.3925\n",
      "epoch 7/100, step 19/62,  total step 391/6200, training_loss = 0.4773\n",
      "epoch 7/100, step 20/62,  total step 392/6200, training_loss = 0.4621\n",
      "epoch 7/100, step 21/62,  total step 393/6200, training_loss = 0.4329\n",
      "epoch 7/100, step 22/62,  total step 394/6200, training_loss = 0.5136\n",
      "epoch 7/100, step 23/62,  total step 395/6200, training_loss = 0.4460\n",
      "epoch 7/100, step 24/62,  total step 396/6200, training_loss = 0.3782\n",
      "epoch 7/100, step 25/62,  total step 397/6200, training_loss = 0.3961\n",
      "epoch 7/100, step 26/62,  total step 398/6200, training_loss = 0.3482\n",
      "epoch 7/100, step 27/62,  total step 399/6200, training_loss = 0.3652\n",
      "epoch 7/100, step 28/62,  total step 400/6200, training_loss = 0.5019\n",
      "epoch 7/100, step 29/62,  total step 401/6200, training_loss = 0.4577\n",
      "epoch 7/100, step 30/62,  total step 402/6200, training_loss = 0.4490\n",
      "epoch 7/100, step 31/62,  total step 403/6200, training_loss = 0.3243\n",
      "epoch 7/100, step 32/62,  total step 404/6200, training_loss = 0.3335\n",
      "epoch 7/100, step 33/62,  total step 405/6200, training_loss = 0.5276\n",
      "epoch 7/100, step 34/62,  total step 406/6200, training_loss = 0.3514\n",
      "epoch 7/100, step 35/62,  total step 407/6200, training_loss = 0.3822\n",
      "epoch 7/100, step 36/62,  total step 408/6200, training_loss = 0.4971\n",
      "epoch 7/100, step 37/62,  total step 409/6200, training_loss = 0.5336\n",
      "epoch 7/100, step 38/62,  total step 410/6200, training_loss = 0.4761\n",
      "epoch 7/100, step 39/62,  total step 411/6200, training_loss = 0.4948\n",
      "epoch 7/100, step 40/62,  total step 412/6200, training_loss = 0.4376\n",
      "epoch 7/100, step 41/62,  total step 413/6200, training_loss = 0.3962\n",
      "epoch 7/100, step 42/62,  total step 414/6200, training_loss = 0.4370\n",
      "epoch 7/100, step 43/62,  total step 415/6200, training_loss = 0.3817\n",
      "epoch 7/100, step 44/62,  total step 416/6200, training_loss = 0.3871\n",
      "epoch 7/100, step 45/62,  total step 417/6200, training_loss = 0.3217\n",
      "epoch 7/100, step 46/62,  total step 418/6200, training_loss = 0.2836\n",
      "epoch 7/100, step 47/62,  total step 419/6200, training_loss = 0.4342\n",
      "epoch 7/100, step 48/62,  total step 420/6200, training_loss = 0.2926\n",
      "epoch 7/100, step 49/62,  total step 421/6200, training_loss = 0.3208\n",
      "epoch 7/100, step 50/62,  total step 422/6200, training_loss = 0.4017\n",
      "epoch 7/100, step 51/62,  total step 423/6200, training_loss = 0.3748\n",
      "epoch 7/100, step 52/62,  total step 424/6200, training_loss = 0.3543\n",
      "epoch 7/100, step 53/62,  total step 425/6200, training_loss = 0.2480\n",
      "epoch 7/100, step 54/62,  total step 426/6200, training_loss = 0.2977\n",
      "epoch 7/100, step 55/62,  total step 427/6200, training_loss = 0.3186\n",
      "epoch 7/100, step 56/62,  total step 428/6200, training_loss = 0.4120\n",
      "epoch 7/100, step 57/62,  total step 429/6200, training_loss = 0.4426\n",
      "epoch 7/100, step 58/62,  total step 430/6200, training_loss = 0.5230\n",
      "epoch 7/100, step 59/62,  total step 431/6200, training_loss = 0.2743\n",
      "epoch 7/100, step 60/62,  total step 432/6200, training_loss = 0.4696\n",
      "epoch 7/100, step 61/62,  total step 433/6200, training_loss = 0.2762\n",
      "epoch 7/100, step 62/62,  total step 434/6200, training_loss = 0.4554 | avg loss: 0.4089 Dice Metric:   0.623\n",
      "epoch 8/100, step 1/62,  total step 435/6200, training_loss = 0.4329\n",
      "epoch 8/100, step 2/62,  total step 436/6200, training_loss = 0.4096\n",
      "epoch 8/100, step 3/62,  total step 437/6200, training_loss = 0.4377\n",
      "epoch 8/100, step 4/62,  total step 438/6200, training_loss = 0.5416\n",
      "epoch 8/100, step 5/62,  total step 439/6200, training_loss = 0.2570\n",
      "epoch 8/100, step 6/62,  total step 440/6200, training_loss = 0.3717\n",
      "epoch 8/100, step 7/62,  total step 441/6200, training_loss = 0.3250\n",
      "epoch 8/100, step 8/62,  total step 442/6200, training_loss = 0.4063\n",
      "epoch 8/100, step 9/62,  total step 443/6200, training_loss = 0.4077\n",
      "epoch 8/100, step 10/62,  total step 444/6200, training_loss = 0.3856\n",
      "epoch 8/100, step 11/62,  total step 445/6200, training_loss = 0.2451\n",
      "epoch 8/100, step 12/62,  total step 446/6200, training_loss = 0.3128\n",
      "epoch 8/100, step 13/62,  total step 447/6200, training_loss = 0.4208\n",
      "epoch 8/100, step 14/62,  total step 448/6200, training_loss = 0.4655\n",
      "epoch 8/100, step 15/62,  total step 449/6200, training_loss = 0.5853\n",
      "epoch 8/100, step 16/62,  total step 450/6200, training_loss = 0.5760\n",
      "epoch 8/100, step 17/62,  total step 451/6200, training_loss = 0.4365\n",
      "epoch 8/100, step 18/62,  total step 452/6200, training_loss = 0.3779\n",
      "epoch 8/100, step 19/62,  total step 453/6200, training_loss = 0.4373\n",
      "epoch 8/100, step 20/62,  total step 454/6200, training_loss = 0.4270\n",
      "epoch 8/100, step 21/62,  total step 455/6200, training_loss = 0.4014\n",
      "epoch 8/100, step 22/62,  total step 456/6200, training_loss = 0.4828\n",
      "epoch 8/100, step 23/62,  total step 457/6200, training_loss = 0.4250\n",
      "epoch 8/100, step 24/62,  total step 458/6200, training_loss = 0.3616\n",
      "epoch 8/100, step 25/62,  total step 459/6200, training_loss = 0.3696\n",
      "epoch 8/100, step 26/62,  total step 460/6200, training_loss = 0.2910\n",
      "epoch 8/100, step 27/62,  total step 461/6200, training_loss = 0.3408\n",
      "epoch 8/100, step 28/62,  total step 462/6200, training_loss = 0.5213\n",
      "epoch 8/100, step 29/62,  total step 463/6200, training_loss = 0.4511\n",
      "epoch 8/100, step 30/62,  total step 464/6200, training_loss = 0.4091\n",
      "epoch 8/100, step 31/62,  total step 465/6200, training_loss = 0.3016\n",
      "epoch 8/100, step 32/62,  total step 466/6200, training_loss = 0.3144\n",
      "epoch 8/100, step 33/62,  total step 467/6200, training_loss = 0.5209\n",
      "epoch 8/100, step 34/62,  total step 468/6200, training_loss = 0.3253\n",
      "epoch 8/100, step 35/62,  total step 469/6200, training_loss = 0.3567\n",
      "epoch 8/100, step 36/62,  total step 470/6200, training_loss = 0.4632\n",
      "epoch 8/100, step 37/62,  total step 471/6200, training_loss = 0.4855\n",
      "epoch 8/100, step 38/62,  total step 472/6200, training_loss = 0.4579\n",
      "epoch 8/100, step 39/62,  total step 473/6200, training_loss = 0.4784\n",
      "epoch 8/100, step 40/62,  total step 474/6200, training_loss = 0.4177\n",
      "epoch 8/100, step 41/62,  total step 475/6200, training_loss = 0.3936\n",
      "epoch 8/100, step 42/62,  total step 476/6200, training_loss = 0.4229\n",
      "epoch 8/100, step 43/62,  total step 477/6200, training_loss = 0.3203\n",
      "epoch 8/100, step 44/62,  total step 478/6200, training_loss = 0.3648\n",
      "epoch 8/100, step 45/62,  total step 479/6200, training_loss = 0.2789\n",
      "epoch 8/100, step 46/62,  total step 480/6200, training_loss = 0.2603\n",
      "epoch 8/100, step 47/62,  total step 481/6200, training_loss = 0.4239\n",
      "epoch 8/100, step 48/62,  total step 482/6200, training_loss = 0.3002\n",
      "epoch 8/100, step 49/62,  total step 483/6200, training_loss = 0.2841\n",
      "epoch 8/100, step 50/62,  total step 484/6200, training_loss = 0.3598\n",
      "epoch 8/100, step 51/62,  total step 485/6200, training_loss = 0.3552\n",
      "epoch 8/100, step 52/62,  total step 486/6200, training_loss = 0.3264\n",
      "epoch 8/100, step 53/62,  total step 487/6200, training_loss = 0.2749\n",
      "epoch 8/100, step 54/62,  total step 488/6200, training_loss = 0.3316\n",
      "epoch 8/100, step 55/62,  total step 489/6200, training_loss = 0.3016\n",
      "epoch 8/100, step 56/62,  total step 490/6200, training_loss = 0.3858\n",
      "epoch 8/100, step 57/62,  total step 491/6200, training_loss = 0.4120\n",
      "epoch 8/100, step 58/62,  total step 492/6200, training_loss = 0.5060\n",
      "epoch 8/100, step 59/62,  total step 493/6200, training_loss = 0.2684\n",
      "epoch 8/100, step 60/62,  total step 494/6200, training_loss = 0.4471\n",
      "epoch 8/100, step 61/62,  total step 495/6200, training_loss = 0.2717\n",
      "epoch 8/100, step 62/62,  total step 496/6200, training_loss = 0.4465 | avg loss: 0.3899 Dice Metric:   0.617\n",
      "epoch 9/100, step 1/62,  total step 497/6200, training_loss = 0.3911\n",
      "epoch 9/100, step 2/62,  total step 498/6200, training_loss = 0.3832\n",
      "epoch 9/100, step 3/62,  total step 499/6200, training_loss = 0.4138\n",
      "epoch 9/100, step 4/62,  total step 500/6200, training_loss = 0.5164\n",
      "epoch 9/100, step 5/62,  total step 501/6200, training_loss = 0.2333\n",
      "epoch 9/100, step 6/62,  total step 502/6200, training_loss = 0.3470\n",
      "epoch 9/100, step 7/62,  total step 503/6200, training_loss = 0.3696\n",
      "epoch 9/100, step 8/62,  total step 504/6200, training_loss = 0.3897\n",
      "epoch 9/100, step 9/62,  total step 505/6200, training_loss = 0.4275\n",
      "epoch 9/100, step 10/62,  total step 506/6200, training_loss = 0.3840\n",
      "epoch 9/100, step 11/62,  total step 507/6200, training_loss = 0.2294\n",
      "epoch 9/100, step 12/62,  total step 508/6200, training_loss = 0.3201\n",
      "epoch 9/100, step 13/62,  total step 509/6200, training_loss = 0.4003\n",
      "epoch 9/100, step 14/62,  total step 510/6200, training_loss = 0.4106\n",
      "epoch 9/100, step 15/62,  total step 511/6200, training_loss = 0.5631\n",
      "epoch 9/100, step 16/62,  total step 512/6200, training_loss = 0.5465\n",
      "epoch 9/100, step 17/62,  total step 513/6200, training_loss = 0.4164\n",
      "epoch 9/100, step 18/62,  total step 514/6200, training_loss = 0.3927\n",
      "epoch 9/100, step 19/62,  total step 515/6200, training_loss = 0.4709\n",
      "epoch 9/100, step 20/62,  total step 516/6200, training_loss = 0.4430\n",
      "epoch 9/100, step 21/62,  total step 517/6200, training_loss = 0.3740\n",
      "epoch 9/100, step 22/62,  total step 518/6200, training_loss = 0.5284\n",
      "epoch 9/100, step 23/62,  total step 519/6200, training_loss = 0.4072\n",
      "epoch 9/100, step 24/62,  total step 520/6200, training_loss = 0.3379\n",
      "epoch 9/100, step 25/62,  total step 521/6200, training_loss = 0.3431\n",
      "epoch 9/100, step 26/62,  total step 522/6200, training_loss = 0.3021\n",
      "epoch 9/100, step 27/62,  total step 523/6200, training_loss = 0.3117\n",
      "epoch 9/100, step 28/62,  total step 524/6200, training_loss = 0.5009\n",
      "epoch 9/100, step 29/62,  total step 525/6200, training_loss = 0.4140\n",
      "epoch 9/100, step 30/62,  total step 526/6200, training_loss = 0.3987\n",
      "epoch 9/100, step 31/62,  total step 527/6200, training_loss = 0.2828\n",
      "epoch 9/100, step 32/62,  total step 528/6200, training_loss = 0.3333\n",
      "epoch 9/100, step 33/62,  total step 529/6200, training_loss = 0.4705\n",
      "epoch 9/100, step 34/62,  total step 530/6200, training_loss = 0.3043\n",
      "epoch 9/100, step 35/62,  total step 531/6200, training_loss = 0.3580\n",
      "epoch 9/100, step 36/62,  total step 532/6200, training_loss = 0.4478\n",
      "epoch 9/100, step 37/62,  total step 533/6200, training_loss = 0.4430\n",
      "epoch 9/100, step 38/62,  total step 534/6200, training_loss = 0.4061\n",
      "epoch 9/100, step 39/62,  total step 535/6200, training_loss = 0.4464\n",
      "epoch 9/100, step 40/62,  total step 536/6200, training_loss = 0.3881\n",
      "epoch 9/100, step 41/62,  total step 537/6200, training_loss = 0.3593\n",
      "epoch 9/100, step 42/62,  total step 538/6200, training_loss = 0.3894\n",
      "epoch 9/100, step 43/62,  total step 539/6200, training_loss = 0.3302\n",
      "epoch 9/100, step 44/62,  total step 540/6200, training_loss = 0.3422\n",
      "epoch 9/100, step 45/62,  total step 541/6200, training_loss = 0.2668\n",
      "epoch 9/100, step 46/62,  total step 542/6200, training_loss = 0.2545\n",
      "epoch 9/100, step 47/62,  total step 543/6200, training_loss = 0.4008\n",
      "epoch 9/100, step 48/62,  total step 544/6200, training_loss = 0.2500\n",
      "epoch 9/100, step 49/62,  total step 545/6200, training_loss = 0.2764\n",
      "epoch 9/100, step 50/62,  total step 546/6200, training_loss = 0.3637\n",
      "epoch 9/100, step 51/62,  total step 547/6200, training_loss = 0.3105\n",
      "epoch 9/100, step 52/62,  total step 548/6200, training_loss = 0.2905\n",
      "epoch 9/100, step 53/62,  total step 549/6200, training_loss = 0.2338\n",
      "epoch 9/100, step 54/62,  total step 550/6200, training_loss = 0.2809\n",
      "epoch 9/100, step 55/62,  total step 551/6200, training_loss = 0.2847\n",
      "epoch 9/100, step 56/62,  total step 552/6200, training_loss = 0.3832\n",
      "epoch 9/100, step 57/62,  total step 553/6200, training_loss = 0.3981\n",
      "epoch 9/100, step 58/62,  total step 554/6200, training_loss = 0.4828\n",
      "epoch 9/100, step 59/62,  total step 555/6200, training_loss = 0.2749\n",
      "epoch 9/100, step 60/62,  total step 556/6200, training_loss = 0.4126\n",
      "epoch 9/100, step 61/62,  total step 557/6200, training_loss = 0.2405\n",
      "epoch 9/100, step 62/62,  total step 558/6200, training_loss = 0.4039 | avg loss: 0.3722 Dice Metric:   0.617\n",
      "epoch 10/100, step 1/62,  total step 559/6200, training_loss = 0.3651\n",
      "epoch 10/100, step 2/62,  total step 560/6200, training_loss = 0.3360\n",
      "epoch 10/100, step 3/62,  total step 561/6200, training_loss = 0.3842\n",
      "epoch 10/100, step 4/62,  total step 562/6200, training_loss = 0.5125\n",
      "epoch 10/100, step 5/62,  total step 563/6200, training_loss = 0.2258\n",
      "epoch 10/100, step 6/62,  total step 564/6200, training_loss = 0.3195\n",
      "epoch 10/100, step 7/62,  total step 565/6200, training_loss = 0.2753\n",
      "epoch 10/100, step 8/62,  total step 566/6200, training_loss = 0.3799\n",
      "epoch 10/100, step 9/62,  total step 567/6200, training_loss = 0.3666\n",
      "epoch 10/100, step 10/62,  total step 568/6200, training_loss = 0.3561\n",
      "epoch 10/100, step 11/62,  total step 569/6200, training_loss = 0.2213\n",
      "epoch 10/100, step 12/62,  total step 570/6200, training_loss = 0.3010\n",
      "epoch 10/100, step 13/62,  total step 571/6200, training_loss = 0.3848\n",
      "epoch 10/100, step 14/62,  total step 572/6200, training_loss = 0.3726\n",
      "epoch 10/100, step 15/62,  total step 573/6200, training_loss = 0.5331\n",
      "epoch 10/100, step 16/62,  total step 574/6200, training_loss = 0.5024\n",
      "epoch 10/100, step 17/62,  total step 575/6200, training_loss = 0.3999\n",
      "epoch 10/100, step 18/62,  total step 576/6200, training_loss = 0.3647\n",
      "epoch 10/100, step 19/62,  total step 577/6200, training_loss = 0.4165\n",
      "epoch 10/100, step 20/62,  total step 578/6200, training_loss = 0.3803\n",
      "epoch 10/100, step 21/62,  total step 579/6200, training_loss = 0.3673\n",
      "epoch 10/100, step 22/62,  total step 580/6200, training_loss = 0.4791\n",
      "epoch 10/100, step 23/62,  total step 581/6200, training_loss = 0.3759\n",
      "epoch 10/100, step 24/62,  total step 582/6200, training_loss = 0.3182\n",
      "epoch 10/100, step 25/62,  total step 583/6200, training_loss = 0.3358\n",
      "epoch 10/100, step 26/62,  total step 584/6200, training_loss = 0.2797\n",
      "epoch 10/100, step 27/62,  total step 585/6200, training_loss = 0.3403\n",
      "epoch 10/100, step 28/62,  total step 586/6200, training_loss = 0.5014\n",
      "epoch 10/100, step 29/62,  total step 587/6200, training_loss = 0.4087\n",
      "epoch 10/100, step 30/62,  total step 588/6200, training_loss = 0.3913\n",
      "epoch 10/100, step 31/62,  total step 589/6200, training_loss = 0.2664\n",
      "epoch 10/100, step 32/62,  total step 590/6200, training_loss = 0.3119\n",
      "epoch 10/100, step 33/62,  total step 591/6200, training_loss = 0.4630\n",
      "epoch 10/100, step 34/62,  total step 592/6200, training_loss = 0.3080\n",
      "epoch 10/100, step 35/62,  total step 593/6200, training_loss = 0.3317\n",
      "epoch 10/100, step 36/62,  total step 594/6200, training_loss = 0.4381\n",
      "epoch 10/100, step 37/62,  total step 595/6200, training_loss = 0.3655\n",
      "epoch 10/100, step 38/62,  total step 596/6200, training_loss = 0.3840\n",
      "epoch 10/100, step 39/62,  total step 597/6200, training_loss = 0.4153\n",
      "epoch 10/100, step 40/62,  total step 598/6200, training_loss = 0.3633\n",
      "epoch 10/100, step 41/62,  total step 599/6200, training_loss = 0.3313\n",
      "epoch 10/100, step 42/62,  total step 600/6200, training_loss = 0.3841\n",
      "epoch 10/100, step 43/62,  total step 601/6200, training_loss = 0.3057\n",
      "epoch 10/100, step 44/62,  total step 602/6200, training_loss = 0.3548\n",
      "epoch 10/100, step 45/62,  total step 603/6200, training_loss = 0.2633\n",
      "epoch 10/100, step 46/62,  total step 604/6200, training_loss = 0.2523\n",
      "epoch 10/100, step 47/62,  total step 605/6200, training_loss = 0.3905\n",
      "epoch 10/100, step 48/62,  total step 606/6200, training_loss = 0.3080\n",
      "epoch 10/100, step 49/62,  total step 607/6200, training_loss = 0.2442\n",
      "epoch 10/100, step 50/62,  total step 608/6200, training_loss = 0.3104\n",
      "epoch 10/100, step 51/62,  total step 609/6200, training_loss = 0.3296\n",
      "epoch 10/100, step 52/62,  total step 610/6200, training_loss = 0.2993\n",
      "epoch 10/100, step 53/62,  total step 611/6200, training_loss = 0.2397\n",
      "epoch 10/100, step 54/62,  total step 612/6200, training_loss = 0.3221\n",
      "epoch 10/100, step 55/62,  total step 613/6200, training_loss = 0.2637\n",
      "epoch 10/100, step 56/62,  total step 614/6200, training_loss = 0.3689\n",
      "epoch 10/100, step 57/62,  total step 615/6200, training_loss = 0.4202\n",
      "epoch 10/100, step 58/62,  total step 616/6200, training_loss = 0.4897\n",
      "epoch 10/100, step 59/62,  total step 617/6200, training_loss = 0.2572\n",
      "epoch 10/100, step 60/62,  total step 618/6200, training_loss = 0.4427\n",
      "epoch 10/100, step 61/62,  total step 619/6200, training_loss = 0.2453\n",
      "epoch 10/100, step 62/62,  total step 620/6200, training_loss = 0.4244 | avg loss: 0.3563 Dice Metric:   0.633\n",
      "epoch 11/100, step 1/62,  total step 621/6200, training_loss = 0.3529\n",
      "epoch 11/100, step 2/62,  total step 622/6200, training_loss = 0.3632\n",
      "epoch 11/100, step 3/62,  total step 623/6200, training_loss = 0.4116\n",
      "epoch 11/100, step 4/62,  total step 624/6200, training_loss = 0.4905\n",
      "epoch 11/100, step 5/62,  total step 625/6200, training_loss = 0.2333\n",
      "epoch 11/100, step 6/62,  total step 626/6200, training_loss = 0.3254\n",
      "epoch 11/100, step 7/62,  total step 627/6200, training_loss = 0.3213\n",
      "epoch 11/100, step 8/62,  total step 628/6200, training_loss = 0.3603\n",
      "epoch 11/100, step 9/62,  total step 629/6200, training_loss = 0.3318\n",
      "epoch 11/100, step 10/62,  total step 630/6200, training_loss = 0.3427\n",
      "epoch 11/100, step 11/62,  total step 631/6200, training_loss = 0.2128\n",
      "epoch 11/100, step 12/62,  total step 632/6200, training_loss = 0.3149\n",
      "epoch 11/100, step 13/62,  total step 633/6200, training_loss = 0.3879\n",
      "epoch 11/100, step 14/62,  total step 634/6200, training_loss = 0.4278\n",
      "epoch 11/100, step 15/62,  total step 635/6200, training_loss = 0.5760\n",
      "epoch 11/100, step 16/62,  total step 636/6200, training_loss = 0.5268\n",
      "epoch 11/100, step 17/62,  total step 637/6200, training_loss = 0.3629\n",
      "epoch 11/100, step 18/62,  total step 638/6200, training_loss = 0.3360\n",
      "epoch 11/100, step 19/62,  total step 639/6200, training_loss = 0.4381\n",
      "epoch 11/100, step 20/62,  total step 640/6200, training_loss = 0.3805\n",
      "epoch 11/100, step 21/62,  total step 641/6200, training_loss = 0.3720\n",
      "epoch 11/100, step 22/62,  total step 642/6200, training_loss = 0.4890\n",
      "epoch 11/100, step 23/62,  total step 643/6200, training_loss = 0.3867\n",
      "epoch 11/100, step 24/62,  total step 644/6200, training_loss = 0.3273\n",
      "epoch 11/100, step 25/62,  total step 645/6200, training_loss = 0.3202\n",
      "epoch 11/100, step 26/62,  total step 646/6200, training_loss = 0.2965\n",
      "epoch 11/100, step 27/62,  total step 647/6200, training_loss = 0.3009\n",
      "epoch 11/100, step 28/62,  total step 648/6200, training_loss = 0.4668\n",
      "epoch 11/100, step 29/62,  total step 649/6200, training_loss = 0.4032\n",
      "epoch 11/100, step 30/62,  total step 650/6200, training_loss = 0.3624\n",
      "epoch 11/100, step 31/62,  total step 651/6200, training_loss = 0.2828\n",
      "epoch 11/100, step 32/62,  total step 652/6200, training_loss = 0.3033\n",
      "epoch 11/100, step 33/62,  total step 653/6200, training_loss = 0.4396\n",
      "epoch 11/100, step 34/62,  total step 654/6200, training_loss = 0.2856\n",
      "epoch 11/100, step 35/62,  total step 655/6200, training_loss = 0.3813\n",
      "epoch 11/100, step 36/62,  total step 656/6200, training_loss = 0.4290\n",
      "epoch 11/100, step 37/62,  total step 657/6200, training_loss = 0.4578\n",
      "epoch 11/100, step 38/62,  total step 658/6200, training_loss = 0.4033\n",
      "epoch 11/100, step 39/62,  total step 659/6200, training_loss = 0.4027\n",
      "epoch 11/100, step 40/62,  total step 660/6200, training_loss = 0.3789\n",
      "epoch 11/100, step 41/62,  total step 661/6200, training_loss = 0.3203\n",
      "epoch 11/100, step 42/62,  total step 662/6200, training_loss = 0.4024\n",
      "epoch 11/100, step 43/62,  total step 663/6200, training_loss = 0.2951\n",
      "epoch 11/100, step 44/62,  total step 664/6200, training_loss = 0.3112\n",
      "epoch 11/100, step 45/62,  total step 665/6200, training_loss = 0.2417\n",
      "epoch 11/100, step 46/62,  total step 666/6200, training_loss = 0.2381\n",
      "epoch 11/100, step 47/62,  total step 667/6200, training_loss = 0.3917\n",
      "epoch 11/100, step 48/62,  total step 668/6200, training_loss = 0.2585\n",
      "epoch 11/100, step 49/62,  total step 669/6200, training_loss = 0.2557\n",
      "epoch 11/100, step 50/62,  total step 670/6200, training_loss = 0.3196\n",
      "epoch 11/100, step 51/62,  total step 671/6200, training_loss = 0.2892\n",
      "epoch 11/100, step 52/62,  total step 672/6200, training_loss = 0.3012\n",
      "epoch 11/100, step 53/62,  total step 673/6200, training_loss = 0.2496\n",
      "epoch 11/100, step 54/62,  total step 674/6200, training_loss = 0.2482\n",
      "epoch 11/100, step 55/62,  total step 675/6200, training_loss = 0.2467\n",
      "epoch 11/100, step 56/62,  total step 676/6200, training_loss = 0.3552\n",
      "epoch 11/100, step 57/62,  total step 677/6200, training_loss = 0.4053\n",
      "epoch 11/100, step 58/62,  total step 678/6200, training_loss = 0.4754\n",
      "epoch 11/100, step 59/62,  total step 679/6200, training_loss = 0.2404\n",
      "epoch 11/100, step 60/62,  total step 680/6200, training_loss = 0.4298\n",
      "epoch 11/100, step 61/62,  total step 681/6200, training_loss = 0.2234\n",
      "epoch 11/100, step 62/62,  total step 682/6200, training_loss = 0.3980 | avg loss: 0.3529 Dice Metric:   0.648\n",
      "epoch 12/100, step 1/62,  total step 683/6200, training_loss = 0.3831\n",
      "epoch 12/100, step 2/62,  total step 684/6200, training_loss = 0.3303\n",
      "epoch 12/100, step 3/62,  total step 685/6200, training_loss = 0.3696\n",
      "epoch 12/100, step 4/62,  total step 686/6200, training_loss = 0.5037\n",
      "epoch 12/100, step 5/62,  total step 687/6200, training_loss = 0.1974\n",
      "epoch 12/100, step 6/62,  total step 688/6200, training_loss = 0.3003\n",
      "epoch 12/100, step 7/62,  total step 689/6200, training_loss = 0.2876\n",
      "epoch 12/100, step 8/62,  total step 690/6200, training_loss = 0.3270\n",
      "epoch 12/100, step 9/62,  total step 691/6200, training_loss = 0.3316\n",
      "epoch 12/100, step 10/62,  total step 692/6200, training_loss = 0.3450\n",
      "epoch 12/100, step 11/62,  total step 693/6200, training_loss = 0.2397\n",
      "epoch 12/100, step 12/62,  total step 694/6200, training_loss = 0.2848\n",
      "epoch 12/100, step 13/62,  total step 695/6200, training_loss = 0.3476\n",
      "epoch 12/100, step 14/62,  total step 696/6200, training_loss = 0.3581\n",
      "epoch 12/100, step 15/62,  total step 697/6200, training_loss = 0.5160\n",
      "epoch 12/100, step 16/62,  total step 698/6200, training_loss = 0.4991\n",
      "epoch 12/100, step 17/62,  total step 699/6200, training_loss = 0.3517\n",
      "epoch 12/100, step 18/62,  total step 700/6200, training_loss = 0.3187\n",
      "epoch 12/100, step 19/62,  total step 701/6200, training_loss = 0.4013\n",
      "epoch 12/100, step 20/62,  total step 702/6200, training_loss = 0.3501\n",
      "epoch 12/100, step 21/62,  total step 703/6200, training_loss = 0.3253\n",
      "epoch 12/100, step 22/62,  total step 704/6200, training_loss = 0.4913\n",
      "epoch 12/100, step 23/62,  total step 705/6200, training_loss = 0.3479\n",
      "epoch 12/100, step 24/62,  total step 706/6200, training_loss = 0.2891\n",
      "epoch 12/100, step 25/62,  total step 707/6200, training_loss = 0.3097\n",
      "epoch 12/100, step 26/62,  total step 708/6200, training_loss = 0.2650\n",
      "epoch 12/100, step 27/62,  total step 709/6200, training_loss = 0.3261\n",
      "epoch 12/100, step 28/62,  total step 710/6200, training_loss = 0.4345\n",
      "epoch 12/100, step 29/62,  total step 711/6200, training_loss = 0.3734\n",
      "epoch 12/100, step 30/62,  total step 712/6200, training_loss = 0.3814\n",
      "epoch 12/100, step 31/62,  total step 713/6200, training_loss = 0.2795\n",
      "epoch 12/100, step 32/62,  total step 714/6200, training_loss = 0.3027\n",
      "epoch 12/100, step 33/62,  total step 715/6200, training_loss = 0.4510\n",
      "epoch 12/100, step 34/62,  total step 716/6200, training_loss = 0.2844\n",
      "epoch 12/100, step 35/62,  total step 717/6200, training_loss = 0.3472\n",
      "epoch 12/100, step 36/62,  total step 718/6200, training_loss = 0.4088\n",
      "epoch 12/100, step 37/62,  total step 719/6200, training_loss = 0.3675\n",
      "epoch 12/100, step 38/62,  total step 720/6200, training_loss = 0.3975\n",
      "epoch 12/100, step 39/62,  total step 721/6200, training_loss = 0.4000\n",
      "epoch 12/100, step 40/62,  total step 722/6200, training_loss = 0.3586\n",
      "epoch 12/100, step 41/62,  total step 723/6200, training_loss = 0.3130\n",
      "epoch 12/100, step 42/62,  total step 724/6200, training_loss = 0.3352\n",
      "epoch 12/100, step 43/62,  total step 725/6200, training_loss = 0.2689\n",
      "epoch 12/100, step 44/62,  total step 726/6200, training_loss = 0.3181\n",
      "epoch 12/100, step 45/62,  total step 727/6200, training_loss = 0.2210\n",
      "epoch 12/100, step 46/62,  total step 728/6200, training_loss = 0.2319\n",
      "epoch 12/100, step 47/62,  total step 729/6200, training_loss = 0.3956\n",
      "epoch 12/100, step 48/62,  total step 730/6200, training_loss = 0.2452\n",
      "epoch 12/100, step 49/62,  total step 731/6200, training_loss = 0.2344\n",
      "epoch 12/100, step 50/62,  total step 732/6200, training_loss = 0.3087\n",
      "epoch 12/100, step 51/62,  total step 733/6200, training_loss = 0.2664\n",
      "epoch 12/100, step 52/62,  total step 734/6200, training_loss = 0.2671\n",
      "epoch 12/100, step 53/62,  total step 735/6200, training_loss = 0.2451\n",
      "epoch 12/100, step 54/62,  total step 736/6200, training_loss = 0.2716\n",
      "epoch 12/100, step 55/62,  total step 737/6200, training_loss = 0.2542\n",
      "epoch 12/100, step 56/62,  total step 738/6200, training_loss = 0.3308\n",
      "epoch 12/100, step 57/62,  total step 739/6200, training_loss = 0.3466\n",
      "epoch 12/100, step 58/62,  total step 740/6200, training_loss = 0.4643\n",
      "epoch 12/100, step 59/62,  total step 741/6200, training_loss = 0.2199\n",
      "epoch 12/100, step 60/62,  total step 742/6200, training_loss = 0.4222\n",
      "epoch 12/100, step 61/62,  total step 743/6200, training_loss = 0.2156\n",
      "epoch 12/100, step 62/62,  total step 744/6200, training_loss = 0.4448 | avg loss: 0.3356 Dice Metric:   0.652\n",
      "epoch 13/100, step 1/62,  total step 745/6200, training_loss = 0.3441\n",
      "epoch 13/100, step 2/62,  total step 746/6200, training_loss = 0.3167\n",
      "epoch 13/100, step 3/62,  total step 747/6200, training_loss = 0.3738\n",
      "epoch 13/100, step 4/62,  total step 748/6200, training_loss = 0.4728\n",
      "epoch 13/100, step 5/62,  total step 749/6200, training_loss = 0.2007\n",
      "epoch 13/100, step 6/62,  total step 750/6200, training_loss = 0.2930\n",
      "epoch 13/100, step 7/62,  total step 751/6200, training_loss = 0.2736\n",
      "epoch 13/100, step 8/62,  total step 752/6200, training_loss = 0.3281\n",
      "epoch 13/100, step 9/62,  total step 753/6200, training_loss = 0.3100\n",
      "epoch 13/100, step 10/62,  total step 754/6200, training_loss = 0.3040\n",
      "epoch 13/100, step 11/62,  total step 755/6200, training_loss = 0.2103\n",
      "epoch 13/100, step 12/62,  total step 756/6200, training_loss = 0.2612\n",
      "epoch 13/100, step 13/62,  total step 757/6200, training_loss = 0.3415\n",
      "epoch 13/100, step 14/62,  total step 758/6200, training_loss = 0.3631\n",
      "epoch 13/100, step 15/62,  total step 759/6200, training_loss = 0.5196\n",
      "epoch 13/100, step 16/62,  total step 760/6200, training_loss = 0.4624\n",
      "epoch 13/100, step 17/62,  total step 761/6200, training_loss = 0.2950\n",
      "epoch 13/100, step 18/62,  total step 762/6200, training_loss = 0.3076\n",
      "epoch 13/100, step 19/62,  total step 763/6200, training_loss = 0.3715\n",
      "epoch 13/100, step 20/62,  total step 764/6200, training_loss = 0.3617\n",
      "epoch 13/100, step 21/62,  total step 765/6200, training_loss = 0.3297\n",
      "epoch 13/100, step 22/62,  total step 766/6200, training_loss = 0.4311\n",
      "epoch 13/100, step 23/62,  total step 767/6200, training_loss = 0.3274\n",
      "epoch 13/100, step 24/62,  total step 768/6200, training_loss = 0.2881\n",
      "epoch 13/100, step 25/62,  total step 769/6200, training_loss = 0.3059\n",
      "epoch 13/100, step 26/62,  total step 770/6200, training_loss = 0.2525\n",
      "epoch 13/100, step 27/62,  total step 771/6200, training_loss = 0.2867\n",
      "epoch 13/100, step 28/62,  total step 772/6200, training_loss = 0.3900\n",
      "epoch 13/100, step 29/62,  total step 773/6200, training_loss = 0.3550\n",
      "epoch 13/100, step 30/62,  total step 774/6200, training_loss = 0.3417\n",
      "epoch 13/100, step 31/62,  total step 775/6200, training_loss = 0.2452\n",
      "epoch 13/100, step 32/62,  total step 776/6200, training_loss = 0.3102\n",
      "epoch 13/100, step 33/62,  total step 777/6200, training_loss = 0.4414\n",
      "epoch 13/100, step 34/62,  total step 778/6200, training_loss = 0.2701\n",
      "epoch 13/100, step 35/62,  total step 779/6200, training_loss = 0.3484\n",
      "epoch 13/100, step 36/62,  total step 780/6200, training_loss = 0.4191\n",
      "epoch 13/100, step 37/62,  total step 781/6200, training_loss = 0.3583\n",
      "epoch 13/100, step 38/62,  total step 782/6200, training_loss = 0.3683\n",
      "epoch 13/100, step 39/62,  total step 783/6200, training_loss = 0.3795\n",
      "epoch 13/100, step 40/62,  total step 784/6200, training_loss = 0.3537\n",
      "epoch 13/100, step 41/62,  total step 785/6200, training_loss = 0.2621\n",
      "epoch 13/100, step 42/62,  total step 786/6200, training_loss = 0.3151\n",
      "epoch 13/100, step 43/62,  total step 787/6200, training_loss = 0.2474\n",
      "epoch 13/100, step 44/62,  total step 788/6200, training_loss = 0.2824\n",
      "epoch 13/100, step 45/62,  total step 789/6200, training_loss = 0.2166\n",
      "epoch 13/100, step 46/62,  total step 790/6200, training_loss = 0.2410\n",
      "epoch 13/100, step 47/62,  total step 791/6200, training_loss = 0.3469\n",
      "epoch 13/100, step 48/62,  total step 792/6200, training_loss = 0.2315\n",
      "epoch 13/100, step 49/62,  total step 793/6200, training_loss = 0.2396\n",
      "epoch 13/100, step 50/62,  total step 794/6200, training_loss = 0.3203\n",
      "epoch 13/100, step 51/62,  total step 795/6200, training_loss = 0.2797\n",
      "epoch 13/100, step 52/62,  total step 796/6200, training_loss = 0.2562\n",
      "epoch 13/100, step 53/62,  total step 797/6200, training_loss = 0.2605\n",
      "epoch 13/100, step 54/62,  total step 798/6200, training_loss = 0.2284\n",
      "epoch 13/100, step 55/62,  total step 799/6200, training_loss = 0.2489\n",
      "epoch 13/100, step 56/62,  total step 800/6200, training_loss = 0.3315\n",
      "epoch 13/100, step 57/62,  total step 801/6200, training_loss = 0.3415\n",
      "epoch 13/100, step 58/62,  total step 802/6200, training_loss = 0.4481\n",
      "epoch 13/100, step 59/62,  total step 803/6200, training_loss = 0.2278\n",
      "epoch 13/100, step 60/62,  total step 804/6200, training_loss = 0.3886\n",
      "epoch 13/100, step 61/62,  total step 805/6200, training_loss = 0.2094\n",
      "epoch 13/100, step 62/62,  total step 806/6200, training_loss = 0.3615 | avg loss: 0.3193 Dice Metric:   0.634\n",
      "epoch 14/100, step 1/62,  total step 807/6200, training_loss = 0.3156\n",
      "epoch 14/100, step 2/62,  total step 808/6200, training_loss = 0.2954\n",
      "epoch 14/100, step 3/62,  total step 809/6200, training_loss = 0.3478\n",
      "epoch 14/100, step 4/62,  total step 810/6200, training_loss = 0.4529\n",
      "epoch 14/100, step 5/62,  total step 811/6200, training_loss = 0.1999\n",
      "epoch 14/100, step 6/62,  total step 812/6200, training_loss = 0.2726\n",
      "epoch 14/100, step 7/62,  total step 813/6200, training_loss = 0.2393\n",
      "epoch 14/100, step 8/62,  total step 814/6200, training_loss = 0.3197\n",
      "epoch 14/100, step 9/62,  total step 815/6200, training_loss = 0.2933\n",
      "epoch 14/100, step 10/62,  total step 816/6200, training_loss = 0.3313\n",
      "epoch 14/100, step 11/62,  total step 817/6200, training_loss = 0.2144\n",
      "epoch 14/100, step 12/62,  total step 818/6200, training_loss = 0.3044\n",
      "epoch 14/100, step 13/62,  total step 819/6200, training_loss = 0.3407\n",
      "epoch 14/100, step 14/62,  total step 820/6200, training_loss = 0.3531\n",
      "epoch 14/100, step 15/62,  total step 821/6200, training_loss = 0.5047\n",
      "epoch 14/100, step 16/62,  total step 822/6200, training_loss = 0.4703\n",
      "epoch 14/100, step 17/62,  total step 823/6200, training_loss = 0.2950\n",
      "epoch 14/100, step 18/62,  total step 824/6200, training_loss = 0.2936\n",
      "epoch 14/100, step 19/62,  total step 825/6200, training_loss = 0.3729\n",
      "epoch 14/100, step 20/62,  total step 826/6200, training_loss = 0.3378\n",
      "epoch 14/100, step 21/62,  total step 827/6200, training_loss = 0.3154\n",
      "epoch 14/100, step 22/62,  total step 828/6200, training_loss = 0.4497\n",
      "epoch 14/100, step 23/62,  total step 829/6200, training_loss = 0.3301\n",
      "epoch 14/100, step 24/62,  total step 830/6200, training_loss = 0.2602\n",
      "epoch 14/100, step 25/62,  total step 831/6200, training_loss = 0.2873\n",
      "epoch 14/100, step 26/62,  total step 832/6200, training_loss = 0.2413\n",
      "epoch 14/100, step 27/62,  total step 833/6200, training_loss = 0.2821\n",
      "epoch 14/100, step 28/62,  total step 834/6200, training_loss = 0.4236\n",
      "epoch 14/100, step 29/62,  total step 835/6200, training_loss = 0.3530\n",
      "epoch 14/100, step 30/62,  total step 836/6200, training_loss = 0.3380\n",
      "epoch 14/100, step 31/62,  total step 837/6200, training_loss = 0.2578\n",
      "epoch 14/100, step 32/62,  total step 838/6200, training_loss = 0.2913\n",
      "epoch 14/100, step 33/62,  total step 839/6200, training_loss = 0.3959\n",
      "epoch 14/100, step 34/62,  total step 840/6200, training_loss = 0.2762\n",
      "epoch 14/100, step 35/62,  total step 841/6200, training_loss = 0.3282\n",
      "epoch 14/100, step 36/62,  total step 842/6200, training_loss = 0.3980\n",
      "epoch 14/100, step 37/62,  total step 843/6200, training_loss = 0.3411\n",
      "epoch 14/100, step 38/62,  total step 844/6200, training_loss = 0.3559\n",
      "epoch 14/100, step 39/62,  total step 845/6200, training_loss = 0.3625\n",
      "epoch 14/100, step 40/62,  total step 846/6200, training_loss = 0.3310\n",
      "epoch 14/100, step 41/62,  total step 847/6200, training_loss = 0.2684\n",
      "epoch 14/100, step 42/62,  total step 848/6200, training_loss = 0.3174\n",
      "epoch 14/100, step 43/62,  total step 849/6200, training_loss = 0.2718\n",
      "epoch 14/100, step 44/62,  total step 850/6200, training_loss = 0.3099\n",
      "epoch 14/100, step 45/62,  total step 851/6200, training_loss = 0.2136\n",
      "epoch 14/100, step 46/62,  total step 852/6200, training_loss = 0.2457\n",
      "epoch 14/100, step 47/62,  total step 853/6200, training_loss = 0.3368\n",
      "epoch 14/100, step 48/62,  total step 854/6200, training_loss = 0.2188\n",
      "epoch 14/100, step 49/62,  total step 855/6200, training_loss = 0.2329\n",
      "epoch 14/100, step 50/62,  total step 856/6200, training_loss = 0.3113\n",
      "epoch 14/100, step 51/62,  total step 857/6200, training_loss = 0.2782\n",
      "epoch 14/100, step 52/62,  total step 858/6200, training_loss = 0.2490\n",
      "epoch 14/100, step 53/62,  total step 859/6200, training_loss = 0.2673\n",
      "epoch 14/100, step 54/62,  total step 860/6200, training_loss = 0.2573\n",
      "epoch 14/100, step 55/62,  total step 861/6200, training_loss = 0.2449\n",
      "epoch 14/100, step 56/62,  total step 862/6200, training_loss = 0.3224\n",
      "epoch 14/100, step 57/62,  total step 863/6200, training_loss = 0.3242\n",
      "epoch 14/100, step 58/62,  total step 864/6200, training_loss = 0.4262\n",
      "epoch 14/100, step 59/62,  total step 865/6200, training_loss = 0.2224\n",
      "epoch 14/100, step 60/62,  total step 866/6200, training_loss = 0.3956\n",
      "epoch 14/100, step 61/62,  total step 867/6200, training_loss = 0.2153\n",
      "epoch 14/100, step 62/62,  total step 868/6200, training_loss = 0.4053 | avg loss: 0.3146 Dice Metric:   0.664\n",
      "epoch 15/100, step 1/62,  total step 869/6200, training_loss = 0.3356\n",
      "epoch 15/100, step 2/62,  total step 870/6200, training_loss = 0.2819\n",
      "epoch 15/100, step 3/62,  total step 871/6200, training_loss = 0.3596\n",
      "epoch 15/100, step 4/62,  total step 872/6200, training_loss = 0.4695\n",
      "epoch 15/100, step 5/62,  total step 873/6200, training_loss = 0.1932\n",
      "epoch 15/100, step 6/62,  total step 874/6200, training_loss = 0.2724\n",
      "epoch 15/100, step 7/62,  total step 875/6200, training_loss = 0.2597\n",
      "epoch 15/100, step 8/62,  total step 876/6200, training_loss = 0.3413\n",
      "epoch 15/100, step 9/62,  total step 877/6200, training_loss = 0.2863\n",
      "epoch 15/100, step 10/62,  total step 878/6200, training_loss = 0.3347\n",
      "epoch 15/100, step 11/62,  total step 879/6200, training_loss = 0.2295\n",
      "epoch 15/100, step 12/62,  total step 880/6200, training_loss = 0.2699\n",
      "epoch 15/100, step 13/62,  total step 881/6200, training_loss = 0.3472\n",
      "epoch 15/100, step 14/62,  total step 882/6200, training_loss = 0.3280\n",
      "epoch 15/100, step 15/62,  total step 883/6200, training_loss = 0.5060\n",
      "epoch 15/100, step 16/62,  total step 884/6200, training_loss = 0.4415\n",
      "epoch 15/100, step 17/62,  total step 885/6200, training_loss = 0.3004\n",
      "epoch 15/100, step 18/62,  total step 886/6200, training_loss = 0.3115\n",
      "epoch 15/100, step 19/62,  total step 887/6200, training_loss = 0.3697\n",
      "epoch 15/100, step 20/62,  total step 888/6200, training_loss = 0.3851\n",
      "epoch 15/100, step 21/62,  total step 889/6200, training_loss = 0.3154\n",
      "epoch 15/100, step 22/62,  total step 890/6200, training_loss = 0.4388\n",
      "epoch 15/100, step 23/62,  total step 891/6200, training_loss = 0.3402\n",
      "epoch 15/100, step 24/62,  total step 892/6200, training_loss = 0.2747\n",
      "epoch 15/100, step 25/62,  total step 893/6200, training_loss = 0.2798\n",
      "epoch 15/100, step 26/62,  total step 894/6200, training_loss = 0.2202\n",
      "epoch 15/100, step 27/62,  total step 895/6200, training_loss = 0.2884\n",
      "epoch 15/100, step 28/62,  total step 896/6200, training_loss = 0.4227\n",
      "epoch 15/100, step 29/62,  total step 897/6200, training_loss = 0.3656\n",
      "epoch 15/100, step 30/62,  total step 898/6200, training_loss = 0.3365\n",
      "epoch 15/100, step 31/62,  total step 899/6200, training_loss = 0.2329\n",
      "epoch 15/100, step 32/62,  total step 900/6200, training_loss = 0.2741\n",
      "epoch 15/100, step 33/62,  total step 901/6200, training_loss = 0.3764\n",
      "epoch 15/100, step 34/62,  total step 902/6200, training_loss = 0.2664\n",
      "epoch 15/100, step 35/62,  total step 903/6200, training_loss = 0.3255\n",
      "epoch 15/100, step 36/62,  total step 904/6200, training_loss = 0.3759\n",
      "epoch 15/100, step 37/62,  total step 905/6200, training_loss = 0.3235\n",
      "epoch 15/100, step 38/62,  total step 906/6200, training_loss = 0.3500\n",
      "epoch 15/100, step 39/62,  total step 907/6200, training_loss = 0.3494\n",
      "epoch 15/100, step 40/62,  total step 908/6200, training_loss = 0.3423\n",
      "epoch 15/100, step 41/62,  total step 909/6200, training_loss = 0.2589\n",
      "epoch 15/100, step 42/62,  total step 910/6200, training_loss = 0.3071\n",
      "epoch 15/100, step 43/62,  total step 911/6200, training_loss = 0.2667\n",
      "epoch 15/100, step 44/62,  total step 912/6200, training_loss = 0.2877\n",
      "epoch 15/100, step 45/62,  total step 913/6200, training_loss = 0.2056\n",
      "epoch 15/100, step 46/62,  total step 914/6200, training_loss = 0.2215\n",
      "epoch 15/100, step 47/62,  total step 915/6200, training_loss = 0.3503\n",
      "epoch 15/100, step 48/62,  total step 916/6200, training_loss = 0.2230\n",
      "epoch 15/100, step 49/62,  total step 917/6200, training_loss = 0.2148\n",
      "epoch 15/100, step 50/62,  total step 918/6200, training_loss = 0.3173\n",
      "epoch 15/100, step 51/62,  total step 919/6200, training_loss = 0.2655\n",
      "epoch 15/100, step 52/62,  total step 920/6200, training_loss = 0.2543\n",
      "epoch 15/100, step 53/62,  total step 921/6200, training_loss = 0.2913\n",
      "epoch 15/100, step 54/62,  total step 922/6200, training_loss = 0.2761\n",
      "epoch 15/100, step 55/62,  total step 923/6200, training_loss = 0.2252\n",
      "epoch 15/100, step 56/62,  total step 924/6200, training_loss = 0.3278\n",
      "epoch 15/100, step 57/62,  total step 925/6200, training_loss = 0.2985\n",
      "epoch 15/100, step 58/62,  total step 926/6200, training_loss = 0.4092\n",
      "epoch 15/100, step 59/62,  total step 927/6200, training_loss = 0.2395\n",
      "epoch 15/100, step 60/62,  total step 928/6200, training_loss = 0.3752\n",
      "epoch 15/100, step 61/62,  total step 929/6200, training_loss = 0.1939\n",
      "epoch 15/100, step 62/62,  total step 930/6200, training_loss = 0.3806 | avg loss: 0.3115 Dice Metric:   0.645\n",
      "epoch 16/100, step 1/62,  total step 931/6200, training_loss = 0.2948\n",
      "epoch 16/100, step 2/62,  total step 932/6200, training_loss = 0.2982\n",
      "epoch 16/100, step 3/62,  total step 933/6200, training_loss = 0.3329\n",
      "epoch 16/100, step 4/62,  total step 934/6200, training_loss = 0.4323\n",
      "epoch 16/100, step 5/62,  total step 935/6200, training_loss = 0.1887\n",
      "epoch 16/100, step 6/62,  total step 936/6200, training_loss = 0.2706\n",
      "epoch 16/100, step 7/62,  total step 937/6200, training_loss = 0.2548\n",
      "epoch 16/100, step 8/62,  total step 938/6200, training_loss = 0.3114\n",
      "epoch 16/100, step 9/62,  total step 939/6200, training_loss = 0.2831\n",
      "epoch 16/100, step 10/62,  total step 940/6200, training_loss = 0.2813\n",
      "epoch 16/100, step 11/62,  total step 941/6200, training_loss = 0.2135\n",
      "epoch 16/100, step 12/62,  total step 942/6200, training_loss = 0.2531\n",
      "epoch 16/100, step 13/62,  total step 943/6200, training_loss = 0.3168\n",
      "epoch 16/100, step 14/62,  total step 944/6200, training_loss = 0.3163\n",
      "epoch 16/100, step 15/62,  total step 945/6200, training_loss = 0.4552\n",
      "epoch 16/100, step 16/62,  total step 946/6200, training_loss = 0.4388\n",
      "epoch 16/100, step 17/62,  total step 947/6200, training_loss = 0.2625\n",
      "epoch 16/100, step 18/62,  total step 948/6200, training_loss = 0.2662\n",
      "epoch 16/100, step 19/62,  total step 949/6200, training_loss = 0.3392\n",
      "epoch 16/100, step 20/62,  total step 950/6200, training_loss = 0.3165\n",
      "epoch 16/100, step 21/62,  total step 951/6200, training_loss = 0.3312\n",
      "epoch 16/100, step 22/62,  total step 952/6200, training_loss = 0.4097\n",
      "epoch 16/100, step 23/62,  total step 953/6200, training_loss = 0.3257\n",
      "epoch 16/100, step 24/62,  total step 954/6200, training_loss = 0.2446\n",
      "epoch 16/100, step 25/62,  total step 955/6200, training_loss = 0.2697\n",
      "epoch 16/100, step 26/62,  total step 956/6200, training_loss = 0.2219\n",
      "epoch 16/100, step 27/62,  total step 957/6200, training_loss = 0.2955\n",
      "epoch 16/100, step 28/62,  total step 958/6200, training_loss = 0.4237\n",
      "epoch 16/100, step 29/62,  total step 959/6200, training_loss = 0.3834\n",
      "epoch 16/100, step 30/62,  total step 960/6200, training_loss = 0.3115\n",
      "epoch 16/100, step 31/62,  total step 961/6200, training_loss = 0.2438\n",
      "epoch 16/100, step 32/62,  total step 962/6200, training_loss = 0.2658\n",
      "epoch 16/100, step 33/62,  total step 963/6200, training_loss = 0.3868\n",
      "epoch 16/100, step 34/62,  total step 964/6200, training_loss = 0.2779\n",
      "epoch 16/100, step 35/62,  total step 965/6200, training_loss = 0.2993\n",
      "epoch 16/100, step 36/62,  total step 966/6200, training_loss = 0.3814\n",
      "epoch 16/100, step 37/62,  total step 967/6200, training_loss = 0.3205\n",
      "epoch 16/100, step 38/62,  total step 968/6200, training_loss = 0.3475\n",
      "epoch 16/100, step 39/62,  total step 969/6200, training_loss = 0.3620\n",
      "epoch 16/100, step 40/62,  total step 970/6200, training_loss = 0.3384\n",
      "epoch 16/100, step 41/62,  total step 971/6200, training_loss = 0.2645\n",
      "epoch 16/100, step 42/62,  total step 972/6200, training_loss = 0.2996\n",
      "epoch 16/100, step 43/62,  total step 973/6200, training_loss = 0.2607\n",
      "epoch 16/100, step 44/62,  total step 974/6200, training_loss = 0.3155\n",
      "epoch 16/100, step 45/62,  total step 975/6200, training_loss = 0.2183\n",
      "epoch 16/100, step 46/62,  total step 976/6200, training_loss = 0.2235\n",
      "epoch 16/100, step 47/62,  total step 977/6200, training_loss = 0.3415\n",
      "epoch 16/100, step 48/62,  total step 978/6200, training_loss = 0.2359\n",
      "epoch 16/100, step 49/62,  total step 979/6200, training_loss = 0.2060\n",
      "epoch 16/100, step 50/62,  total step 980/6200, training_loss = 0.2942\n",
      "epoch 16/100, step 51/62,  total step 981/6200, training_loss = 0.2907\n",
      "epoch 16/100, step 52/62,  total step 982/6200, training_loss = 0.2316\n",
      "epoch 16/100, step 53/62,  total step 983/6200, training_loss = 0.2347\n",
      "epoch 16/100, step 54/62,  total step 984/6200, training_loss = 0.2310\n",
      "epoch 16/100, step 55/62,  total step 985/6200, training_loss = 0.2181\n",
      "epoch 16/100, step 56/62,  total step 986/6200, training_loss = 0.3046\n",
      "epoch 16/100, step 57/62,  total step 987/6200, training_loss = 0.3086\n",
      "epoch 16/100, step 58/62,  total step 988/6200, training_loss = 0.4180\n",
      "epoch 16/100, step 59/62,  total step 989/6200, training_loss = 0.2005\n",
      "epoch 16/100, step 60/62,  total step 990/6200, training_loss = 0.3826\n",
      "epoch 16/100, step 61/62,  total step 991/6200, training_loss = 0.2046\n",
      "epoch 16/100, step 62/62,  total step 992/6200, training_loss = 0.3680 | avg loss: 0.3003 Dice Metric:    0.66\n",
      "epoch 17/100, step 1/62,  total step 993/6200, training_loss = 0.2888\n",
      "epoch 17/100, step 2/62,  total step 994/6200, training_loss = 0.2698\n",
      "epoch 17/100, step 3/62,  total step 995/6200, training_loss = 0.3280\n",
      "epoch 17/100, step 4/62,  total step 996/6200, training_loss = 0.4649\n",
      "epoch 17/100, step 5/62,  total step 997/6200, training_loss = 0.1713\n",
      "epoch 17/100, step 6/62,  total step 998/6200, training_loss = 0.2791\n",
      "epoch 17/100, step 7/62,  total step 999/6200, training_loss = 0.2390\n",
      "epoch 17/100, step 8/62,  total step 1000/6200, training_loss = 0.3324\n",
      "epoch 17/100, step 9/62,  total step 1001/6200, training_loss = 0.2861\n",
      "epoch 17/100, step 10/62,  total step 1002/6200, training_loss = 0.3045\n",
      "epoch 17/100, step 11/62,  total step 1003/6200, training_loss = 0.1931\n",
      "epoch 17/100, step 12/62,  total step 1004/6200, training_loss = 0.2547\n",
      "epoch 17/100, step 13/62,  total step 1005/6200, training_loss = 0.3137\n",
      "epoch 17/100, step 14/62,  total step 1006/6200, training_loss = 0.3248\n",
      "epoch 17/100, step 15/62,  total step 1007/6200, training_loss = 0.4749\n",
      "epoch 17/100, step 16/62,  total step 1008/6200, training_loss = 0.4303\n",
      "epoch 17/100, step 17/62,  total step 1009/6200, training_loss = 0.2704\n",
      "epoch 17/100, step 18/62,  total step 1010/6200, training_loss = 0.2755\n",
      "epoch 17/100, step 19/62,  total step 1011/6200, training_loss = 0.3746\n",
      "epoch 17/100, step 20/62,  total step 1012/6200, training_loss = 0.3221\n",
      "epoch 17/100, step 21/62,  total step 1013/6200, training_loss = 0.3053\n",
      "epoch 17/100, step 22/62,  total step 1014/6200, training_loss = 0.4058\n",
      "epoch 17/100, step 23/62,  total step 1015/6200, training_loss = 0.3031\n",
      "epoch 17/100, step 24/62,  total step 1016/6200, training_loss = 0.2512\n",
      "epoch 17/100, step 25/62,  total step 1017/6200, training_loss = 0.2718\n",
      "epoch 17/100, step 26/62,  total step 1018/6200, training_loss = 0.2396\n",
      "epoch 17/100, step 27/62,  total step 1019/6200, training_loss = 0.2948\n",
      "epoch 17/100, step 28/62,  total step 1020/6200, training_loss = 0.4076\n",
      "epoch 17/100, step 29/62,  total step 1021/6200, training_loss = 0.3511\n",
      "epoch 17/100, step 30/62,  total step 1022/6200, training_loss = 0.3329\n",
      "epoch 17/100, step 31/62,  total step 1023/6200, training_loss = 0.2249\n",
      "epoch 17/100, step 32/62,  total step 1024/6200, training_loss = 0.2465\n",
      "epoch 17/100, step 33/62,  total step 1025/6200, training_loss = 0.3699\n",
      "epoch 17/100, step 34/62,  total step 1026/6200, training_loss = 0.2623\n",
      "epoch 17/100, step 35/62,  total step 1027/6200, training_loss = 0.3063\n",
      "epoch 17/100, step 36/62,  total step 1028/6200, training_loss = 0.3971\n",
      "epoch 17/100, step 37/62,  total step 1029/6200, training_loss = 0.3088\n",
      "epoch 17/100, step 38/62,  total step 1030/6200, training_loss = 0.3520\n",
      "epoch 17/100, step 39/62,  total step 1031/6200, training_loss = 0.3573\n",
      "epoch 17/100, step 40/62,  total step 1032/6200, training_loss = 0.3252\n",
      "epoch 17/100, step 41/62,  total step 1033/6200, training_loss = 0.2757\n",
      "epoch 17/100, step 42/62,  total step 1034/6200, training_loss = 0.3110\n",
      "epoch 17/100, step 43/62,  total step 1035/6200, training_loss = 0.2667\n",
      "epoch 17/100, step 44/62,  total step 1036/6200, training_loss = 0.2837\n",
      "epoch 17/100, step 45/62,  total step 1037/6200, training_loss = 0.2162\n",
      "epoch 17/100, step 46/62,  total step 1038/6200, training_loss = 0.2156\n",
      "epoch 17/100, step 47/62,  total step 1039/6200, training_loss = 0.3780\n",
      "epoch 17/100, step 48/62,  total step 1040/6200, training_loss = 0.1914\n",
      "epoch 17/100, step 49/62,  total step 1041/6200, training_loss = 0.2121\n",
      "epoch 17/100, step 50/62,  total step 1042/6200, training_loss = 0.2896\n",
      "epoch 17/100, step 51/62,  total step 1043/6200, training_loss = 0.2702\n",
      "epoch 17/100, step 52/62,  total step 1044/6200, training_loss = 0.2115\n",
      "epoch 17/100, step 53/62,  total step 1045/6200, training_loss = 0.2533\n",
      "epoch 17/100, step 54/62,  total step 1046/6200, training_loss = 0.2450\n",
      "epoch 17/100, step 55/62,  total step 1047/6200, training_loss = 0.2412\n",
      "epoch 17/100, step 56/62,  total step 1048/6200, training_loss = 0.2976\n",
      "epoch 17/100, step 57/62,  total step 1049/6200, training_loss = 0.2974\n",
      "epoch 17/100, step 58/62,  total step 1050/6200, training_loss = 0.4165\n",
      "epoch 17/100, step 59/62,  total step 1051/6200, training_loss = 0.2166\n",
      "epoch 17/100, step 60/62,  total step 1052/6200, training_loss = 0.4117\n",
      "epoch 17/100, step 61/62,  total step 1053/6200, training_loss = 0.1987\n",
      "epoch 17/100, step 62/62,  total step 1054/6200, training_loss = 0.3548 | avg loss: 0.2995 Dice Metric:   0.643\n",
      "epoch 18/100, step 1/62,  total step 1055/6200, training_loss = 0.3046\n",
      "epoch 18/100, step 2/62,  total step 1056/6200, training_loss = 0.2411\n",
      "epoch 18/100, step 3/62,  total step 1057/6200, training_loss = 0.3278\n",
      "epoch 18/100, step 4/62,  total step 1058/6200, training_loss = 0.4346\n",
      "epoch 18/100, step 5/62,  total step 1059/6200, training_loss = 0.1614\n",
      "epoch 18/100, step 6/62,  total step 1060/6200, training_loss = 0.2552\n",
      "epoch 18/100, step 7/62,  total step 1061/6200, training_loss = 0.2338\n",
      "epoch 18/100, step 8/62,  total step 1062/6200, training_loss = 0.3161\n",
      "epoch 18/100, step 9/62,  total step 1063/6200, training_loss = 0.2658\n",
      "epoch 18/100, step 10/62,  total step 1064/6200, training_loss = 0.2828\n",
      "epoch 18/100, step 11/62,  total step 1065/6200, training_loss = 0.1986\n",
      "epoch 18/100, step 12/62,  total step 1066/6200, training_loss = 0.2508\n",
      "epoch 18/100, step 13/62,  total step 1067/6200, training_loss = 0.3243\n",
      "epoch 18/100, step 14/62,  total step 1068/6200, training_loss = 0.3163\n",
      "epoch 18/100, step 15/62,  total step 1069/6200, training_loss = 0.4664\n",
      "epoch 18/100, step 16/62,  total step 1070/6200, training_loss = 0.4181\n",
      "epoch 18/100, step 17/62,  total step 1071/6200, training_loss = 0.3130\n",
      "epoch 18/100, step 18/62,  total step 1072/6200, training_loss = 0.2747\n",
      "epoch 18/100, step 19/62,  total step 1073/6200, training_loss = 0.3493\n",
      "epoch 18/100, step 20/62,  total step 1074/6200, training_loss = 0.3132\n",
      "epoch 18/100, step 21/62,  total step 1075/6200, training_loss = 0.3174\n",
      "epoch 18/100, step 22/62,  total step 1076/6200, training_loss = 0.4508\n",
      "epoch 18/100, step 23/62,  total step 1077/6200, training_loss = 0.3267\n",
      "epoch 18/100, step 24/62,  total step 1078/6200, training_loss = 0.2275\n",
      "epoch 18/100, step 25/62,  total step 1079/6200, training_loss = 0.2760\n",
      "epoch 18/100, step 26/62,  total step 1080/6200, training_loss = 0.2145\n",
      "epoch 18/100, step 27/62,  total step 1081/6200, training_loss = 0.2785\n",
      "epoch 18/100, step 28/62,  total step 1082/6200, training_loss = 0.3722\n",
      "epoch 18/100, step 29/62,  total step 1083/6200, training_loss = 0.3593\n",
      "epoch 18/100, step 30/62,  total step 1084/6200, training_loss = 0.3334\n",
      "epoch 18/100, step 31/62,  total step 1085/6200, training_loss = 0.2220\n",
      "epoch 18/100, step 32/62,  total step 1086/6200, training_loss = 0.2276\n",
      "epoch 18/100, step 33/62,  total step 1087/6200, training_loss = 0.4004\n",
      "epoch 18/100, step 34/62,  total step 1088/6200, training_loss = 0.2395\n",
      "epoch 18/100, step 35/62,  total step 1089/6200, training_loss = 0.2860\n",
      "epoch 18/100, step 36/62,  total step 1090/6200, training_loss = 0.3574\n",
      "epoch 18/100, step 37/62,  total step 1091/6200, training_loss = 0.3609\n",
      "epoch 18/100, step 38/62,  total step 1092/6200, training_loss = 0.3495\n",
      "epoch 18/100, step 39/62,  total step 1093/6200, training_loss = 0.3599\n",
      "epoch 18/100, step 40/62,  total step 1094/6200, training_loss = 0.3186\n",
      "epoch 18/100, step 41/62,  total step 1095/6200, training_loss = 0.2277\n",
      "epoch 18/100, step 42/62,  total step 1096/6200, training_loss = 0.2723\n",
      "epoch 18/100, step 43/62,  total step 1097/6200, training_loss = 0.2603\n",
      "epoch 18/100, step 44/62,  total step 1098/6200, training_loss = 0.3432\n",
      "epoch 18/100, step 45/62,  total step 1099/6200, training_loss = 0.2169\n",
      "epoch 18/100, step 46/62,  total step 1100/6200, training_loss = 0.2212\n",
      "epoch 18/100, step 47/62,  total step 1101/6200, training_loss = 0.2888\n",
      "epoch 18/100, step 48/62,  total step 1102/6200, training_loss = 0.1986\n",
      "epoch 18/100, step 49/62,  total step 1103/6200, training_loss = 0.2055\n",
      "epoch 18/100, step 50/62,  total step 1104/6200, training_loss = 0.2767\n",
      "epoch 18/100, step 51/62,  total step 1105/6200, training_loss = 0.2505\n",
      "epoch 18/100, step 52/62,  total step 1106/6200, training_loss = 0.2082\n",
      "epoch 18/100, step 53/62,  total step 1107/6200, training_loss = 0.2638\n",
      "epoch 18/100, step 54/62,  total step 1108/6200, training_loss = 0.2400\n",
      "epoch 18/100, step 55/62,  total step 1109/6200, training_loss = 0.2369\n",
      "epoch 18/100, step 56/62,  total step 1110/6200, training_loss = 0.3088\n",
      "epoch 18/100, step 57/62,  total step 1111/6200, training_loss = 0.3293\n",
      "epoch 18/100, step 58/62,  total step 1112/6200, training_loss = 0.4052\n",
      "epoch 18/100, step 59/62,  total step 1113/6200, training_loss = 0.1911\n",
      "epoch 18/100, step 60/62,  total step 1114/6200, training_loss = 0.3700\n",
      "epoch 18/100, step 61/62,  total step 1115/6200, training_loss = 0.1954\n",
      "epoch 18/100, step 62/62,  total step 1116/6200, training_loss = 0.3584 | avg loss: 0.2935 Dice Metric:   0.643\n",
      "epoch 19/100, step 1/62,  total step 1117/6200, training_loss = 0.2873\n",
      "epoch 19/100, step 2/62,  total step 1118/6200, training_loss = 0.2585\n",
      "epoch 19/100, step 3/62,  total step 1119/6200, training_loss = 0.3265\n",
      "epoch 19/100, step 4/62,  total step 1120/6200, training_loss = 0.4211\n",
      "epoch 19/100, step 5/62,  total step 1121/6200, training_loss = 0.1710\n",
      "epoch 19/100, step 6/62,  total step 1122/6200, training_loss = 0.2525\n",
      "epoch 19/100, step 7/62,  total step 1123/6200, training_loss = 0.2408\n",
      "epoch 19/100, step 8/62,  total step 1124/6200, training_loss = 0.3046\n",
      "epoch 19/100, step 9/62,  total step 1125/6200, training_loss = 0.2263\n",
      "epoch 19/100, step 10/62,  total step 1126/6200, training_loss = 0.2683\n",
      "epoch 19/100, step 11/62,  total step 1127/6200, training_loss = 0.2133\n",
      "epoch 19/100, step 12/62,  total step 1128/6200, training_loss = 0.2429\n",
      "epoch 19/100, step 13/62,  total step 1129/6200, training_loss = 0.2994\n",
      "epoch 19/100, step 14/62,  total step 1130/6200, training_loss = 0.2926\n",
      "epoch 19/100, step 15/62,  total step 1131/6200, training_loss = 0.4764\n",
      "epoch 19/100, step 16/62,  total step 1132/6200, training_loss = 0.4420\n",
      "epoch 19/100, step 17/62,  total step 1133/6200, training_loss = 0.2599\n",
      "epoch 19/100, step 18/62,  total step 1134/6200, training_loss = 0.2523\n",
      "epoch 19/100, step 19/62,  total step 1135/6200, training_loss = 0.3485\n",
      "epoch 19/100, step 20/62,  total step 1136/6200, training_loss = 0.3191\n",
      "epoch 19/100, step 21/62,  total step 1137/6200, training_loss = 0.3022\n",
      "epoch 19/100, step 22/62,  total step 1138/6200, training_loss = 0.4097\n",
      "epoch 19/100, step 23/62,  total step 1139/6200, training_loss = 0.3042\n",
      "epoch 19/100, step 24/62,  total step 1140/6200, training_loss = 0.2262\n",
      "epoch 19/100, step 25/62,  total step 1141/6200, training_loss = 0.2565\n",
      "epoch 19/100, step 26/62,  total step 1142/6200, training_loss = 0.2392\n",
      "epoch 19/100, step 27/62,  total step 1143/6200, training_loss = 0.2455\n",
      "epoch 19/100, step 28/62,  total step 1144/6200, training_loss = 0.3724\n",
      "epoch 19/100, step 29/62,  total step 1145/6200, training_loss = 0.3154\n",
      "epoch 19/100, step 30/62,  total step 1146/6200, training_loss = 0.3409\n",
      "epoch 19/100, step 31/62,  total step 1147/6200, training_loss = 0.2346\n",
      "epoch 19/100, step 32/62,  total step 1148/6200, training_loss = 0.2538\n",
      "epoch 19/100, step 33/62,  total step 1149/6200, training_loss = 0.3958\n",
      "epoch 19/100, step 34/62,  total step 1150/6200, training_loss = 0.2611\n",
      "epoch 19/100, step 35/62,  total step 1151/6200, training_loss = 0.2712\n",
      "epoch 19/100, step 36/62,  total step 1152/6200, training_loss = 0.3779\n",
      "epoch 19/100, step 37/62,  total step 1153/6200, training_loss = 0.3139\n",
      "epoch 19/100, step 38/62,  total step 1154/6200, training_loss = 0.3442\n",
      "epoch 19/100, step 39/62,  total step 1155/6200, training_loss = 0.3580\n",
      "epoch 19/100, step 40/62,  total step 1156/6200, training_loss = 0.3287\n",
      "epoch 19/100, step 41/62,  total step 1157/6200, training_loss = 0.2654\n",
      "epoch 19/100, step 42/62,  total step 1158/6200, training_loss = 0.2764\n",
      "epoch 19/100, step 43/62,  total step 1159/6200, training_loss = 0.2369\n",
      "epoch 19/100, step 44/62,  total step 1160/6200, training_loss = 0.2729\n",
      "epoch 19/100, step 45/62,  total step 1161/6200, training_loss = 0.1800\n",
      "epoch 19/100, step 46/62,  total step 1162/6200, training_loss = 0.2623\n",
      "epoch 19/100, step 47/62,  total step 1163/6200, training_loss = 0.3719\n",
      "epoch 19/100, step 48/62,  total step 1164/6200, training_loss = 0.1914\n",
      "epoch 19/100, step 49/62,  total step 1165/6200, training_loss = 0.2037\n",
      "epoch 19/100, step 50/62,  total step 1166/6200, training_loss = 0.2854\n",
      "epoch 19/100, step 51/62,  total step 1167/6200, training_loss = 0.2348\n",
      "epoch 19/100, step 52/62,  total step 1168/6200, training_loss = 0.2348\n",
      "epoch 19/100, step 53/62,  total step 1169/6200, training_loss = 0.2554\n",
      "epoch 19/100, step 54/62,  total step 1170/6200, training_loss = 0.2250\n",
      "epoch 19/100, step 55/62,  total step 1171/6200, training_loss = 0.2186\n",
      "epoch 19/100, step 56/62,  total step 1172/6200, training_loss = 0.2653\n",
      "epoch 19/100, step 57/62,  total step 1173/6200, training_loss = 0.2819\n",
      "epoch 19/100, step 58/62,  total step 1174/6200, training_loss = 0.4001\n",
      "epoch 19/100, step 59/62,  total step 1175/6200, training_loss = 0.2333\n",
      "epoch 19/100, step 60/62,  total step 1176/6200, training_loss = 0.3656\n",
      "epoch 19/100, step 61/62,  total step 1177/6200, training_loss = 0.1698\n",
      "epoch 19/100, step 62/62,  total step 1178/6200, training_loss = 0.2853 | avg loss: 0.2866 Dice Metric:   0.662\n",
      "epoch 20/100, step 1/62,  total step 1179/6200, training_loss = 0.2914\n",
      "epoch 20/100, step 2/62,  total step 1180/6200, training_loss = 0.2541\n",
      "epoch 20/100, step 3/62,  total step 1181/6200, training_loss = 0.3109\n",
      "epoch 20/100, step 4/62,  total step 1182/6200, training_loss = 0.4345\n",
      "epoch 20/100, step 5/62,  total step 1183/6200, training_loss = 0.1778\n",
      "epoch 20/100, step 6/62,  total step 1184/6200, training_loss = 0.2491\n",
      "epoch 20/100, step 7/62,  total step 1185/6200, training_loss = 0.2479\n",
      "epoch 20/100, step 8/62,  total step 1186/6200, training_loss = 0.2944\n",
      "epoch 20/100, step 9/62,  total step 1187/6200, training_loss = 0.2269\n",
      "epoch 20/100, step 10/62,  total step 1188/6200, training_loss = 0.3198\n",
      "epoch 20/100, step 11/62,  total step 1189/6200, training_loss = 0.2121\n",
      "epoch 20/100, step 12/62,  total step 1190/6200, training_loss = 0.2417\n",
      "epoch 20/100, step 13/62,  total step 1191/6200, training_loss = 0.2871\n",
      "epoch 20/100, step 14/62,  total step 1192/6200, training_loss = 0.3153\n",
      "epoch 20/100, step 15/62,  total step 1193/6200, training_loss = 0.4550\n",
      "epoch 20/100, step 16/62,  total step 1194/6200, training_loss = 0.4114\n",
      "epoch 20/100, step 17/62,  total step 1195/6200, training_loss = 0.2576\n",
      "epoch 20/100, step 18/62,  total step 1196/6200, training_loss = 0.2747\n",
      "epoch 20/100, step 19/62,  total step 1197/6200, training_loss = 0.3216\n",
      "epoch 20/100, step 20/62,  total step 1198/6200, training_loss = 0.3361\n",
      "epoch 20/100, step 21/62,  total step 1199/6200, training_loss = 0.3062\n",
      "epoch 20/100, step 22/62,  total step 1200/6200, training_loss = 0.3971\n",
      "epoch 20/100, step 23/62,  total step 1201/6200, training_loss = 0.2950\n",
      "epoch 20/100, step 24/62,  total step 1202/6200, training_loss = 0.2239\n",
      "epoch 20/100, step 25/62,  total step 1203/6200, training_loss = 0.2508\n",
      "epoch 20/100, step 26/62,  total step 1204/6200, training_loss = 0.1879\n",
      "epoch 20/100, step 27/62,  total step 1205/6200, training_loss = 0.2815\n",
      "epoch 20/100, step 28/62,  total step 1206/6200, training_loss = 0.4111\n",
      "epoch 20/100, step 29/62,  total step 1207/6200, training_loss = 0.3424\n",
      "epoch 20/100, step 30/62,  total step 1208/6200, training_loss = 0.3128\n",
      "epoch 20/100, step 31/62,  total step 1209/6200, training_loss = 0.2102\n",
      "epoch 20/100, step 32/62,  total step 1210/6200, training_loss = 0.2406\n",
      "epoch 20/100, step 33/62,  total step 1211/6200, training_loss = 0.3653\n",
      "epoch 20/100, step 34/62,  total step 1212/6200, training_loss = 0.2510\n",
      "epoch 20/100, step 35/62,  total step 1213/6200, training_loss = 0.2882\n",
      "epoch 20/100, step 36/62,  total step 1214/6200, training_loss = 0.3743\n",
      "epoch 20/100, step 37/62,  total step 1215/6200, training_loss = 0.3028\n",
      "epoch 20/100, step 38/62,  total step 1216/6200, training_loss = 0.3089\n",
      "epoch 20/100, step 39/62,  total step 1217/6200, training_loss = 0.3247\n",
      "epoch 20/100, step 40/62,  total step 1218/6200, training_loss = 0.3159\n",
      "epoch 20/100, step 41/62,  total step 1219/6200, training_loss = 0.2677\n",
      "epoch 20/100, step 42/62,  total step 1220/6200, training_loss = 0.2520\n",
      "epoch 20/100, step 43/62,  total step 1221/6200, training_loss = 0.2436\n",
      "epoch 20/100, step 44/62,  total step 1222/6200, training_loss = 0.2662\n",
      "epoch 20/100, step 45/62,  total step 1223/6200, training_loss = 0.1784\n",
      "epoch 20/100, step 46/62,  total step 1224/6200, training_loss = 0.2378\n",
      "epoch 20/100, step 47/62,  total step 1225/6200, training_loss = 0.2829\n",
      "epoch 20/100, step 48/62,  total step 1226/6200, training_loss = 0.2161\n",
      "epoch 20/100, step 49/62,  total step 1227/6200, training_loss = 0.2115\n",
      "epoch 20/100, step 50/62,  total step 1228/6200, training_loss = 0.2727\n",
      "epoch 20/100, step 51/62,  total step 1229/6200, training_loss = 0.2700\n",
      "epoch 20/100, step 52/62,  total step 1230/6200, training_loss = 0.2206\n",
      "epoch 20/100, step 53/62,  total step 1231/6200, training_loss = 0.2499\n",
      "epoch 20/100, step 54/62,  total step 1232/6200, training_loss = 0.2763\n",
      "epoch 20/100, step 55/62,  total step 1233/6200, training_loss = 0.2066\n",
      "epoch 20/100, step 56/62,  total step 1234/6200, training_loss = 0.3083\n",
      "epoch 20/100, step 57/62,  total step 1235/6200, training_loss = 0.2973\n",
      "epoch 20/100, step 58/62,  total step 1236/6200, training_loss = 0.3742\n",
      "epoch 20/100, step 59/62,  total step 1237/6200, training_loss = 0.2061\n",
      "epoch 20/100, step 60/62,  total step 1238/6200, training_loss = 0.3664\n",
      "epoch 20/100, step 61/62,  total step 1239/6200, training_loss = 0.1687\n",
      "epoch 20/100, step 62/62,  total step 1240/6200, training_loss = 0.3382 | avg loss: 0.2842 Dice Metric:   0.678\n",
      "epoch 21/100, step 1/62,  total step 1241/6200, training_loss = 0.2942\n",
      "epoch 21/100, step 2/62,  total step 1242/6200, training_loss = 0.2495\n",
      "epoch 21/100, step 3/62,  total step 1243/6200, training_loss = 0.3160\n",
      "epoch 21/100, step 4/62,  total step 1244/6200, training_loss = 0.4403\n",
      "epoch 21/100, step 5/62,  total step 1245/6200, training_loss = 0.1732\n",
      "epoch 21/100, step 6/62,  total step 1246/6200, training_loss = 0.2451\n",
      "epoch 21/100, step 7/62,  total step 1247/6200, training_loss = 0.2202\n",
      "epoch 21/100, step 8/62,  total step 1248/6200, training_loss = 0.3116\n",
      "epoch 21/100, step 9/62,  total step 1249/6200, training_loss = 0.2910\n",
      "epoch 21/100, step 10/62,  total step 1250/6200, training_loss = 0.2705\n",
      "epoch 21/100, step 11/62,  total step 1251/6200, training_loss = 0.2225\n",
      "epoch 21/100, step 12/62,  total step 1252/6200, training_loss = 0.2288\n",
      "epoch 21/100, step 13/62,  total step 1253/6200, training_loss = 0.2685\n",
      "epoch 21/100, step 14/62,  total step 1254/6200, training_loss = 0.2856\n",
      "epoch 21/100, step 15/62,  total step 1255/6200, training_loss = 0.4422\n",
      "epoch 21/100, step 16/62,  total step 1256/6200, training_loss = 0.3846\n",
      "epoch 21/100, step 17/62,  total step 1257/6200, training_loss = 0.2432\n",
      "epoch 21/100, step 18/62,  total step 1258/6200, training_loss = 0.2652\n",
      "epoch 21/100, step 19/62,  total step 1259/6200, training_loss = 0.3232\n",
      "epoch 21/100, step 20/62,  total step 1260/6200, training_loss = 0.3150\n",
      "epoch 21/100, step 21/62,  total step 1261/6200, training_loss = 0.3110\n",
      "epoch 21/100, step 22/62,  total step 1262/6200, training_loss = 0.4037\n",
      "epoch 21/100, step 23/62,  total step 1263/6200, training_loss = 0.2845\n",
      "epoch 21/100, step 24/62,  total step 1264/6200, training_loss = 0.2411\n",
      "epoch 21/100, step 25/62,  total step 1265/6200, training_loss = 0.2603\n",
      "epoch 21/100, step 26/62,  total step 1266/6200, training_loss = 0.2317\n",
      "epoch 21/100, step 27/62,  total step 1267/6200, training_loss = 0.2376\n",
      "epoch 21/100, step 28/62,  total step 1268/6200, training_loss = 0.3697\n",
      "epoch 21/100, step 29/62,  total step 1269/6200, training_loss = 0.3244\n",
      "epoch 21/100, step 30/62,  total step 1270/6200, training_loss = 0.2899\n",
      "epoch 21/100, step 31/62,  total step 1271/6200, training_loss = 0.2024\n",
      "epoch 21/100, step 32/62,  total step 1272/6200, training_loss = 0.2756\n",
      "epoch 21/100, step 33/62,  total step 1273/6200, training_loss = 0.3746\n",
      "epoch 21/100, step 34/62,  total step 1274/6200, training_loss = 0.2529\n",
      "epoch 21/100, step 35/62,  total step 1275/6200, training_loss = 0.2921\n",
      "epoch 21/100, step 36/62,  total step 1276/6200, training_loss = 0.3536\n",
      "epoch 21/100, step 37/62,  total step 1277/6200, training_loss = 0.2990\n",
      "epoch 21/100, step 38/62,  total step 1278/6200, training_loss = 0.2959\n",
      "epoch 21/100, step 39/62,  total step 1279/6200, training_loss = 0.3062\n",
      "epoch 21/100, step 40/62,  total step 1280/6200, training_loss = 0.2942\n",
      "epoch 21/100, step 41/62,  total step 1281/6200, training_loss = 0.2507\n",
      "epoch 21/100, step 42/62,  total step 1282/6200, training_loss = 0.2605\n",
      "epoch 21/100, step 43/62,  total step 1283/6200, training_loss = 0.2350\n",
      "epoch 21/100, step 44/62,  total step 1284/6200, training_loss = 0.2610\n",
      "epoch 21/100, step 45/62,  total step 1285/6200, training_loss = 0.1754\n",
      "epoch 21/100, step 46/62,  total step 1286/6200, training_loss = 0.2216\n",
      "epoch 21/100, step 47/62,  total step 1287/6200, training_loss = 0.2951\n",
      "epoch 21/100, step 48/62,  total step 1288/6200, training_loss = 0.1901\n",
      "epoch 21/100, step 49/62,  total step 1289/6200, training_loss = 0.2015\n",
      "epoch 21/100, step 50/62,  total step 1290/6200, training_loss = 0.2617\n",
      "epoch 21/100, step 51/62,  total step 1291/6200, training_loss = 0.2537\n",
      "epoch 21/100, step 52/62,  total step 1292/6200, training_loss = 0.1941\n",
      "epoch 21/100, step 53/62,  total step 1293/6200, training_loss = 0.2434\n",
      "epoch 21/100, step 54/62,  total step 1294/6200, training_loss = 0.2204\n",
      "epoch 21/100, step 55/62,  total step 1295/6200, training_loss = 0.2052\n",
      "epoch 21/100, step 56/62,  total step 1296/6200, training_loss = 0.2856\n",
      "epoch 21/100, step 57/62,  total step 1297/6200, training_loss = 0.3022\n",
      "epoch 21/100, step 58/62,  total step 1298/6200, training_loss = 0.3744\n",
      "epoch 21/100, step 59/62,  total step 1299/6200, training_loss = 0.2150\n",
      "epoch 21/100, step 60/62,  total step 1300/6200, training_loss = 0.3484\n",
      "epoch 21/100, step 61/62,  total step 1301/6200, training_loss = 0.1955\n",
      "epoch 21/100, step 62/62,  total step 1302/6200, training_loss = 0.4010 | avg loss: 0.2787 Dice Metric:   0.657\n",
      "epoch 22/100, step 1/62,  total step 1303/6200, training_loss = 0.2823\n",
      "epoch 22/100, step 2/62,  total step 1304/6200, training_loss = 0.2532\n",
      "epoch 22/100, step 3/62,  total step 1305/6200, training_loss = 0.3128\n",
      "epoch 22/100, step 4/62,  total step 1306/6200, training_loss = 0.4116\n",
      "epoch 22/100, step 5/62,  total step 1307/6200, training_loss = 0.1640\n",
      "epoch 22/100, step 6/62,  total step 1308/6200, training_loss = 0.2448\n",
      "epoch 22/100, step 7/62,  total step 1309/6200, training_loss = 0.2391\n",
      "epoch 22/100, step 8/62,  total step 1310/6200, training_loss = 0.3152\n",
      "epoch 22/100, step 9/62,  total step 1311/6200, training_loss = 0.2597\n",
      "epoch 22/100, step 10/62,  total step 1312/6200, training_loss = 0.2893\n",
      "epoch 22/100, step 11/62,  total step 1313/6200, training_loss = 0.1941\n",
      "epoch 22/100, step 12/62,  total step 1314/6200, training_loss = 0.2294\n",
      "epoch 22/100, step 13/62,  total step 1315/6200, training_loss = 0.3133\n",
      "epoch 22/100, step 14/62,  total step 1316/6200, training_loss = 0.2868\n",
      "epoch 22/100, step 15/62,  total step 1317/6200, training_loss = 0.4164\n",
      "epoch 22/100, step 16/62,  total step 1318/6200, training_loss = 0.4103\n",
      "epoch 22/100, step 17/62,  total step 1319/6200, training_loss = 0.2452\n",
      "epoch 22/100, step 18/62,  total step 1320/6200, training_loss = 0.2561\n",
      "epoch 22/100, step 19/62,  total step 1321/6200, training_loss = 0.3116\n",
      "epoch 22/100, step 20/62,  total step 1322/6200, training_loss = 0.2954\n",
      "epoch 22/100, step 21/62,  total step 1323/6200, training_loss = 0.3274\n",
      "epoch 22/100, step 22/62,  total step 1324/6200, training_loss = 0.4161\n",
      "epoch 22/100, step 23/62,  total step 1325/6200, training_loss = 0.3293\n",
      "epoch 22/100, step 24/62,  total step 1326/6200, training_loss = 0.2453\n",
      "epoch 22/100, step 25/62,  total step 1327/6200, training_loss = 0.2656\n",
      "epoch 22/100, step 26/62,  total step 1328/6200, training_loss = 0.1934\n",
      "epoch 22/100, step 27/62,  total step 1329/6200, training_loss = 0.2783\n",
      "epoch 22/100, step 28/62,  total step 1330/6200, training_loss = 0.4141\n",
      "epoch 22/100, step 29/62,  total step 1331/6200, training_loss = 0.3074\n",
      "epoch 22/100, step 30/62,  total step 1332/6200, training_loss = 0.3171\n",
      "epoch 22/100, step 31/62,  total step 1333/6200, training_loss = 0.1939\n",
      "epoch 22/100, step 32/62,  total step 1334/6200, training_loss = 0.2362\n",
      "epoch 22/100, step 33/62,  total step 1335/6200, training_loss = 0.3594\n",
      "epoch 22/100, step 34/62,  total step 1336/6200, training_loss = 0.2369\n",
      "epoch 22/100, step 35/62,  total step 1337/6200, training_loss = 0.2701\n",
      "epoch 22/100, step 36/62,  total step 1338/6200, training_loss = 0.3428\n",
      "epoch 22/100, step 37/62,  total step 1339/6200, training_loss = 0.2948\n",
      "epoch 22/100, step 38/62,  total step 1340/6200, training_loss = 0.3250\n",
      "epoch 22/100, step 39/62,  total step 1341/6200, training_loss = 0.2946\n",
      "epoch 22/100, step 40/62,  total step 1342/6200, training_loss = 0.3080\n",
      "epoch 22/100, step 41/62,  total step 1343/6200, training_loss = 0.2131\n",
      "epoch 22/100, step 42/62,  total step 1344/6200, training_loss = 0.2555\n",
      "epoch 22/100, step 43/62,  total step 1345/6200, training_loss = 0.2348\n",
      "epoch 22/100, step 44/62,  total step 1346/6200, training_loss = 0.2771\n",
      "epoch 22/100, step 45/62,  total step 1347/6200, training_loss = 0.1728\n",
      "epoch 22/100, step 46/62,  total step 1348/6200, training_loss = 0.1879\n",
      "epoch 22/100, step 47/62,  total step 1349/6200, training_loss = 0.2915\n",
      "epoch 22/100, step 48/62,  total step 1350/6200, training_loss = 0.2015\n",
      "epoch 22/100, step 49/62,  total step 1351/6200, training_loss = 0.1989\n",
      "epoch 22/100, step 50/62,  total step 1352/6200, training_loss = 0.2619\n",
      "epoch 22/100, step 51/62,  total step 1353/6200, training_loss = 0.2690\n",
      "epoch 22/100, step 52/62,  total step 1354/6200, training_loss = 0.2188\n",
      "epoch 22/100, step 53/62,  total step 1355/6200, training_loss = 0.2280\n",
      "epoch 22/100, step 54/62,  total step 1356/6200, training_loss = 0.2722\n",
      "epoch 22/100, step 55/62,  total step 1357/6200, training_loss = 0.2296\n",
      "epoch 22/100, step 56/62,  total step 1358/6200, training_loss = 0.2753\n",
      "epoch 22/100, step 57/62,  total step 1359/6200, training_loss = 0.2943\n",
      "epoch 22/100, step 58/62,  total step 1360/6200, training_loss = 0.3841\n",
      "epoch 22/100, step 59/62,  total step 1361/6200, training_loss = 0.2159\n",
      "epoch 22/100, step 60/62,  total step 1362/6200, training_loss = 0.3403\n",
      "epoch 22/100, step 61/62,  total step 1363/6200, training_loss = 0.1889\n",
      "epoch 22/100, step 62/62,  total step 1364/6200, training_loss = 0.2729 | avg loss: 0.2769 Dice Metric:   0.652\n",
      "epoch 23/100, step 1/62,  total step 1365/6200, training_loss = 0.2741\n",
      "epoch 23/100, step 2/62,  total step 1366/6200, training_loss = 0.2353\n",
      "epoch 23/100, step 3/62,  total step 1367/6200, training_loss = 0.2792\n",
      "epoch 23/100, step 4/62,  total step 1368/6200, training_loss = 0.4189\n",
      "epoch 23/100, step 5/62,  total step 1369/6200, training_loss = 0.1627\n",
      "epoch 23/100, step 6/62,  total step 1370/6200, training_loss = 0.2157\n",
      "epoch 23/100, step 7/62,  total step 1371/6200, training_loss = 0.2373\n",
      "epoch 23/100, step 8/62,  total step 1372/6200, training_loss = 0.2804\n",
      "epoch 23/100, step 9/62,  total step 1373/6200, training_loss = 0.2293\n",
      "epoch 23/100, step 10/62,  total step 1374/6200, training_loss = 0.2623\n",
      "epoch 23/100, step 11/62,  total step 1375/6200, training_loss = 0.2249\n",
      "epoch 23/100, step 12/62,  total step 1376/6200, training_loss = 0.2270\n",
      "epoch 23/100, step 13/62,  total step 1377/6200, training_loss = 0.2918\n",
      "epoch 23/100, step 14/62,  total step 1378/6200, training_loss = 0.2900\n",
      "epoch 23/100, step 15/62,  total step 1379/6200, training_loss = 0.4337\n",
      "epoch 23/100, step 16/62,  total step 1380/6200, training_loss = 0.3907\n",
      "epoch 23/100, step 17/62,  total step 1381/6200, training_loss = 0.2480\n",
      "epoch 23/100, step 18/62,  total step 1382/6200, training_loss = 0.2596\n",
      "epoch 23/100, step 19/62,  total step 1383/6200, training_loss = 0.3243\n",
      "epoch 23/100, step 20/62,  total step 1384/6200, training_loss = 0.2588\n",
      "epoch 23/100, step 21/62,  total step 1385/6200, training_loss = 0.2831\n",
      "epoch 23/100, step 22/62,  total step 1386/6200, training_loss = 0.4162\n",
      "epoch 23/100, step 23/62,  total step 1387/6200, training_loss = 0.2935\n",
      "epoch 23/100, step 24/62,  total step 1388/6200, training_loss = 0.2343\n",
      "epoch 23/100, step 25/62,  total step 1389/6200, training_loss = 0.2465\n",
      "epoch 23/100, step 26/62,  total step 1390/6200, training_loss = 0.2205\n",
      "epoch 23/100, step 27/62,  total step 1391/6200, training_loss = 0.2646\n",
      "epoch 23/100, step 28/62,  total step 1392/6200, training_loss = 0.3878\n",
      "epoch 23/100, step 29/62,  total step 1393/6200, training_loss = 0.3348\n",
      "epoch 23/100, step 30/62,  total step 1394/6200, training_loss = 0.3142\n",
      "epoch 23/100, step 31/62,  total step 1395/6200, training_loss = 0.1938\n",
      "epoch 23/100, step 32/62,  total step 1396/6200, training_loss = 0.2441\n",
      "epoch 23/100, step 33/62,  total step 1397/6200, training_loss = 0.3556\n",
      "epoch 23/100, step 34/62,  total step 1398/6200, training_loss = 0.2337\n",
      "epoch 23/100, step 35/62,  total step 1399/6200, training_loss = 0.2612\n",
      "epoch 23/100, step 36/62,  total step 1400/6200, training_loss = 0.3434\n",
      "epoch 23/100, step 37/62,  total step 1401/6200, training_loss = 0.2964\n",
      "epoch 23/100, step 38/62,  total step 1402/6200, training_loss = 0.2967\n",
      "epoch 23/100, step 39/62,  total step 1403/6200, training_loss = 0.3067\n",
      "epoch 23/100, step 40/62,  total step 1404/6200, training_loss = 0.3292\n",
      "epoch 23/100, step 41/62,  total step 1405/6200, training_loss = 0.2318\n",
      "epoch 23/100, step 42/62,  total step 1406/6200, training_loss = 0.2697\n",
      "epoch 23/100, step 43/62,  total step 1407/6200, training_loss = 0.2382\n",
      "epoch 23/100, step 44/62,  total step 1408/6200, training_loss = 0.2503\n",
      "epoch 23/100, step 45/62,  total step 1409/6200, training_loss = 0.1784\n",
      "epoch 23/100, step 46/62,  total step 1410/6200, training_loss = 0.2486\n",
      "epoch 23/100, step 47/62,  total step 1411/6200, training_loss = 0.3546\n",
      "epoch 23/100, step 48/62,  total step 1412/6200, training_loss = 0.1965\n",
      "epoch 23/100, step 49/62,  total step 1413/6200, training_loss = 0.2123\n",
      "epoch 23/100, step 50/62,  total step 1414/6200, training_loss = 0.2791\n",
      "epoch 23/100, step 51/62,  total step 1415/6200, training_loss = 0.2443\n",
      "epoch 23/100, step 52/62,  total step 1416/6200, training_loss = 0.2280\n",
      "epoch 23/100, step 53/62,  total step 1417/6200, training_loss = 0.2321\n",
      "epoch 23/100, step 54/62,  total step 1418/6200, training_loss = 0.2158\n",
      "epoch 23/100, step 55/62,  total step 1419/6200, training_loss = 0.2173\n",
      "epoch 23/100, step 56/62,  total step 1420/6200, training_loss = 0.2774\n",
      "epoch 23/100, step 57/62,  total step 1421/6200, training_loss = 0.2865\n",
      "epoch 23/100, step 58/62,  total step 1422/6200, training_loss = 0.3851\n",
      "epoch 23/100, step 59/62,  total step 1423/6200, training_loss = 0.2044\n",
      "epoch 23/100, step 60/62,  total step 1424/6200, training_loss = 0.3432\n",
      "epoch 23/100, step 61/62,  total step 1425/6200, training_loss = 0.1800\n",
      "epoch 23/100, step 62/62,  total step 1426/6200, training_loss = 0.3898 | avg loss: 0.2752 Dice Metric:   0.661\n",
      "epoch 24/100, step 1/62,  total step 1427/6200, training_loss = 0.3049\n",
      "epoch 24/100, step 2/62,  total step 1428/6200, training_loss = 0.2640\n",
      "epoch 24/100, step 3/62,  total step 1429/6200, training_loss = 0.3054\n",
      "epoch 24/100, step 4/62,  total step 1430/6200, training_loss = 0.4283\n",
      "epoch 24/100, step 5/62,  total step 1431/6200, training_loss = 0.1739\n",
      "epoch 24/100, step 6/62,  total step 1432/6200, training_loss = 0.2854\n",
      "epoch 24/100, step 7/62,  total step 1433/6200, training_loss = 0.2536\n",
      "epoch 24/100, step 8/62,  total step 1434/6200, training_loss = 0.2865\n",
      "epoch 24/100, step 9/62,  total step 1435/6200, training_loss = 0.2517\n",
      "epoch 24/100, step 10/62,  total step 1436/6200, training_loss = 0.2543\n",
      "epoch 24/100, step 11/62,  total step 1437/6200, training_loss = 0.2060\n",
      "epoch 24/100, step 12/62,  total step 1438/6200, training_loss = 0.2241\n",
      "epoch 24/100, step 13/62,  total step 1439/6200, training_loss = 0.2501\n",
      "epoch 24/100, step 14/62,  total step 1440/6200, training_loss = 0.2994\n",
      "epoch 24/100, step 15/62,  total step 1441/6200, training_loss = 0.4434\n",
      "epoch 24/100, step 16/62,  total step 1442/6200, training_loss = 0.4124\n",
      "epoch 24/100, step 17/62,  total step 1443/6200, training_loss = 0.2576\n",
      "epoch 24/100, step 18/62,  total step 1444/6200, training_loss = 0.2453\n",
      "epoch 24/100, step 19/62,  total step 1445/6200, training_loss = 0.3550\n",
      "epoch 24/100, step 20/62,  total step 1446/6200, training_loss = 0.2783\n",
      "epoch 24/100, step 21/62,  total step 1447/6200, training_loss = 0.3092\n",
      "epoch 24/100, step 22/62,  total step 1448/6200, training_loss = 0.3940\n",
      "epoch 24/100, step 23/62,  total step 1449/6200, training_loss = 0.2764\n",
      "epoch 24/100, step 24/62,  total step 1450/6200, training_loss = 0.2101\n",
      "epoch 24/100, step 25/62,  total step 1451/6200, training_loss = 0.2473\n",
      "epoch 24/100, step 26/62,  total step 1452/6200, training_loss = 0.1947\n",
      "epoch 24/100, step 27/62,  total step 1453/6200, training_loss = 0.2721\n",
      "epoch 24/100, step 28/62,  total step 1454/6200, training_loss = 0.3911\n",
      "epoch 24/100, step 29/62,  total step 1455/6200, training_loss = 0.3364\n",
      "epoch 24/100, step 30/62,  total step 1456/6200, training_loss = 0.3341\n",
      "epoch 24/100, step 31/62,  total step 1457/6200, training_loss = 0.1900\n",
      "epoch 24/100, step 32/62,  total step 1458/6200, training_loss = 0.2515\n",
      "epoch 24/100, step 33/62,  total step 1459/6200, training_loss = 0.3605\n",
      "epoch 24/100, step 34/62,  total step 1460/6200, training_loss = 0.2640\n",
      "epoch 24/100, step 35/62,  total step 1461/6200, training_loss = 0.2921\n",
      "epoch 24/100, step 36/62,  total step 1462/6200, training_loss = 0.3475\n",
      "epoch 24/100, step 37/62,  total step 1463/6200, training_loss = 0.2833\n",
      "epoch 24/100, step 38/62,  total step 1464/6200, training_loss = 0.3089\n",
      "epoch 24/100, step 39/62,  total step 1465/6200, training_loss = 0.3251\n",
      "epoch 24/100, step 40/62,  total step 1466/6200, training_loss = 0.3086\n",
      "epoch 24/100, step 41/62,  total step 1467/6200, training_loss = 0.2178\n",
      "epoch 24/100, step 42/62,  total step 1468/6200, training_loss = 0.2508\n",
      "epoch 24/100, step 43/62,  total step 1469/6200, training_loss = 0.2430\n",
      "epoch 24/100, step 44/62,  total step 1470/6200, training_loss = 0.2726\n",
      "epoch 24/100, step 45/62,  total step 1471/6200, training_loss = 0.1776\n",
      "epoch 24/100, step 46/62,  total step 1472/6200, training_loss = 0.2079\n",
      "epoch 24/100, step 47/62,  total step 1473/6200, training_loss = 0.2970\n",
      "epoch 24/100, step 48/62,  total step 1474/6200, training_loss = 0.2234\n",
      "epoch 24/100, step 49/62,  total step 1475/6200, training_loss = 0.2113\n",
      "epoch 24/100, step 50/62,  total step 1476/6200, training_loss = 0.2576\n",
      "epoch 24/100, step 51/62,  total step 1477/6200, training_loss = 0.2346\n",
      "epoch 24/100, step 52/62,  total step 1478/6200, training_loss = 0.1936\n",
      "epoch 24/100, step 53/62,  total step 1479/6200, training_loss = 0.2316\n",
      "epoch 24/100, step 54/62,  total step 1480/6200, training_loss = 0.2204\n",
      "epoch 24/100, step 55/62,  total step 1481/6200, training_loss = 0.2045\n",
      "epoch 24/100, step 56/62,  total step 1482/6200, training_loss = 0.2628\n",
      "epoch 24/100, step 57/62,  total step 1483/6200, training_loss = 0.2732\n",
      "epoch 24/100, step 58/62,  total step 1484/6200, training_loss = 0.3727\n",
      "epoch 24/100, step 59/62,  total step 1485/6200, training_loss = 0.2092\n",
      "epoch 24/100, step 60/62,  total step 1486/6200, training_loss = 0.3587\n",
      "epoch 24/100, step 61/62,  total step 1487/6200, training_loss = 0.1880\n",
      "epoch 24/100, step 62/62,  total step 1488/6200, training_loss = 0.3966 | avg loss: 0.2771 Dice Metric:   0.663\n",
      "epoch 25/100, step 1/62,  total step 1489/6200, training_loss = 0.2458\n",
      "epoch 25/100, step 2/62,  total step 1490/6200, training_loss = 0.2340\n",
      "epoch 25/100, step 3/62,  total step 1491/6200, training_loss = 0.2764\n",
      "epoch 25/100, step 4/62,  total step 1492/6200, training_loss = 0.4118\n",
      "epoch 25/100, step 5/62,  total step 1493/6200, training_loss = 0.1667\n",
      "epoch 25/100, step 6/62,  total step 1494/6200, training_loss = 0.2223\n",
      "epoch 25/100, step 7/62,  total step 1495/6200, training_loss = 0.2106\n",
      "epoch 25/100, step 8/62,  total step 1496/6200, training_loss = 0.2679\n",
      "epoch 25/100, step 9/62,  total step 1497/6200, training_loss = 0.2406\n",
      "epoch 25/100, step 10/62,  total step 1498/6200, training_loss = 0.2532\n",
      "epoch 25/100, step 11/62,  total step 1499/6200, training_loss = 0.1949\n",
      "epoch 25/100, step 12/62,  total step 1500/6200, training_loss = 0.2301\n",
      "epoch 25/100, step 13/62,  total step 1501/6200, training_loss = 0.2564\n",
      "epoch 25/100, step 14/62,  total step 1502/6200, training_loss = 0.2899\n",
      "epoch 25/100, step 15/62,  total step 1503/6200, training_loss = 0.4235\n",
      "epoch 25/100, step 16/62,  total step 1504/6200, training_loss = 0.3641\n",
      "epoch 25/100, step 17/62,  total step 1505/6200, training_loss = 0.2497\n",
      "epoch 25/100, step 18/62,  total step 1506/6200, training_loss = 0.2351\n",
      "epoch 25/100, step 19/62,  total step 1507/6200, training_loss = 0.3079\n",
      "epoch 25/100, step 20/62,  total step 1508/6200, training_loss = 0.2972\n",
      "epoch 25/100, step 21/62,  total step 1509/6200, training_loss = 0.2855\n",
      "epoch 25/100, step 22/62,  total step 1510/6200, training_loss = 0.3956\n",
      "epoch 25/100, step 23/62,  total step 1511/6200, training_loss = 0.3200\n",
      "epoch 25/100, step 24/62,  total step 1512/6200, training_loss = 0.2049\n",
      "epoch 25/100, step 25/62,  total step 1513/6200, training_loss = 0.2656\n",
      "epoch 25/100, step 26/62,  total step 1514/6200, training_loss = 0.2022\n",
      "epoch 25/100, step 27/62,  total step 1515/6200, training_loss = 0.2446\n",
      "epoch 25/100, step 28/62,  total step 1516/6200, training_loss = 0.3714\n",
      "epoch 25/100, step 29/62,  total step 1517/6200, training_loss = 0.3463\n",
      "epoch 25/100, step 30/62,  total step 1518/6200, training_loss = 0.2951\n",
      "epoch 25/100, step 31/62,  total step 1519/6200, training_loss = 0.2219\n",
      "epoch 25/100, step 32/62,  total step 1520/6200, training_loss = 0.2493\n",
      "epoch 25/100, step 33/62,  total step 1521/6200, training_loss = 0.3614\n",
      "epoch 25/100, step 34/62,  total step 1522/6200, training_loss = 0.2540\n",
      "epoch 25/100, step 35/62,  total step 1523/6200, training_loss = 0.2643\n",
      "epoch 25/100, step 36/62,  total step 1524/6200, training_loss = 0.3516\n",
      "epoch 25/100, step 37/62,  total step 1525/6200, training_loss = 0.2973\n",
      "epoch 25/100, step 38/62,  total step 1526/6200, training_loss = 0.3079\n",
      "epoch 25/100, step 39/62,  total step 1527/6200, training_loss = 0.3147\n",
      "epoch 25/100, step 40/62,  total step 1528/6200, training_loss = 0.2975\n",
      "epoch 25/100, step 41/62,  total step 1529/6200, training_loss = 0.2227\n",
      "epoch 25/100, step 42/62,  total step 1530/6200, training_loss = 0.2679\n",
      "epoch 25/100, step 43/62,  total step 1531/6200, training_loss = 0.2508\n",
      "epoch 25/100, step 44/62,  total step 1532/6200, training_loss = 0.2614\n",
      "epoch 25/100, step 45/62,  total step 1533/6200, training_loss = 0.1934\n",
      "epoch 25/100, step 46/62,  total step 1534/6200, training_loss = 0.2025\n",
      "epoch 25/100, step 47/62,  total step 1535/6200, training_loss = 0.2948\n",
      "epoch 25/100, step 48/62,  total step 1536/6200, training_loss = 0.1795\n",
      "epoch 25/100, step 49/62,  total step 1537/6200, training_loss = 0.2022\n",
      "epoch 25/100, step 50/62,  total step 1538/6200, training_loss = 0.2582\n",
      "epoch 25/100, step 51/62,  total step 1539/6200, training_loss = 0.2361\n",
      "epoch 25/100, step 52/62,  total step 1540/6200, training_loss = 0.2095\n",
      "epoch 25/100, step 53/62,  total step 1541/6200, training_loss = 0.2116\n",
      "epoch 25/100, step 54/62,  total step 1542/6200, training_loss = 0.1819\n",
      "epoch 25/100, step 55/62,  total step 1543/6200, training_loss = 0.2235\n",
      "epoch 25/100, step 56/62,  total step 1544/6200, training_loss = 0.2850\n",
      "epoch 25/100, step 57/62,  total step 1545/6200, training_loss = 0.2897\n",
      "epoch 25/100, step 58/62,  total step 1546/6200, training_loss = 0.3674\n",
      "epoch 25/100, step 59/62,  total step 1547/6200, training_loss = 0.1999\n",
      "epoch 25/100, step 60/62,  total step 1548/6200, training_loss = 0.3715\n",
      "epoch 25/100, step 61/62,  total step 1549/6200, training_loss = 0.1931\n",
      "epoch 25/100, step 62/62,  total step 1550/6200, training_loss = 0.2973 | avg loss: 0.2682 Dice Metric:   0.646\n",
      "epoch 26/100, step 1/62,  total step 1551/6200, training_loss = 0.2827\n",
      "epoch 26/100, step 2/62,  total step 1552/6200, training_loss = 0.2189\n",
      "epoch 26/100, step 3/62,  total step 1553/6200, training_loss = 0.2924\n",
      "epoch 26/100, step 4/62,  total step 1554/6200, training_loss = 0.4074\n",
      "epoch 26/100, step 5/62,  total step 1555/6200, training_loss = 0.1678\n",
      "epoch 26/100, step 6/62,  total step 1556/6200, training_loss = 0.2495\n",
      "epoch 26/100, step 7/62,  total step 1557/6200, training_loss = 0.2609\n",
      "epoch 26/100, step 8/62,  total step 1558/6200, training_loss = 0.3304\n",
      "epoch 26/100, step 9/62,  total step 1559/6200, training_loss = 0.2550\n",
      "epoch 26/100, step 10/62,  total step 1560/6200, training_loss = 0.2583\n",
      "epoch 26/100, step 11/62,  total step 1561/6200, training_loss = 0.1945\n",
      "epoch 26/100, step 12/62,  total step 1562/6200, training_loss = 0.2147\n",
      "epoch 26/100, step 13/62,  total step 1563/6200, training_loss = 0.2721\n",
      "epoch 26/100, step 14/62,  total step 1564/6200, training_loss = 0.2976\n",
      "epoch 26/100, step 15/62,  total step 1565/6200, training_loss = 0.4353\n",
      "epoch 26/100, step 16/62,  total step 1566/6200, training_loss = 0.3953\n",
      "epoch 26/100, step 17/62,  total step 1567/6200, training_loss = 0.2666\n",
      "epoch 26/100, step 18/62,  total step 1568/6200, training_loss = 0.2460\n",
      "epoch 26/100, step 19/62,  total step 1569/6200, training_loss = 0.3357\n",
      "epoch 26/100, step 20/62,  total step 1570/6200, training_loss = 0.2749\n",
      "epoch 26/100, step 21/62,  total step 1571/6200, training_loss = 0.3044\n",
      "epoch 26/100, step 22/62,  total step 1572/6200, training_loss = 0.3966\n",
      "epoch 26/100, step 23/62,  total step 1573/6200, training_loss = 0.2931\n",
      "epoch 26/100, step 24/62,  total step 1574/6200, training_loss = 0.2219\n",
      "epoch 26/100, step 25/62,  total step 1575/6200, training_loss = 0.2378\n",
      "epoch 26/100, step 26/62,  total step 1576/6200, training_loss = 0.1996\n",
      "epoch 26/100, step 27/62,  total step 1577/6200, training_loss = 0.2526\n",
      "epoch 26/100, step 28/62,  total step 1578/6200, training_loss = 0.3955\n",
      "epoch 26/100, step 29/62,  total step 1579/6200, training_loss = 0.3298\n",
      "epoch 26/100, step 30/62,  total step 1580/6200, training_loss = 0.2981\n",
      "epoch 26/100, step 31/62,  total step 1581/6200, training_loss = 0.1991\n",
      "epoch 26/100, step 32/62,  total step 1582/6200, training_loss = 0.2526\n",
      "epoch 26/100, step 33/62,  total step 1583/6200, training_loss = 0.3737\n",
      "epoch 26/100, step 34/62,  total step 1584/6200, training_loss = 0.2294\n",
      "epoch 26/100, step 35/62,  total step 1585/6200, training_loss = 0.2533\n",
      "epoch 26/100, step 36/62,  total step 1586/6200, training_loss = 0.3271\n",
      "epoch 26/100, step 37/62,  total step 1587/6200, training_loss = 0.2722\n",
      "epoch 26/100, step 38/62,  total step 1588/6200, training_loss = 0.2789\n",
      "epoch 26/100, step 39/62,  total step 1589/6200, training_loss = 0.2844\n",
      "epoch 26/100, step 40/62,  total step 1590/6200, training_loss = 0.3175\n",
      "epoch 26/100, step 41/62,  total step 1591/6200, training_loss = 0.2453\n",
      "epoch 26/100, step 42/62,  total step 1592/6200, training_loss = 0.2780\n",
      "epoch 26/100, step 43/62,  total step 1593/6200, training_loss = 0.2402\n",
      "epoch 26/100, step 44/62,  total step 1594/6200, training_loss = 0.2532\n",
      "epoch 26/100, step 45/62,  total step 1595/6200, training_loss = 0.1686\n",
      "epoch 26/100, step 46/62,  total step 1596/6200, training_loss = 0.2111\n",
      "epoch 26/100, step 47/62,  total step 1597/6200, training_loss = 0.3109\n",
      "epoch 26/100, step 48/62,  total step 1598/6200, training_loss = 0.2180\n",
      "epoch 26/100, step 49/62,  total step 1599/6200, training_loss = 0.1957\n",
      "epoch 26/100, step 50/62,  total step 1600/6200, training_loss = 0.2399\n",
      "epoch 26/100, step 51/62,  total step 1601/6200, training_loss = 0.2650\n",
      "epoch 26/100, step 52/62,  total step 1602/6200, training_loss = 0.2180\n",
      "epoch 26/100, step 53/62,  total step 1603/6200, training_loss = 0.2656\n",
      "epoch 26/100, step 54/62,  total step 1604/6200, training_loss = 0.2303\n",
      "epoch 26/100, step 55/62,  total step 1605/6200, training_loss = 0.2228\n",
      "epoch 26/100, step 56/62,  total step 1606/6200, training_loss = 0.2816\n",
      "epoch 26/100, step 57/62,  total step 1607/6200, training_loss = 0.2887\n",
      "epoch 26/100, step 58/62,  total step 1608/6200, training_loss = 0.3629\n",
      "epoch 26/100, step 59/62,  total step 1609/6200, training_loss = 0.2303\n",
      "epoch 26/100, step 60/62,  total step 1610/6200, training_loss = 0.3344\n",
      "epoch 26/100, step 61/62,  total step 1611/6200, training_loss = 0.1831\n",
      "epoch 26/100, step 62/62,  total step 1612/6200, training_loss = 0.2795 | avg loss: 0.2725 Dice Metric:   0.685\n",
      "epoch 27/100, step 1/62,  total step 1613/6200, training_loss = 0.2800\n",
      "epoch 27/100, step 2/62,  total step 1614/6200, training_loss = 0.2378\n",
      "epoch 27/100, step 3/62,  total step 1615/6200, training_loss = 0.3060\n",
      "epoch 27/100, step 4/62,  total step 1616/6200, training_loss = 0.4206\n",
      "epoch 27/100, step 5/62,  total step 1617/6200, training_loss = 0.1610\n",
      "epoch 27/100, step 6/62,  total step 1618/6200, training_loss = 0.2291\n",
      "epoch 27/100, step 7/62,  total step 1619/6200, training_loss = 0.2223\n",
      "epoch 27/100, step 8/62,  total step 1620/6200, training_loss = 0.3099\n",
      "epoch 27/100, step 9/62,  total step 1621/6200, training_loss = 0.2275\n",
      "epoch 27/100, step 10/62,  total step 1622/6200, training_loss = 0.2403\n",
      "epoch 27/100, step 11/62,  total step 1623/6200, training_loss = 0.1777\n",
      "epoch 27/100, step 12/62,  total step 1624/6200, training_loss = 0.2059\n",
      "epoch 27/100, step 13/62,  total step 1625/6200, training_loss = 0.2378\n",
      "epoch 27/100, step 14/62,  total step 1626/6200, training_loss = 0.2769\n",
      "epoch 27/100, step 15/62,  total step 1627/6200, training_loss = 0.4163\n",
      "epoch 27/100, step 16/62,  total step 1628/6200, training_loss = 0.3760\n",
      "epoch 27/100, step 17/62,  total step 1629/6200, training_loss = 0.2460\n",
      "epoch 27/100, step 18/62,  total step 1630/6200, training_loss = 0.2300\n",
      "epoch 27/100, step 19/62,  total step 1631/6200, training_loss = 0.3317\n",
      "epoch 27/100, step 20/62,  total step 1632/6200, training_loss = 0.2668\n",
      "epoch 27/100, step 21/62,  total step 1633/6200, training_loss = 0.2548\n",
      "epoch 27/100, step 22/62,  total step 1634/6200, training_loss = 0.3846\n",
      "epoch 27/100, step 23/62,  total step 1635/6200, training_loss = 0.3006\n",
      "epoch 27/100, step 24/62,  total step 1636/6200, training_loss = 0.2377\n",
      "epoch 27/100, step 25/62,  total step 1637/6200, training_loss = 0.2595\n",
      "epoch 27/100, step 26/62,  total step 1638/6200, training_loss = 0.1840\n",
      "epoch 27/100, step 27/62,  total step 1639/6200, training_loss = 0.2635\n",
      "epoch 27/100, step 28/62,  total step 1640/6200, training_loss = 0.3887\n",
      "epoch 27/100, step 29/62,  total step 1641/6200, training_loss = 0.2930\n",
      "epoch 27/100, step 30/62,  total step 1642/6200, training_loss = 0.3019\n",
      "epoch 27/100, step 31/62,  total step 1643/6200, training_loss = 0.1998\n",
      "epoch 27/100, step 32/62,  total step 1644/6200, training_loss = 0.2403\n",
      "epoch 27/100, step 33/62,  total step 1645/6200, training_loss = 0.3707\n",
      "epoch 27/100, step 34/62,  total step 1646/6200, training_loss = 0.2362\n",
      "epoch 27/100, step 35/62,  total step 1647/6200, training_loss = 0.2744\n",
      "epoch 27/100, step 36/62,  total step 1648/6200, training_loss = 0.3491\n",
      "epoch 27/100, step 37/62,  total step 1649/6200, training_loss = 0.2879\n",
      "epoch 27/100, step 38/62,  total step 1650/6200, training_loss = 0.3103\n",
      "epoch 27/100, step 39/62,  total step 1651/6200, training_loss = 0.3143\n",
      "epoch 27/100, step 40/62,  total step 1652/6200, training_loss = 0.3059\n",
      "epoch 27/100, step 41/62,  total step 1653/6200, training_loss = 0.2139\n",
      "epoch 27/100, step 42/62,  total step 1654/6200, training_loss = 0.2467\n",
      "epoch 27/100, step 43/62,  total step 1655/6200, training_loss = 0.2417\n",
      "epoch 27/100, step 44/62,  total step 1656/6200, training_loss = 0.2425\n",
      "epoch 27/100, step 45/62,  total step 1657/6200, training_loss = 0.1754\n",
      "epoch 27/100, step 46/62,  total step 1658/6200, training_loss = 0.1917\n",
      "epoch 27/100, step 47/62,  total step 1659/6200, training_loss = 0.3101\n",
      "epoch 27/100, step 48/62,  total step 1660/6200, training_loss = 0.2018\n",
      "epoch 27/100, step 49/62,  total step 1661/6200, training_loss = 0.1866\n",
      "epoch 27/100, step 50/62,  total step 1662/6200, training_loss = 0.2327\n",
      "epoch 27/100, step 51/62,  total step 1663/6200, training_loss = 0.2445\n",
      "epoch 27/100, step 52/62,  total step 1664/6200, training_loss = 0.1871\n",
      "epoch 27/100, step 53/62,  total step 1665/6200, training_loss = 0.2388\n",
      "epoch 27/100, step 54/62,  total step 1666/6200, training_loss = 0.2257\n",
      "epoch 27/100, step 55/62,  total step 1667/6200, training_loss = 0.2041\n",
      "epoch 27/100, step 56/62,  total step 1668/6200, training_loss = 0.2618\n",
      "epoch 27/100, step 57/62,  total step 1669/6200, training_loss = 0.2612\n",
      "epoch 27/100, step 58/62,  total step 1670/6200, training_loss = 0.3649\n",
      "epoch 27/100, step 59/62,  total step 1671/6200, training_loss = 0.2177\n",
      "epoch 27/100, step 60/62,  total step 1672/6200, training_loss = 0.3297\n",
      "epoch 27/100, step 61/62,  total step 1673/6200, training_loss = 0.1583\n",
      "epoch 27/100, step 62/62,  total step 1674/6200, training_loss = 0.2438 | avg loss: 0.2635 Dice Metric:   0.673\n",
      "epoch 28/100, step 1/62,  total step 1675/6200, training_loss = 0.2559\n",
      "epoch 28/100, step 2/62,  total step 1676/6200, training_loss = 0.2317\n",
      "epoch 28/100, step 3/62,  total step 1677/6200, training_loss = 0.3082\n",
      "epoch 28/100, step 4/62,  total step 1678/6200, training_loss = 0.3883\n",
      "epoch 28/100, step 5/62,  total step 1679/6200, training_loss = 0.1809\n",
      "epoch 28/100, step 6/62,  total step 1680/6200, training_loss = 0.2067\n",
      "epoch 28/100, step 7/62,  total step 1681/6200, training_loss = 0.2333\n",
      "epoch 28/100, step 8/62,  total step 1682/6200, training_loss = 0.2675\n",
      "epoch 28/100, step 9/62,  total step 1683/6200, training_loss = 0.2102\n",
      "epoch 28/100, step 10/62,  total step 1684/6200, training_loss = 0.2833\n",
      "epoch 28/100, step 11/62,  total step 1685/6200, training_loss = 0.1963\n",
      "epoch 28/100, step 12/62,  total step 1686/6200, training_loss = 0.1745\n",
      "epoch 28/100, step 13/62,  total step 1687/6200, training_loss = 0.2697\n",
      "epoch 28/100, step 14/62,  total step 1688/6200, training_loss = 0.2899\n",
      "epoch 28/100, step 15/62,  total step 1689/6200, training_loss = 0.4240\n",
      "epoch 28/100, step 16/62,  total step 1690/6200, training_loss = 0.3821\n",
      "epoch 28/100, step 17/62,  total step 1691/6200, training_loss = 0.2283\n",
      "epoch 28/100, step 18/62,  total step 1692/6200, training_loss = 0.2320\n",
      "epoch 28/100, step 19/62,  total step 1693/6200, training_loss = 0.3340\n",
      "epoch 28/100, step 20/62,  total step 1694/6200, training_loss = 0.2717\n",
      "epoch 28/100, step 21/62,  total step 1695/6200, training_loss = 0.3017\n",
      "epoch 28/100, step 22/62,  total step 1696/6200, training_loss = 0.4139\n",
      "epoch 28/100, step 23/62,  total step 1697/6200, training_loss = 0.2686\n",
      "epoch 28/100, step 24/62,  total step 1698/6200, training_loss = 0.2162\n",
      "epoch 28/100, step 25/62,  total step 1699/6200, training_loss = 0.2322\n",
      "epoch 28/100, step 26/62,  total step 1700/6200, training_loss = 0.1794\n",
      "epoch 28/100, step 27/62,  total step 1701/6200, training_loss = 0.2421\n",
      "epoch 28/100, step 28/62,  total step 1702/6200, training_loss = 0.3857\n",
      "epoch 28/100, step 29/62,  total step 1703/6200, training_loss = 0.2949\n",
      "epoch 28/100, step 30/62,  total step 1704/6200, training_loss = 0.2671\n",
      "epoch 28/100, step 31/62,  total step 1705/6200, training_loss = 0.1888\n",
      "epoch 28/100, step 32/62,  total step 1706/6200, training_loss = 0.2109\n",
      "epoch 28/100, step 33/62,  total step 1707/6200, training_loss = 0.3216\n",
      "epoch 28/100, step 34/62,  total step 1708/6200, training_loss = 0.2312\n",
      "epoch 28/100, step 35/62,  total step 1709/6200, training_loss = 0.2500\n",
      "epoch 28/100, step 36/62,  total step 1710/6200, training_loss = 0.3431\n",
      "epoch 28/100, step 37/62,  total step 1711/6200, training_loss = 0.2554\n",
      "epoch 28/100, step 38/62,  total step 1712/6200, training_loss = 0.3025\n",
      "epoch 28/100, step 39/62,  total step 1713/6200, training_loss = 0.2754\n",
      "epoch 28/100, step 40/62,  total step 1714/6200, training_loss = 0.3053\n",
      "epoch 28/100, step 41/62,  total step 1715/6200, training_loss = 0.2228\n",
      "epoch 28/100, step 42/62,  total step 1716/6200, training_loss = 0.2392\n",
      "epoch 28/100, step 43/62,  total step 1717/6200, training_loss = 0.2559\n",
      "epoch 28/100, step 44/62,  total step 1718/6200, training_loss = 0.2660\n",
      "epoch 28/100, step 45/62,  total step 1719/6200, training_loss = 0.1771\n",
      "epoch 28/100, step 46/62,  total step 1720/6200, training_loss = 0.2035\n",
      "epoch 28/100, step 47/62,  total step 1721/6200, training_loss = 0.2945\n",
      "epoch 28/100, step 48/62,  total step 1722/6200, training_loss = 0.1971\n",
      "epoch 28/100, step 49/62,  total step 1723/6200, training_loss = 0.2051\n",
      "epoch 28/100, step 50/62,  total step 1724/6200, training_loss = 0.2805\n",
      "epoch 28/100, step 51/62,  total step 1725/6200, training_loss = 0.2201\n",
      "epoch 28/100, step 52/62,  total step 1726/6200, training_loss = 0.2215\n",
      "epoch 28/100, step 53/62,  total step 1727/6200, training_loss = 0.2119\n",
      "epoch 28/100, step 54/62,  total step 1728/6200, training_loss = 0.2663\n",
      "epoch 28/100, step 55/62,  total step 1729/6200, training_loss = 0.2238\n",
      "epoch 28/100, step 56/62,  total step 1730/6200, training_loss = 0.2655\n",
      "epoch 28/100, step 57/62,  total step 1731/6200, training_loss = 0.2646\n",
      "epoch 28/100, step 58/62,  total step 1732/6200, training_loss = 0.3577\n",
      "epoch 28/100, step 59/62,  total step 1733/6200, training_loss = 0.2024\n",
      "epoch 28/100, step 60/62,  total step 1734/6200, training_loss = 0.3168\n",
      "epoch 28/100, step 61/62,  total step 1735/6200, training_loss = 0.1837\n",
      "epoch 28/100, step 62/62,  total step 1736/6200, training_loss = 0.3193 | avg loss: 0.2621 Dice Metric:   0.665\n",
      "epoch 29/100, step 1/62,  total step 1737/6200, training_loss = 0.2801\n",
      "epoch 29/100, step 2/62,  total step 1738/6200, training_loss = 0.2295\n",
      "epoch 29/100, step 3/62,  total step 1739/6200, training_loss = 0.2774\n",
      "epoch 29/100, step 4/62,  total step 1740/6200, training_loss = 0.4071\n",
      "epoch 29/100, step 5/62,  total step 1741/6200, training_loss = 0.1600\n",
      "epoch 29/100, step 6/62,  total step 1742/6200, training_loss = 0.2339\n",
      "epoch 29/100, step 7/62,  total step 1743/6200, training_loss = 0.1981\n",
      "epoch 29/100, step 8/62,  total step 1744/6200, training_loss = 0.2935\n",
      "epoch 29/100, step 9/62,  total step 1745/6200, training_loss = 0.2130\n",
      "epoch 29/100, step 10/62,  total step 1746/6200, training_loss = 0.2433\n",
      "epoch 29/100, step 11/62,  total step 1747/6200, training_loss = 0.1894\n",
      "epoch 29/100, step 12/62,  total step 1748/6200, training_loss = 0.2216\n",
      "epoch 29/100, step 13/62,  total step 1749/6200, training_loss = 0.2694\n",
      "epoch 29/100, step 14/62,  total step 1750/6200, training_loss = 0.2798\n",
      "epoch 29/100, step 15/62,  total step 1751/6200, training_loss = 0.4262\n",
      "epoch 29/100, step 16/62,  total step 1752/6200, training_loss = 0.3768\n",
      "epoch 29/100, step 17/62,  total step 1753/6200, training_loss = 0.2129\n",
      "epoch 29/100, step 18/62,  total step 1754/6200, training_loss = 0.2593\n",
      "epoch 29/100, step 19/62,  total step 1755/6200, training_loss = 0.3009\n",
      "epoch 29/100, step 20/62,  total step 1756/6200, training_loss = 0.3005\n",
      "epoch 29/100, step 21/62,  total step 1757/6200, training_loss = 0.2740\n",
      "epoch 29/100, step 22/62,  total step 1758/6200, training_loss = 0.3845\n",
      "epoch 29/100, step 23/62,  total step 1759/6200, training_loss = 0.2643\n",
      "epoch 29/100, step 24/62,  total step 1760/6200, training_loss = 0.2105\n",
      "epoch 29/100, step 25/62,  total step 1761/6200, training_loss = 0.2428\n",
      "epoch 29/100, step 26/62,  total step 1762/6200, training_loss = 0.2056\n",
      "epoch 29/100, step 27/62,  total step 1763/6200, training_loss = 0.2481\n",
      "epoch 29/100, step 28/62,  total step 1764/6200, training_loss = 0.3693\n",
      "epoch 29/100, step 29/62,  total step 1765/6200, training_loss = 0.3055\n",
      "epoch 29/100, step 30/62,  total step 1766/6200, training_loss = 0.2910\n",
      "epoch 29/100, step 31/62,  total step 1767/6200, training_loss = 0.1809\n",
      "epoch 29/100, step 32/62,  total step 1768/6200, training_loss = 0.2567\n",
      "epoch 29/100, step 33/62,  total step 1769/6200, training_loss = 0.3614\n",
      "epoch 29/100, step 34/62,  total step 1770/6200, training_loss = 0.2294\n",
      "epoch 29/100, step 35/62,  total step 1771/6200, training_loss = 0.2688\n",
      "epoch 29/100, step 36/62,  total step 1772/6200, training_loss = 0.3324\n",
      "epoch 29/100, step 37/62,  total step 1773/6200, training_loss = 0.2636\n",
      "epoch 29/100, step 38/62,  total step 1774/6200, training_loss = 0.2900\n",
      "epoch 29/100, step 39/62,  total step 1775/6200, training_loss = 0.3186\n",
      "epoch 29/100, step 40/62,  total step 1776/6200, training_loss = 0.3145\n",
      "epoch 29/100, step 41/62,  total step 1777/6200, training_loss = 0.2419\n",
      "epoch 29/100, step 42/62,  total step 1778/6200, training_loss = 0.2611\n",
      "epoch 29/100, step 43/62,  total step 1779/6200, training_loss = 0.2263\n",
      "epoch 29/100, step 44/62,  total step 1780/6200, training_loss = 0.2475\n",
      "epoch 29/100, step 45/62,  total step 1781/6200, training_loss = 0.1667\n",
      "epoch 29/100, step 46/62,  total step 1782/6200, training_loss = 0.2263\n",
      "epoch 29/100, step 47/62,  total step 1783/6200, training_loss = 0.3261\n",
      "epoch 29/100, step 48/62,  total step 1784/6200, training_loss = 0.1694\n",
      "epoch 29/100, step 49/62,  total step 1785/6200, training_loss = 0.1870\n",
      "epoch 29/100, step 50/62,  total step 1786/6200, training_loss = 0.2517\n",
      "epoch 29/100, step 51/62,  total step 1787/6200, training_loss = 0.2307\n",
      "epoch 29/100, step 52/62,  total step 1788/6200, training_loss = 0.2191\n",
      "epoch 29/100, step 53/62,  total step 1789/6200, training_loss = 0.2163\n",
      "epoch 29/100, step 54/62,  total step 1790/6200, training_loss = 0.2082\n",
      "epoch 29/100, step 55/62,  total step 1791/6200, training_loss = 0.2208\n",
      "epoch 29/100, step 56/62,  total step 1792/6200, training_loss = 0.2821\n",
      "epoch 29/100, step 57/62,  total step 1793/6200, training_loss = 0.2855\n",
      "epoch 29/100, step 58/62,  total step 1794/6200, training_loss = 0.3819\n",
      "epoch 29/100, step 59/62,  total step 1795/6200, training_loss = 0.2132\n",
      "epoch 29/100, step 60/62,  total step 1796/6200, training_loss = 0.3783\n",
      "epoch 29/100, step 61/62,  total step 1797/6200, training_loss = 0.1723\n",
      "epoch 29/100, step 62/62,  total step 1798/6200, training_loss = 0.2567 | avg loss: 0.2638 Dice Metric:   0.669\n",
      "epoch 30/100, step 1/62,  total step 1799/6200, training_loss = 0.2556\n",
      "epoch 30/100, step 2/62,  total step 1800/6200, training_loss = 0.2319\n",
      "epoch 30/100, step 3/62,  total step 1801/6200, training_loss = 0.2787\n",
      "epoch 30/100, step 4/62,  total step 1802/6200, training_loss = 0.4019\n",
      "epoch 30/100, step 5/62,  total step 1803/6200, training_loss = 0.1631\n",
      "epoch 30/100, step 6/62,  total step 1804/6200, training_loss = 0.2120\n",
      "epoch 30/100, step 7/62,  total step 1805/6200, training_loss = 0.2211\n",
      "epoch 30/100, step 8/62,  total step 1806/6200, training_loss = 0.3052\n",
      "epoch 30/100, step 9/62,  total step 1807/6200, training_loss = 0.2670\n",
      "epoch 30/100, step 10/62,  total step 1808/6200, training_loss = 0.2685\n",
      "epoch 30/100, step 11/62,  total step 1809/6200, training_loss = 0.1713\n",
      "epoch 30/100, step 12/62,  total step 1810/6200, training_loss = 0.2028\n",
      "epoch 30/100, step 13/62,  total step 1811/6200, training_loss = 0.2719\n",
      "epoch 30/100, step 14/62,  total step 1812/6200, training_loss = 0.3016\n",
      "epoch 30/100, step 15/62,  total step 1813/6200, training_loss = 0.4054\n",
      "epoch 30/100, step 16/62,  total step 1814/6200, training_loss = 0.3690\n",
      "epoch 30/100, step 17/62,  total step 1815/6200, training_loss = 0.2398\n",
      "epoch 30/100, step 18/62,  total step 1816/6200, training_loss = 0.2598\n",
      "epoch 30/100, step 19/62,  total step 1817/6200, training_loss = 0.3097\n",
      "epoch 30/100, step 20/62,  total step 1818/6200, training_loss = 0.2627\n",
      "epoch 30/100, step 21/62,  total step 1819/6200, training_loss = 0.2898\n",
      "epoch 30/100, step 22/62,  total step 1820/6200, training_loss = 0.3651\n",
      "epoch 30/100, step 23/62,  total step 1821/6200, training_loss = 0.2631\n",
      "epoch 30/100, step 24/62,  total step 1822/6200, training_loss = 0.2096\n",
      "epoch 30/100, step 25/62,  total step 1823/6200, training_loss = 0.2379\n",
      "epoch 30/100, step 26/62,  total step 1824/6200, training_loss = 0.1762\n",
      "epoch 30/100, step 27/62,  total step 1825/6200, training_loss = 0.2606\n",
      "epoch 30/100, step 28/62,  total step 1826/6200, training_loss = 0.3429\n",
      "epoch 30/100, step 29/62,  total step 1827/6200, training_loss = 0.3033\n",
      "epoch 30/100, step 30/62,  total step 1828/6200, training_loss = 0.2901\n",
      "epoch 30/100, step 31/62,  total step 1829/6200, training_loss = 0.1920\n",
      "epoch 30/100, step 32/62,  total step 1830/6200, training_loss = 0.2333\n",
      "epoch 30/100, step 33/62,  total step 1831/6200, training_loss = 0.3453\n",
      "epoch 30/100, step 34/62,  total step 1832/6200, training_loss = 0.2260\n",
      "epoch 30/100, step 35/62,  total step 1833/6200, training_loss = 0.2560\n",
      "epoch 30/100, step 36/62,  total step 1834/6200, training_loss = 0.3341\n",
      "epoch 30/100, step 37/62,  total step 1835/6200, training_loss = 0.2797\n",
      "epoch 30/100, step 38/62,  total step 1836/6200, training_loss = 0.3084\n",
      "epoch 30/100, step 39/62,  total step 1837/6200, training_loss = 0.2979\n",
      "epoch 30/100, step 40/62,  total step 1838/6200, training_loss = 0.3165\n",
      "epoch 30/100, step 41/62,  total step 1839/6200, training_loss = 0.2296\n",
      "epoch 30/100, step 42/62,  total step 1840/6200, training_loss = 0.2498\n",
      "epoch 30/100, step 43/62,  total step 1841/6200, training_loss = 0.2392\n",
      "epoch 30/100, step 44/62,  total step 1842/6200, training_loss = 0.2858\n",
      "epoch 30/100, step 45/62,  total step 1843/6200, training_loss = 0.1608\n",
      "epoch 30/100, step 46/62,  total step 1844/6200, training_loss = 0.1969\n",
      "epoch 30/100, step 47/62,  total step 1845/6200, training_loss = 0.3163\n",
      "epoch 30/100, step 48/62,  total step 1846/6200, training_loss = 0.1790\n",
      "epoch 30/100, step 49/62,  total step 1847/6200, training_loss = 0.1869\n",
      "epoch 30/100, step 50/62,  total step 1848/6200, training_loss = 0.2718\n",
      "epoch 30/100, step 51/62,  total step 1849/6200, training_loss = 0.2579\n",
      "epoch 30/100, step 52/62,  total step 1850/6200, training_loss = 0.2245\n",
      "epoch 30/100, step 53/62,  total step 1851/6200, training_loss = 0.2418\n",
      "epoch 30/100, step 54/62,  total step 1852/6200, training_loss = 0.2006\n",
      "epoch 30/100, step 55/62,  total step 1853/6200, training_loss = 0.2158\n",
      "epoch 30/100, step 56/62,  total step 1854/6200, training_loss = 0.2957\n",
      "epoch 30/100, step 57/62,  total step 1855/6200, training_loss = 0.2808\n",
      "epoch 30/100, step 58/62,  total step 1856/6200, training_loss = 0.3756\n",
      "epoch 30/100, step 59/62,  total step 1857/6200, training_loss = 0.2286\n",
      "epoch 30/100, step 60/62,  total step 1858/6200, training_loss = 0.3277\n",
      "epoch 30/100, step 61/62,  total step 1859/6200, training_loss = 0.1693\n",
      "epoch 30/100, step 62/62,  total step 1860/6200, training_loss = 0.2821 | avg loss: 0.2636 Dice Metric:   0.663\n",
      "epoch 31/100, step 1/62,  total step 1861/6200, training_loss = 0.2512\n",
      "epoch 31/100, step 2/62,  total step 1862/6200, training_loss = 0.2319\n",
      "epoch 31/100, step 3/62,  total step 1863/6200, training_loss = 0.2782\n",
      "epoch 31/100, step 4/62,  total step 1864/6200, training_loss = 0.3881\n",
      "epoch 31/100, step 5/62,  total step 1865/6200, training_loss = 0.1695\n",
      "epoch 31/100, step 6/62,  total step 1866/6200, training_loss = 0.2461\n",
      "epoch 31/100, step 7/62,  total step 1867/6200, training_loss = 0.2145\n",
      "epoch 31/100, step 8/62,  total step 1868/6200, training_loss = 0.2546\n",
      "epoch 31/100, step 9/62,  total step 1869/6200, training_loss = 0.2143\n",
      "epoch 31/100, step 10/62,  total step 1870/6200, training_loss = 0.2621\n",
      "epoch 31/100, step 11/62,  total step 1871/6200, training_loss = 0.1767\n",
      "epoch 31/100, step 12/62,  total step 1872/6200, training_loss = 0.1970\n",
      "epoch 31/100, step 13/62,  total step 1873/6200, training_loss = 0.2524\n",
      "epoch 31/100, step 14/62,  total step 1874/6200, training_loss = 0.2825\n",
      "epoch 31/100, step 15/62,  total step 1875/6200, training_loss = 0.3955\n",
      "epoch 31/100, step 16/62,  total step 1876/6200, training_loss = 0.3618\n",
      "epoch 31/100, step 17/62,  total step 1877/6200, training_loss = 0.2601\n",
      "epoch 31/100, step 18/62,  total step 1878/6200, training_loss = 0.2227\n",
      "epoch 31/100, step 19/62,  total step 1879/6200, training_loss = 0.3219\n",
      "epoch 31/100, step 20/62,  total step 1880/6200, training_loss = 0.2708\n",
      "epoch 31/100, step 21/62,  total step 1881/6200, training_loss = 0.2761\n",
      "epoch 31/100, step 22/62,  total step 1882/6200, training_loss = 0.3880\n",
      "epoch 31/100, step 23/62,  total step 1883/6200, training_loss = 0.2942\n",
      "epoch 31/100, step 24/62,  total step 1884/6200, training_loss = 0.1944\n",
      "epoch 31/100, step 25/62,  total step 1885/6200, training_loss = 0.2595\n",
      "epoch 31/100, step 26/62,  total step 1886/6200, training_loss = 0.1778\n",
      "epoch 31/100, step 27/62,  total step 1887/6200, training_loss = 0.2381\n",
      "epoch 31/100, step 28/62,  total step 1888/6200, training_loss = 0.3382\n",
      "epoch 31/100, step 29/62,  total step 1889/6200, training_loss = 0.2885\n",
      "epoch 31/100, step 30/62,  total step 1890/6200, training_loss = 0.2922\n",
      "epoch 31/100, step 31/62,  total step 1891/6200, training_loss = 0.1870\n",
      "epoch 31/100, step 32/62,  total step 1892/6200, training_loss = 0.2083\n",
      "epoch 31/100, step 33/62,  total step 1893/6200, training_loss = 0.3688\n",
      "epoch 31/100, step 34/62,  total step 1894/6200, training_loss = 0.2231\n",
      "epoch 31/100, step 35/62,  total step 1895/6200, training_loss = 0.2571\n",
      "epoch 31/100, step 36/62,  total step 1896/6200, training_loss = 0.3424\n",
      "epoch 31/100, step 37/62,  total step 1897/6200, training_loss = 0.2542\n",
      "epoch 31/100, step 38/62,  total step 1898/6200, training_loss = 0.2710\n",
      "epoch 31/100, step 39/62,  total step 1899/6200, training_loss = 0.2968\n",
      "epoch 31/100, step 40/62,  total step 1900/6200, training_loss = 0.3082\n",
      "epoch 31/100, step 41/62,  total step 1901/6200, training_loss = 0.2026\n",
      "epoch 31/100, step 42/62,  total step 1902/6200, training_loss = 0.2533\n",
      "epoch 31/100, step 43/62,  total step 1903/6200, training_loss = 0.2244\n",
      "epoch 31/100, step 44/62,  total step 1904/6200, training_loss = 0.2842\n",
      "epoch 31/100, step 45/62,  total step 1905/6200, training_loss = 0.1629\n",
      "epoch 31/100, step 46/62,  total step 1906/6200, training_loss = 0.2124\n",
      "epoch 31/100, step 47/62,  total step 1907/6200, training_loss = 0.3015\n",
      "epoch 31/100, step 48/62,  total step 1908/6200, training_loss = 0.1863\n",
      "epoch 31/100, step 49/62,  total step 1909/6200, training_loss = 0.2197\n",
      "epoch 31/100, step 50/62,  total step 1910/6200, training_loss = 0.2619\n",
      "epoch 31/100, step 51/62,  total step 1911/6200, training_loss = 0.2508\n",
      "epoch 31/100, step 52/62,  total step 1912/6200, training_loss = 0.2025\n",
      "epoch 31/100, step 53/62,  total step 1913/6200, training_loss = 0.2481\n",
      "epoch 31/100, step 54/62,  total step 1914/6200, training_loss = 0.2187\n",
      "epoch 31/100, step 55/62,  total step 1915/6200, training_loss = 0.2090\n",
      "epoch 31/100, step 56/62,  total step 1916/6200, training_loss = 0.2573\n",
      "epoch 31/100, step 57/62,  total step 1917/6200, training_loss = 0.2676\n",
      "epoch 31/100, step 58/62,  total step 1918/6200, training_loss = 0.3642\n",
      "epoch 31/100, step 59/62,  total step 1919/6200, training_loss = 0.1981\n",
      "epoch 31/100, step 60/62,  total step 1920/6200, training_loss = 0.3264\n",
      "epoch 31/100, step 61/62,  total step 1921/6200, training_loss = 0.1705\n",
      "epoch 31/100, step 62/62,  total step 1922/6200, training_loss = 0.3264 | avg loss: 0.2591 Dice Metric:   0.681\n",
      "epoch 32/100, step 1/62,  total step 1923/6200, training_loss = 0.2632\n",
      "epoch 32/100, step 2/62,  total step 1924/6200, training_loss = 0.2280\n",
      "epoch 32/100, step 3/62,  total step 1925/6200, training_loss = 0.2940\n",
      "epoch 32/100, step 4/62,  total step 1926/6200, training_loss = 0.4180\n",
      "epoch 32/100, step 5/62,  total step 1927/6200, training_loss = 0.1712\n",
      "epoch 32/100, step 6/62,  total step 1928/6200, training_loss = 0.2680\n",
      "epoch 32/100, step 7/62,  total step 1929/6200, training_loss = 0.2046\n",
      "epoch 32/100, step 8/62,  total step 1930/6200, training_loss = 0.2803\n",
      "epoch 32/100, step 9/62,  total step 1931/6200, training_loss = 0.2343\n",
      "epoch 32/100, step 10/62,  total step 1932/6200, training_loss = 0.3142\n",
      "epoch 32/100, step 11/62,  total step 1933/6200, training_loss = 0.2205\n",
      "epoch 32/100, step 12/62,  total step 1934/6200, training_loss = 0.2191\n",
      "epoch 32/100, step 13/62,  total step 1935/6200, training_loss = 0.2417\n",
      "epoch 32/100, step 14/62,  total step 1936/6200, training_loss = 0.2984\n",
      "epoch 32/100, step 15/62,  total step 1937/6200, training_loss = 0.3824\n",
      "epoch 32/100, step 16/62,  total step 1938/6200, training_loss = 0.3593\n",
      "epoch 32/100, step 17/62,  total step 1939/6200, training_loss = 0.2431\n",
      "epoch 32/100, step 18/62,  total step 1940/6200, training_loss = 0.2202\n",
      "epoch 32/100, step 19/62,  total step 1941/6200, training_loss = 0.3302\n",
      "epoch 32/100, step 20/62,  total step 1942/6200, training_loss = 0.2540\n",
      "epoch 32/100, step 21/62,  total step 1943/6200, training_loss = 0.2620\n",
      "epoch 32/100, step 22/62,  total step 1944/6200, training_loss = 0.3666\n",
      "epoch 32/100, step 23/62,  total step 1945/6200, training_loss = 0.3007\n",
      "epoch 32/100, step 24/62,  total step 1946/6200, training_loss = 0.1982\n",
      "epoch 32/100, step 25/62,  total step 1947/6200, training_loss = 0.2445\n",
      "epoch 32/100, step 26/62,  total step 1948/6200, training_loss = 0.1787\n",
      "epoch 32/100, step 27/62,  total step 1949/6200, training_loss = 0.2709\n",
      "epoch 32/100, step 28/62,  total step 1950/6200, training_loss = 0.4032\n",
      "epoch 32/100, step 29/62,  total step 1951/6200, training_loss = 0.2889\n",
      "epoch 32/100, step 30/62,  total step 1952/6200, training_loss = 0.2654\n",
      "epoch 32/100, step 31/62,  total step 1953/6200, training_loss = 0.1799\n",
      "epoch 32/100, step 32/62,  total step 1954/6200, training_loss = 0.2423\n",
      "epoch 32/100, step 33/62,  total step 1955/6200, training_loss = 0.3542\n",
      "epoch 32/100, step 34/62,  total step 1956/6200, training_loss = 0.2320\n",
      "epoch 32/100, step 35/62,  total step 1957/6200, training_loss = 0.2403\n",
      "epoch 32/100, step 36/62,  total step 1958/6200, training_loss = 0.3314\n",
      "epoch 32/100, step 37/62,  total step 1959/6200, training_loss = 0.2660\n",
      "epoch 32/100, step 38/62,  total step 1960/6200, training_loss = 0.2973\n",
      "epoch 32/100, step 39/62,  total step 1961/6200, training_loss = 0.3152\n",
      "epoch 32/100, step 40/62,  total step 1962/6200, training_loss = 0.3325\n",
      "epoch 32/100, step 41/62,  total step 1963/6200, training_loss = 0.2121\n",
      "epoch 32/100, step 42/62,  total step 1964/6200, training_loss = 0.2655\n",
      "epoch 32/100, step 43/62,  total step 1965/6200, training_loss = 0.2114\n",
      "epoch 32/100, step 44/62,  total step 1966/6200, training_loss = 0.2535\n",
      "epoch 32/100, step 45/62,  total step 1967/6200, training_loss = 0.1630\n",
      "epoch 32/100, step 46/62,  total step 1968/6200, training_loss = 0.1752\n",
      "epoch 32/100, step 47/62,  total step 1969/6200, training_loss = 0.2953\n",
      "epoch 32/100, step 48/62,  total step 1970/6200, training_loss = 0.1584\n",
      "epoch 32/100, step 49/62,  total step 1971/6200, training_loss = 0.1624\n",
      "epoch 32/100, step 50/62,  total step 1972/6200, training_loss = 0.2296\n",
      "epoch 32/100, step 51/62,  total step 1973/6200, training_loss = 0.2384\n",
      "epoch 32/100, step 52/62,  total step 1974/6200, training_loss = 0.1885\n",
      "epoch 32/100, step 53/62,  total step 1975/6200, training_loss = 0.2275\n",
      "epoch 32/100, step 54/62,  total step 1976/6200, training_loss = 0.1589\n",
      "epoch 32/100, step 55/62,  total step 1977/6200, training_loss = 0.2003\n",
      "epoch 32/100, step 56/62,  total step 1978/6200, training_loss = 0.2788\n",
      "epoch 32/100, step 57/62,  total step 1979/6200, training_loss = 0.2689\n",
      "epoch 32/100, step 58/62,  total step 1980/6200, training_loss = 0.3656\n",
      "epoch 32/100, step 59/62,  total step 1981/6200, training_loss = 0.2026\n",
      "epoch 32/100, step 60/62,  total step 1982/6200, training_loss = 0.3091\n",
      "epoch 32/100, step 61/62,  total step 1983/6200, training_loss = 0.1600\n",
      "epoch 32/100, step 62/62,  total step 1984/6200, training_loss = 0.2968 | avg loss: 0.2587 Dice Metric:   0.648\n",
      "epoch 33/100, step 1/62,  total step 1985/6200, training_loss = 0.2542\n",
      "epoch 33/100, step 2/62,  total step 1986/6200, training_loss = 0.2102\n",
      "epoch 33/100, step 3/62,  total step 1987/6200, training_loss = 0.2597\n",
      "epoch 33/100, step 4/62,  total step 1988/6200, training_loss = 0.3819\n",
      "epoch 33/100, step 5/62,  total step 1989/6200, training_loss = 0.1583\n",
      "epoch 33/100, step 6/62,  total step 1990/6200, training_loss = 0.2328\n",
      "epoch 33/100, step 7/62,  total step 1991/6200, training_loss = 0.2131\n",
      "epoch 33/100, step 8/62,  total step 1992/6200, training_loss = 0.2799\n",
      "epoch 33/100, step 9/62,  total step 1993/6200, training_loss = 0.2190\n",
      "epoch 33/100, step 10/62,  total step 1994/6200, training_loss = 0.2693\n",
      "epoch 33/100, step 11/62,  total step 1995/6200, training_loss = 0.1785\n",
      "epoch 33/100, step 12/62,  total step 1996/6200, training_loss = 0.2119\n",
      "epoch 33/100, step 13/62,  total step 1997/6200, training_loss = 0.2706\n",
      "epoch 33/100, step 14/62,  total step 1998/6200, training_loss = 0.2689\n",
      "epoch 33/100, step 15/62,  total step 1999/6200, training_loss = 0.3953\n",
      "epoch 33/100, step 16/62,  total step 2000/6200, training_loss = 0.3627\n",
      "epoch 33/100, step 17/62,  total step 2001/6200, training_loss = 0.2353\n",
      "epoch 33/100, step 18/62,  total step 2002/6200, training_loss = 0.2564\n",
      "epoch 33/100, step 19/62,  total step 2003/6200, training_loss = 0.3222\n",
      "epoch 33/100, step 20/62,  total step 2004/6200, training_loss = 0.2720\n",
      "epoch 33/100, step 21/62,  total step 2005/6200, training_loss = 0.2754\n",
      "epoch 33/100, step 22/62,  total step 2006/6200, training_loss = 0.3761\n",
      "epoch 33/100, step 23/62,  total step 2007/6200, training_loss = 0.2793\n",
      "epoch 33/100, step 24/62,  total step 2008/6200, training_loss = 0.1797\n",
      "epoch 33/100, step 25/62,  total step 2009/6200, training_loss = 0.2589\n",
      "epoch 33/100, step 26/62,  total step 2010/6200, training_loss = 0.1718\n",
      "epoch 33/100, step 27/62,  total step 2011/6200, training_loss = 0.2516\n",
      "epoch 33/100, step 28/62,  total step 2012/6200, training_loss = 0.3824\n",
      "epoch 33/100, step 29/62,  total step 2013/6200, training_loss = 0.2804\n",
      "epoch 33/100, step 30/62,  total step 2014/6200, training_loss = 0.2642\n",
      "epoch 33/100, step 31/62,  total step 2015/6200, training_loss = 0.1869\n",
      "epoch 33/100, step 32/62,  total step 2016/6200, training_loss = 0.2049\n",
      "epoch 33/100, step 33/62,  total step 2017/6200, training_loss = 0.3188\n",
      "epoch 33/100, step 34/62,  total step 2018/6200, training_loss = 0.2304\n",
      "epoch 33/100, step 35/62,  total step 2019/6200, training_loss = 0.2360\n",
      "epoch 33/100, step 36/62,  total step 2020/6200, training_loss = 0.3187\n",
      "epoch 33/100, step 37/62,  total step 2021/6200, training_loss = 0.2500\n",
      "epoch 33/100, step 38/62,  total step 2022/6200, training_loss = 0.3032\n",
      "epoch 33/100, step 39/62,  total step 2023/6200, training_loss = 0.3036\n",
      "epoch 33/100, step 40/62,  total step 2024/6200, training_loss = 0.2936\n",
      "epoch 33/100, step 41/62,  total step 2025/6200, training_loss = 0.2173\n",
      "epoch 33/100, step 42/62,  total step 2026/6200, training_loss = 0.2664\n",
      "epoch 33/100, step 43/62,  total step 2027/6200, training_loss = 0.2297\n",
      "epoch 33/100, step 44/62,  total step 2028/6200, training_loss = 0.2404\n",
      "epoch 33/100, step 45/62,  total step 2029/6200, training_loss = 0.1457\n",
      "epoch 33/100, step 46/62,  total step 2030/6200, training_loss = 0.1664\n",
      "epoch 33/100, step 47/62,  total step 2031/6200, training_loss = 0.3158\n",
      "epoch 33/100, step 48/62,  total step 2032/6200, training_loss = 0.1758\n",
      "epoch 33/100, step 49/62,  total step 2033/6200, training_loss = 0.2068\n",
      "epoch 33/100, step 50/62,  total step 2034/6200, training_loss = 0.2504\n",
      "epoch 33/100, step 51/62,  total step 2035/6200, training_loss = 0.2360\n",
      "epoch 33/100, step 52/62,  total step 2036/6200, training_loss = 0.1952\n",
      "epoch 33/100, step 53/62,  total step 2037/6200, training_loss = 0.2457\n",
      "epoch 33/100, step 54/62,  total step 2038/6200, training_loss = 0.1651\n",
      "epoch 33/100, step 55/62,  total step 2039/6200, training_loss = 0.2058\n",
      "epoch 33/100, step 56/62,  total step 2040/6200, training_loss = 0.2547\n",
      "epoch 33/100, step 57/62,  total step 2041/6200, training_loss = 0.2434\n",
      "epoch 33/100, step 58/62,  total step 2042/6200, training_loss = 0.3450\n",
      "epoch 33/100, step 59/62,  total step 2043/6200, training_loss = 0.1823\n",
      "epoch 33/100, step 60/62,  total step 2044/6200, training_loss = 0.2949\n",
      "epoch 33/100, step 61/62,  total step 2045/6200, training_loss = 0.1697\n",
      "epoch 33/100, step 62/62,  total step 2046/6200, training_loss = 0.2738 | avg loss: 0.2523 Dice Metric:   0.673\n",
      "epoch 34/100, step 1/62,  total step 2047/6200, training_loss = 0.2349\n",
      "epoch 34/100, step 2/62,  total step 2048/6200, training_loss = 0.2179\n",
      "epoch 34/100, step 3/62,  total step 2049/6200, training_loss = 0.2582\n",
      "epoch 34/100, step 4/62,  total step 2050/6200, training_loss = 0.4030\n",
      "epoch 34/100, step 5/62,  total step 2051/6200, training_loss = 0.1699\n",
      "epoch 34/100, step 6/62,  total step 2052/6200, training_loss = 0.2485\n",
      "epoch 34/100, step 7/62,  total step 2053/6200, training_loss = 0.2070\n",
      "epoch 34/100, step 8/62,  total step 2054/6200, training_loss = 0.2618\n",
      "epoch 34/100, step 9/62,  total step 2055/6200, training_loss = 0.2077\n",
      "epoch 34/100, step 10/62,  total step 2056/6200, training_loss = 0.2632\n",
      "epoch 34/100, step 11/62,  total step 2057/6200, training_loss = 0.1842\n",
      "epoch 34/100, step 12/62,  total step 2058/6200, training_loss = 0.1958\n",
      "epoch 34/100, step 13/62,  total step 2059/6200, training_loss = 0.2679\n",
      "epoch 34/100, step 14/62,  total step 2060/6200, training_loss = 0.2755\n",
      "epoch 34/100, step 15/62,  total step 2061/6200, training_loss = 0.4149\n",
      "epoch 34/100, step 16/62,  total step 2062/6200, training_loss = 0.3824\n",
      "epoch 34/100, step 17/62,  total step 2063/6200, training_loss = 0.2364\n",
      "epoch 34/100, step 18/62,  total step 2064/6200, training_loss = 0.2187\n",
      "epoch 34/100, step 19/62,  total step 2065/6200, training_loss = 0.3016\n",
      "epoch 34/100, step 20/62,  total step 2066/6200, training_loss = 0.2885\n",
      "epoch 34/100, step 21/62,  total step 2067/6200, training_loss = 0.2638\n",
      "epoch 34/100, step 22/62,  total step 2068/6200, training_loss = 0.3581\n",
      "epoch 34/100, step 23/62,  total step 2069/6200, training_loss = 0.2935\n",
      "epoch 34/100, step 24/62,  total step 2070/6200, training_loss = 0.2101\n",
      "epoch 34/100, step 25/62,  total step 2071/6200, training_loss = 0.2407\n",
      "epoch 34/100, step 26/62,  total step 2072/6200, training_loss = 0.1785\n",
      "epoch 34/100, step 27/62,  total step 2073/6200, training_loss = 0.2571\n",
      "epoch 34/100, step 28/62,  total step 2074/6200, training_loss = 0.3360\n",
      "epoch 34/100, step 29/62,  total step 2075/6200, training_loss = 0.3085\n",
      "epoch 34/100, step 30/62,  total step 2076/6200, training_loss = 0.2928\n",
      "epoch 34/100, step 31/62,  total step 2077/6200, training_loss = 0.1757\n",
      "epoch 34/100, step 32/62,  total step 2078/6200, training_loss = 0.2132\n",
      "epoch 34/100, step 33/62,  total step 2079/6200, training_loss = 0.3675\n",
      "epoch 34/100, step 34/62,  total step 2080/6200, training_loss = 0.2222\n",
      "epoch 34/100, step 35/62,  total step 2081/6200, training_loss = 0.2493\n",
      "epoch 34/100, step 36/62,  total step 2082/6200, training_loss = 0.3297\n",
      "epoch 34/100, step 37/62,  total step 2083/6200, training_loss = 0.2554\n",
      "epoch 34/100, step 38/62,  total step 2084/6200, training_loss = 0.3233\n",
      "epoch 34/100, step 39/62,  total step 2085/6200, training_loss = 0.2862\n",
      "epoch 34/100, step 40/62,  total step 2086/6200, training_loss = 0.2930\n",
      "epoch 34/100, step 41/62,  total step 2087/6200, training_loss = 0.2096\n",
      "epoch 34/100, step 42/62,  total step 2088/6200, training_loss = 0.2490\n",
      "epoch 34/100, step 43/62,  total step 2089/6200, training_loss = 0.2367\n",
      "epoch 34/100, step 44/62,  total step 2090/6200, training_loss = 0.2275\n",
      "epoch 34/100, step 45/62,  total step 2091/6200, training_loss = 0.1565\n",
      "epoch 34/100, step 46/62,  total step 2092/6200, training_loss = 0.1973\n",
      "epoch 34/100, step 47/62,  total step 2093/6200, training_loss = 0.2663\n",
      "epoch 34/100, step 48/62,  total step 2094/6200, training_loss = 0.1773\n",
      "epoch 34/100, step 49/62,  total step 2095/6200, training_loss = 0.2126\n",
      "epoch 34/100, step 50/62,  total step 2096/6200, training_loss = 0.2764\n",
      "epoch 34/100, step 51/62,  total step 2097/6200, training_loss = 0.2413\n",
      "epoch 34/100, step 52/62,  total step 2098/6200, training_loss = 0.2149\n",
      "epoch 34/100, step 53/62,  total step 2099/6200, training_loss = 0.2657\n",
      "epoch 34/100, step 54/62,  total step 2100/6200, training_loss = 0.2304\n",
      "epoch 34/100, step 55/62,  total step 2101/6200, training_loss = 0.2090\n",
      "epoch 34/100, step 56/62,  total step 2102/6200, training_loss = 0.2574\n",
      "epoch 34/100, step 57/62,  total step 2103/6200, training_loss = 0.2630\n",
      "epoch 34/100, step 58/62,  total step 2104/6200, training_loss = 0.3437\n",
      "epoch 34/100, step 59/62,  total step 2105/6200, training_loss = 0.2047\n",
      "epoch 34/100, step 60/62,  total step 2106/6200, training_loss = 0.3857\n",
      "epoch 34/100, step 61/62,  total step 2107/6200, training_loss = 0.1662\n",
      "epoch 34/100, step 62/62,  total step 2108/6200, training_loss = 0.2688 | avg loss: 0.2573 Dice Metric:   0.652\n",
      "epoch 35/100, step 1/62,  total step 2109/6200, training_loss = 0.2321\n",
      "epoch 35/100, step 2/62,  total step 2110/6200, training_loss = 0.1901\n",
      "epoch 35/100, step 3/62,  total step 2111/6200, training_loss = 0.2658\n",
      "epoch 35/100, step 4/62,  total step 2112/6200, training_loss = 0.3829\n",
      "epoch 35/100, step 5/62,  total step 2113/6200, training_loss = 0.1585\n",
      "epoch 35/100, step 6/62,  total step 2114/6200, training_loss = 0.2255\n",
      "epoch 35/100, step 7/62,  total step 2115/6200, training_loss = 0.2191\n",
      "epoch 35/100, step 8/62,  total step 2116/6200, training_loss = 0.2588\n",
      "epoch 35/100, step 9/62,  total step 2117/6200, training_loss = 0.2262\n",
      "epoch 35/100, step 10/62,  total step 2118/6200, training_loss = 0.2530\n",
      "epoch 35/100, step 11/62,  total step 2119/6200, training_loss = 0.1778\n",
      "epoch 35/100, step 12/62,  total step 2120/6200, training_loss = 0.2170\n",
      "epoch 35/100, step 13/62,  total step 2121/6200, training_loss = 0.2660\n",
      "epoch 35/100, step 14/62,  total step 2122/6200, training_loss = 0.2936\n",
      "epoch 35/100, step 15/62,  total step 2123/6200, training_loss = 0.4464\n",
      "epoch 35/100, step 16/62,  total step 2124/6200, training_loss = 0.3701\n",
      "epoch 35/100, step 17/62,  total step 2125/6200, training_loss = 0.2442\n",
      "epoch 35/100, step 18/62,  total step 2126/6200, training_loss = 0.2591\n",
      "epoch 35/100, step 19/62,  total step 2127/6200, training_loss = 0.3114\n",
      "epoch 35/100, step 20/62,  total step 2128/6200, training_loss = 0.2221\n",
      "epoch 35/100, step 21/62,  total step 2129/6200, training_loss = 0.2509\n",
      "epoch 35/100, step 22/62,  total step 2130/6200, training_loss = 0.3867\n",
      "epoch 35/100, step 23/62,  total step 2131/6200, training_loss = 0.3255\n",
      "epoch 35/100, step 24/62,  total step 2132/6200, training_loss = 0.1909\n",
      "epoch 35/100, step 25/62,  total step 2133/6200, training_loss = 0.2388\n",
      "epoch 35/100, step 26/62,  total step 2134/6200, training_loss = 0.1715\n",
      "epoch 35/100, step 27/62,  total step 2135/6200, training_loss = 0.2699\n",
      "epoch 35/100, step 28/62,  total step 2136/6200, training_loss = 0.3816\n",
      "epoch 35/100, step 29/62,  total step 2137/6200, training_loss = 0.2888\n",
      "epoch 35/100, step 30/62,  total step 2138/6200, training_loss = 0.2857\n",
      "epoch 35/100, step 31/62,  total step 2139/6200, training_loss = 0.1900\n",
      "epoch 35/100, step 32/62,  total step 2140/6200, training_loss = 0.2066\n",
      "epoch 35/100, step 33/62,  total step 2141/6200, training_loss = 0.3443\n",
      "epoch 35/100, step 34/62,  total step 2142/6200, training_loss = 0.2227\n",
      "epoch 35/100, step 35/62,  total step 2143/6200, training_loss = 0.2304\n",
      "epoch 35/100, step 36/62,  total step 2144/6200, training_loss = 0.2945\n",
      "epoch 35/100, step 37/62,  total step 2145/6200, training_loss = 0.2380\n",
      "epoch 35/100, step 38/62,  total step 2146/6200, training_loss = 0.2791\n",
      "epoch 35/100, step 39/62,  total step 2147/6200, training_loss = 0.2648\n",
      "epoch 35/100, step 40/62,  total step 2148/6200, training_loss = 0.2680\n",
      "epoch 35/100, step 41/62,  total step 2149/6200, training_loss = 0.2176\n",
      "epoch 35/100, step 42/62,  total step 2150/6200, training_loss = 0.2494\n",
      "epoch 35/100, step 43/62,  total step 2151/6200, training_loss = 0.2194\n",
      "epoch 35/100, step 44/62,  total step 2152/6200, training_loss = 0.2597\n",
      "epoch 35/100, step 45/62,  total step 2153/6200, training_loss = 0.1576\n",
      "epoch 35/100, step 46/62,  total step 2154/6200, training_loss = 0.1981\n",
      "epoch 35/100, step 47/62,  total step 2155/6200, training_loss = 0.2791\n",
      "epoch 35/100, step 48/62,  total step 2156/6200, training_loss = 0.1942\n",
      "epoch 35/100, step 49/62,  total step 2157/6200, training_loss = 0.1975\n",
      "epoch 35/100, step 50/62,  total step 2158/6200, training_loss = 0.2255\n",
      "epoch 35/100, step 51/62,  total step 2159/6200, training_loss = 0.2345\n",
      "epoch 35/100, step 52/62,  total step 2160/6200, training_loss = 0.2055\n",
      "epoch 35/100, step 53/62,  total step 2161/6200, training_loss = 0.2275\n",
      "epoch 35/100, step 54/62,  total step 2162/6200, training_loss = 0.2042\n",
      "epoch 35/100, step 55/62,  total step 2163/6200, training_loss = 0.2028\n",
      "epoch 35/100, step 56/62,  total step 2164/6200, training_loss = 0.2785\n",
      "epoch 35/100, step 57/62,  total step 2165/6200, training_loss = 0.2706\n",
      "epoch 35/100, step 58/62,  total step 2166/6200, training_loss = 0.3580\n",
      "epoch 35/100, step 59/62,  total step 2167/6200, training_loss = 0.2426\n",
      "epoch 35/100, step 60/62,  total step 2168/6200, training_loss = 0.2961\n",
      "epoch 35/100, step 61/62,  total step 2169/6200, training_loss = 0.1550\n",
      "epoch 35/100, step 62/62,  total step 2170/6200, training_loss = 0.2835 | avg loss: 0.2533 Dice Metric:   0.659\n",
      "epoch 36/100, step 1/62,  total step 2171/6200, training_loss = 0.2816\n",
      "epoch 36/100, step 2/62,  total step 2172/6200, training_loss = 0.2291\n",
      "epoch 36/100, step 3/62,  total step 2173/6200, training_loss = 0.2480\n",
      "epoch 36/100, step 4/62,  total step 2174/6200, training_loss = 0.3872\n",
      "epoch 36/100, step 5/62,  total step 2175/6200, training_loss = 0.1535\n",
      "epoch 36/100, step 6/62,  total step 2176/6200, training_loss = 0.2240\n",
      "epoch 36/100, step 7/62,  total step 2177/6200, training_loss = 0.2288\n",
      "epoch 36/100, step 8/62,  total step 2178/6200, training_loss = 0.2700\n",
      "epoch 36/100, step 9/62,  total step 2179/6200, training_loss = 0.2123\n",
      "epoch 36/100, step 10/62,  total step 2180/6200, training_loss = 0.2486\n",
      "epoch 36/100, step 11/62,  total step 2181/6200, training_loss = 0.1977\n",
      "epoch 36/100, step 12/62,  total step 2182/6200, training_loss = 0.1846\n",
      "epoch 36/100, step 13/62,  total step 2183/6200, training_loss = 0.2704\n",
      "epoch 36/100, step 14/62,  total step 2184/6200, training_loss = 0.2633\n",
      "epoch 36/100, step 15/62,  total step 2185/6200, training_loss = 0.3915\n",
      "epoch 36/100, step 16/62,  total step 2186/6200, training_loss = 0.3545\n",
      "epoch 36/100, step 17/62,  total step 2187/6200, training_loss = 0.2461\n",
      "epoch 36/100, step 18/62,  total step 2188/6200, training_loss = 0.2260\n",
      "epoch 36/100, step 19/62,  total step 2189/6200, training_loss = 0.2913\n",
      "epoch 36/100, step 20/62,  total step 2190/6200, training_loss = 0.2485\n",
      "epoch 36/100, step 21/62,  total step 2191/6200, training_loss = 0.2659\n",
      "epoch 36/100, step 22/62,  total step 2192/6200, training_loss = 0.3598\n",
      "epoch 36/100, step 23/62,  total step 2193/6200, training_loss = 0.2565\n",
      "epoch 36/100, step 24/62,  total step 2194/6200, training_loss = 0.2084\n",
      "epoch 36/100, step 25/62,  total step 2195/6200, training_loss = 0.2220\n",
      "epoch 36/100, step 26/62,  total step 2196/6200, training_loss = 0.2035\n",
      "epoch 36/100, step 27/62,  total step 2197/6200, training_loss = 0.2353\n",
      "epoch 36/100, step 28/62,  total step 2198/6200, training_loss = 0.3872\n",
      "epoch 36/100, step 29/62,  total step 2199/6200, training_loss = 0.2907\n",
      "epoch 36/100, step 30/62,  total step 2200/6200, training_loss = 0.2754\n",
      "epoch 36/100, step 31/62,  total step 2201/6200, training_loss = 0.1748\n",
      "epoch 36/100, step 32/62,  total step 2202/6200, training_loss = 0.2167\n",
      "epoch 36/100, step 33/62,  total step 2203/6200, training_loss = 0.3374\n",
      "epoch 36/100, step 34/62,  total step 2204/6200, training_loss = 0.2126\n",
      "epoch 36/100, step 35/62,  total step 2205/6200, training_loss = 0.2670\n",
      "epoch 36/100, step 36/62,  total step 2206/6200, training_loss = 0.2951\n",
      "epoch 36/100, step 37/62,  total step 2207/6200, training_loss = 0.2800\n",
      "epoch 36/100, step 38/62,  total step 2208/6200, training_loss = 0.2765\n",
      "epoch 36/100, step 39/62,  total step 2209/6200, training_loss = 0.2481\n",
      "epoch 36/100, step 40/62,  total step 2210/6200, training_loss = 0.2692\n",
      "epoch 36/100, step 41/62,  total step 2211/6200, training_loss = 0.2098\n",
      "epoch 36/100, step 42/62,  total step 2212/6200, training_loss = 0.2403\n",
      "epoch 36/100, step 43/62,  total step 2213/6200, training_loss = 0.2396\n",
      "epoch 36/100, step 44/62,  total step 2214/6200, training_loss = 0.2948\n",
      "epoch 36/100, step 45/62,  total step 2215/6200, training_loss = 0.1542\n",
      "epoch 36/100, step 46/62,  total step 2216/6200, training_loss = 0.1792\n",
      "epoch 36/100, step 47/62,  total step 2217/6200, training_loss = 0.2816\n",
      "epoch 36/100, step 48/62,  total step 2218/6200, training_loss = 0.2096\n",
      "epoch 36/100, step 49/62,  total step 2219/6200, training_loss = 0.1694\n",
      "epoch 36/100, step 50/62,  total step 2220/6200, training_loss = 0.2423\n",
      "epoch 36/100, step 51/62,  total step 2221/6200, training_loss = 0.2297\n",
      "epoch 36/100, step 52/62,  total step 2222/6200, training_loss = 0.1897\n",
      "epoch 36/100, step 53/62,  total step 2223/6200, training_loss = 0.2193\n",
      "epoch 36/100, step 54/62,  total step 2224/6200, training_loss = 0.2147\n",
      "epoch 36/100, step 55/62,  total step 2225/6200, training_loss = 0.2451\n",
      "epoch 36/100, step 56/62,  total step 2226/6200, training_loss = 0.2826\n",
      "epoch 36/100, step 57/62,  total step 2227/6200, training_loss = 0.2607\n",
      "epoch 36/100, step 58/62,  total step 2228/6200, training_loss = 0.3516\n",
      "epoch 36/100, step 59/62,  total step 2229/6200, training_loss = 0.2041\n",
      "epoch 36/100, step 60/62,  total step 2230/6200, training_loss = 0.2978\n",
      "epoch 36/100, step 61/62,  total step 2231/6200, training_loss = 0.1570\n",
      "epoch 36/100, step 62/62,  total step 2232/6200, training_loss = 0.2361 | avg loss: 0.2508 Dice Metric:    0.67\n",
      "epoch 37/100, step 1/62,  total step 2233/6200, training_loss = 0.2324\n",
      "epoch 37/100, step 2/62,  total step 2234/6200, training_loss = 0.2393\n",
      "epoch 37/100, step 3/62,  total step 2235/6200, training_loss = 0.2451\n",
      "epoch 37/100, step 4/62,  total step 2236/6200, training_loss = 0.3963\n",
      "epoch 37/100, step 5/62,  total step 2237/6200, training_loss = 0.1478\n",
      "epoch 37/100, step 6/62,  total step 2238/6200, training_loss = 0.2176\n",
      "epoch 37/100, step 7/62,  total step 2239/6200, training_loss = 0.2064\n",
      "epoch 37/100, step 8/62,  total step 2240/6200, training_loss = 0.2520\n",
      "epoch 37/100, step 9/62,  total step 2241/6200, training_loss = 0.2056\n",
      "epoch 37/100, step 10/62,  total step 2242/6200, training_loss = 0.2167\n",
      "epoch 37/100, step 11/62,  total step 2243/6200, training_loss = 0.1881\n",
      "epoch 37/100, step 12/62,  total step 2244/6200, training_loss = 0.1895\n",
      "epoch 37/100, step 13/62,  total step 2245/6200, training_loss = 0.2473\n",
      "epoch 37/100, step 14/62,  total step 2246/6200, training_loss = 0.2718\n",
      "epoch 37/100, step 15/62,  total step 2247/6200, training_loss = 0.3591\n",
      "epoch 37/100, step 16/62,  total step 2248/6200, training_loss = 0.3664\n",
      "epoch 37/100, step 17/62,  total step 2249/6200, training_loss = 0.2315\n",
      "epoch 37/100, step 18/62,  total step 2250/6200, training_loss = 0.2027\n",
      "epoch 37/100, step 19/62,  total step 2251/6200, training_loss = 0.3133\n",
      "epoch 37/100, step 20/62,  total step 2252/6200, training_loss = 0.2670\n",
      "epoch 37/100, step 21/62,  total step 2253/6200, training_loss = 0.2611\n",
      "epoch 37/100, step 22/62,  total step 2254/6200, training_loss = 0.4347\n",
      "epoch 37/100, step 23/62,  total step 2255/6200, training_loss = 0.2467\n",
      "epoch 37/100, step 24/62,  total step 2256/6200, training_loss = 0.1851\n",
      "epoch 37/100, step 25/62,  total step 2257/6200, training_loss = 0.2492\n",
      "epoch 37/100, step 26/62,  total step 2258/6200, training_loss = 0.1727\n",
      "epoch 37/100, step 27/62,  total step 2259/6200, training_loss = 0.2452\n",
      "epoch 37/100, step 28/62,  total step 2260/6200, training_loss = 0.3251\n",
      "epoch 37/100, step 29/62,  total step 2261/6200, training_loss = 0.2882\n",
      "epoch 37/100, step 30/62,  total step 2262/6200, training_loss = 0.2730\n",
      "epoch 37/100, step 31/62,  total step 2263/6200, training_loss = 0.1886\n",
      "epoch 37/100, step 32/62,  total step 2264/6200, training_loss = 0.1993\n",
      "epoch 37/100, step 33/62,  total step 2265/6200, training_loss = 0.3762\n",
      "epoch 37/100, step 34/62,  total step 2266/6200, training_loss = 0.2193\n",
      "epoch 37/100, step 35/62,  total step 2267/6200, training_loss = 0.2523\n",
      "epoch 37/100, step 36/62,  total step 2268/6200, training_loss = 0.3077\n",
      "epoch 37/100, step 37/62,  total step 2269/6200, training_loss = 0.2854\n",
      "epoch 37/100, step 38/62,  total step 2270/6200, training_loss = 0.2919\n",
      "epoch 37/100, step 39/62,  total step 2271/6200, training_loss = 0.2885\n",
      "epoch 37/100, step 40/62,  total step 2272/6200, training_loss = 0.2929\n",
      "epoch 37/100, step 41/62,  total step 2273/6200, training_loss = 0.1988\n",
      "epoch 37/100, step 42/62,  total step 2274/6200, training_loss = 0.2452\n",
      "epoch 37/100, step 43/62,  total step 2275/6200, training_loss = 0.2264\n",
      "epoch 37/100, step 44/62,  total step 2276/6200, training_loss = 0.2454\n",
      "epoch 37/100, step 45/62,  total step 2277/6200, training_loss = 0.1576\n",
      "epoch 37/100, step 46/62,  total step 2278/6200, training_loss = 0.1760\n",
      "epoch 37/100, step 47/62,  total step 2279/6200, training_loss = 0.2808\n",
      "epoch 37/100, step 48/62,  total step 2280/6200, training_loss = 0.1661\n",
      "epoch 37/100, step 49/62,  total step 2281/6200, training_loss = 0.1901\n",
      "epoch 37/100, step 50/62,  total step 2282/6200, training_loss = 0.2436\n",
      "epoch 37/100, step 51/62,  total step 2283/6200, training_loss = 0.2339\n",
      "epoch 37/100, step 52/62,  total step 2284/6200, training_loss = 0.1888\n",
      "epoch 37/100, step 53/62,  total step 2285/6200, training_loss = 0.2560\n",
      "epoch 37/100, step 54/62,  total step 2286/6200, training_loss = 0.2285\n",
      "epoch 37/100, step 55/62,  total step 2287/6200, training_loss = 0.1927\n",
      "epoch 37/100, step 56/62,  total step 2288/6200, training_loss = 0.2369\n",
      "epoch 37/100, step 57/62,  total step 2289/6200, training_loss = 0.2533\n",
      "epoch 37/100, step 58/62,  total step 2290/6200, training_loss = 0.3315\n",
      "epoch 37/100, step 59/62,  total step 2291/6200, training_loss = 0.1722\n",
      "epoch 37/100, step 60/62,  total step 2292/6200, training_loss = 0.2976\n",
      "epoch 37/100, step 61/62,  total step 2293/6200, training_loss = 0.1559\n",
      "epoch 37/100, step 62/62,  total step 2294/6200, training_loss = 0.4265 | avg loss: 0.2497 Dice Metric:   0.638\n",
      "epoch 38/100, step 1/62,  total step 2295/6200, training_loss = 0.2491\n",
      "epoch 38/100, step 2/62,  total step 2296/6200, training_loss = 0.2231\n",
      "epoch 38/100, step 3/62,  total step 2297/6200, training_loss = 0.2731\n",
      "epoch 38/100, step 4/62,  total step 2298/6200, training_loss = 0.3900\n",
      "epoch 38/100, step 5/62,  total step 2299/6200, training_loss = 0.1434\n",
      "epoch 38/100, step 6/62,  total step 2300/6200, training_loss = 0.2545\n",
      "epoch 38/100, step 7/62,  total step 2301/6200, training_loss = 0.2406\n",
      "epoch 38/100, step 8/62,  total step 2302/6200, training_loss = 0.3061\n",
      "epoch 38/100, step 9/62,  total step 2303/6200, training_loss = 0.2968\n",
      "epoch 38/100, step 10/62,  total step 2304/6200, training_loss = 0.3038\n",
      "epoch 38/100, step 11/62,  total step 2305/6200, training_loss = 0.1901\n",
      "epoch 38/100, step 12/62,  total step 2306/6200, training_loss = 0.2166\n",
      "epoch 38/100, step 13/62,  total step 2307/6200, training_loss = 0.2633\n",
      "epoch 38/100, step 14/62,  total step 2308/6200, training_loss = 0.2792\n",
      "epoch 38/100, step 15/62,  total step 2309/6200, training_loss = 0.4273\n",
      "epoch 38/100, step 16/62,  total step 2310/6200, training_loss = 0.3963\n",
      "epoch 38/100, step 17/62,  total step 2311/6200, training_loss = 0.2310\n",
      "epoch 38/100, step 18/62,  total step 2312/6200, training_loss = 0.2332\n",
      "epoch 38/100, step 19/62,  total step 2313/6200, training_loss = 0.3352\n",
      "epoch 38/100, step 20/62,  total step 2314/6200, training_loss = 0.2937\n",
      "epoch 38/100, step 21/62,  total step 2315/6200, training_loss = 0.2962\n",
      "epoch 38/100, step 22/62,  total step 2316/6200, training_loss = 0.3800\n",
      "epoch 38/100, step 23/62,  total step 2317/6200, training_loss = 0.2694\n",
      "epoch 38/100, step 24/62,  total step 2318/6200, training_loss = 0.2066\n",
      "epoch 38/100, step 25/62,  total step 2319/6200, training_loss = 0.2354\n",
      "epoch 38/100, step 26/62,  total step 2320/6200, training_loss = 0.1941\n",
      "epoch 38/100, step 27/62,  total step 2321/6200, training_loss = 0.2474\n",
      "epoch 38/100, step 28/62,  total step 2322/6200, training_loss = 0.3525\n",
      "epoch 38/100, step 29/62,  total step 2323/6200, training_loss = 0.3077\n",
      "epoch 38/100, step 30/62,  total step 2324/6200, training_loss = 0.2907\n",
      "epoch 38/100, step 31/62,  total step 2325/6200, training_loss = 0.2150\n",
      "epoch 38/100, step 32/62,  total step 2326/6200, training_loss = 0.2055\n",
      "epoch 38/100, step 33/62,  total step 2327/6200, training_loss = 0.3394\n",
      "epoch 38/100, step 34/62,  total step 2328/6200, training_loss = 0.2432\n",
      "epoch 38/100, step 35/62,  total step 2329/6200, training_loss = 0.2551\n",
      "epoch 38/100, step 36/62,  total step 2330/6200, training_loss = 0.3167\n",
      "epoch 38/100, step 37/62,  total step 2331/6200, training_loss = 0.3148\n",
      "epoch 38/100, step 38/62,  total step 2332/6200, training_loss = 0.2455\n",
      "epoch 38/100, step 39/62,  total step 2333/6200, training_loss = 0.2881\n",
      "epoch 38/100, step 40/62,  total step 2334/6200, training_loss = 0.2857\n",
      "epoch 38/100, step 41/62,  total step 2335/6200, training_loss = 0.2245\n",
      "epoch 38/100, step 42/62,  total step 2336/6200, training_loss = 0.2737\n",
      "epoch 38/100, step 43/62,  total step 2337/6200, training_loss = 0.2364\n",
      "epoch 38/100, step 44/62,  total step 2338/6200, training_loss = 0.2402\n",
      "epoch 38/100, step 45/62,  total step 2339/6200, training_loss = 0.1750\n",
      "epoch 38/100, step 46/62,  total step 2340/6200, training_loss = 0.2090\n",
      "epoch 38/100, step 47/62,  total step 2341/6200, training_loss = 0.2997\n",
      "epoch 38/100, step 48/62,  total step 2342/6200, training_loss = 0.1879\n",
      "epoch 38/100, step 49/62,  total step 2343/6200, training_loss = 0.1917\n",
      "epoch 38/100, step 50/62,  total step 2344/6200, training_loss = 0.2642\n",
      "epoch 38/100, step 51/62,  total step 2345/6200, training_loss = 0.2180\n",
      "epoch 38/100, step 52/62,  total step 2346/6200, training_loss = 0.1836\n",
      "epoch 38/100, step 53/62,  total step 2347/6200, training_loss = 0.2241\n",
      "epoch 38/100, step 54/62,  total step 2348/6200, training_loss = 0.2426\n",
      "epoch 38/100, step 55/62,  total step 2349/6200, training_loss = 0.2294\n",
      "epoch 38/100, step 56/62,  total step 2350/6200, training_loss = 0.2323\n",
      "epoch 38/100, step 57/62,  total step 2351/6200, training_loss = 0.2812\n",
      "epoch 38/100, step 58/62,  total step 2352/6200, training_loss = 0.3698\n",
      "epoch 38/100, step 59/62,  total step 2353/6200, training_loss = 0.2389\n",
      "epoch 38/100, step 60/62,  total step 2354/6200, training_loss = 0.3269\n",
      "epoch 38/100, step 61/62,  total step 2355/6200, training_loss = 0.1641\n",
      "epoch 38/100, step 62/62,  total step 2356/6200, training_loss = 0.2320 | avg loss: 0.2626 Dice Metric:   0.665\n",
      "epoch 39/100, step 1/62,  total step 2357/6200, training_loss = 0.2670\n",
      "epoch 39/100, step 2/62,  total step 2358/6200, training_loss = 0.2515\n",
      "epoch 39/100, step 3/62,  total step 2359/6200, training_loss = 0.2579\n",
      "epoch 39/100, step 4/62,  total step 2360/6200, training_loss = 0.3968\n",
      "epoch 39/100, step 5/62,  total step 2361/6200, training_loss = 0.1546\n",
      "epoch 39/100, step 6/62,  total step 2362/6200, training_loss = 0.2424\n",
      "epoch 39/100, step 7/62,  total step 2363/6200, training_loss = 0.2180\n",
      "epoch 39/100, step 8/62,  total step 2364/6200, training_loss = 0.2593\n",
      "epoch 39/100, step 9/62,  total step 2365/6200, training_loss = 0.2064\n",
      "epoch 39/100, step 10/62,  total step 2366/6200, training_loss = 0.2293\n",
      "epoch 39/100, step 11/62,  total step 2367/6200, training_loss = 0.1781\n",
      "epoch 39/100, step 12/62,  total step 2368/6200, training_loss = 0.2070\n",
      "epoch 39/100, step 13/62,  total step 2369/6200, training_loss = 0.2662\n",
      "epoch 39/100, step 14/62,  total step 2370/6200, training_loss = 0.2814\n",
      "epoch 39/100, step 15/62,  total step 2371/6200, training_loss = 0.4307\n",
      "epoch 39/100, step 16/62,  total step 2372/6200, training_loss = 0.3944\n",
      "epoch 39/100, step 17/62,  total step 2373/6200, training_loss = 0.2244\n",
      "epoch 39/100, step 18/62,  total step 2374/6200, training_loss = 0.2096\n",
      "epoch 39/100, step 19/62,  total step 2375/6200, training_loss = 0.3017\n",
      "epoch 39/100, step 20/62,  total step 2376/6200, training_loss = 0.2902\n",
      "epoch 39/100, step 21/62,  total step 2377/6200, training_loss = 0.2755\n",
      "epoch 39/100, step 22/62,  total step 2378/6200, training_loss = 0.3626\n",
      "epoch 39/100, step 23/62,  total step 2379/6200, training_loss = 0.2742\n",
      "epoch 39/100, step 24/62,  total step 2380/6200, training_loss = 0.1900\n",
      "epoch 39/100, step 25/62,  total step 2381/6200, training_loss = 0.2219\n",
      "epoch 39/100, step 26/62,  total step 2382/6200, training_loss = 0.1624\n",
      "epoch 39/100, step 27/62,  total step 2383/6200, training_loss = 0.2671\n",
      "epoch 39/100, step 28/62,  total step 2384/6200, training_loss = 0.4011\n",
      "epoch 39/100, step 29/62,  total step 2385/6200, training_loss = 0.2936\n",
      "epoch 39/100, step 30/62,  total step 2386/6200, training_loss = 0.2943\n",
      "epoch 39/100, step 31/62,  total step 2387/6200, training_loss = 0.1768\n",
      "epoch 39/100, step 32/62,  total step 2388/6200, training_loss = 0.1954\n",
      "epoch 39/100, step 33/62,  total step 2389/6200, training_loss = 0.3082\n",
      "epoch 39/100, step 34/62,  total step 2390/6200, training_loss = 0.2089\n",
      "epoch 39/100, step 35/62,  total step 2391/6200, training_loss = 0.2500\n",
      "epoch 39/100, step 36/62,  total step 2392/6200, training_loss = 0.3063\n",
      "epoch 39/100, step 37/62,  total step 2393/6200, training_loss = 0.2923\n",
      "epoch 39/100, step 38/62,  total step 2394/6200, training_loss = 0.2750\n",
      "epoch 39/100, step 39/62,  total step 2395/6200, training_loss = 0.2782\n",
      "epoch 39/100, step 40/62,  total step 2396/6200, training_loss = 0.2936\n",
      "epoch 39/100, step 41/62,  total step 2397/6200, training_loss = 0.1979\n",
      "epoch 39/100, step 42/62,  total step 2398/6200, training_loss = 0.2310\n",
      "epoch 39/100, step 43/62,  total step 2399/6200, training_loss = 0.2230\n",
      "epoch 39/100, step 44/62,  total step 2400/6200, training_loss = 0.2592\n",
      "epoch 39/100, step 45/62,  total step 2401/6200, training_loss = 0.1787\n",
      "epoch 39/100, step 46/62,  total step 2402/6200, training_loss = 0.1887\n",
      "epoch 39/100, step 47/62,  total step 2403/6200, training_loss = 0.2487\n",
      "epoch 39/100, step 48/62,  total step 2404/6200, training_loss = 0.2004\n",
      "epoch 39/100, step 49/62,  total step 2405/6200, training_loss = 0.1801\n",
      "epoch 39/100, step 50/62,  total step 2406/6200, training_loss = 0.2459\n",
      "epoch 39/100, step 51/62,  total step 2407/6200, training_loss = 0.2476\n",
      "epoch 39/100, step 52/62,  total step 2408/6200, training_loss = 0.1825\n",
      "epoch 39/100, step 53/62,  total step 2409/6200, training_loss = 0.2098\n",
      "epoch 39/100, step 54/62,  total step 2410/6200, training_loss = 0.1586\n",
      "epoch 39/100, step 55/62,  total step 2411/6200, training_loss = 0.1970\n",
      "epoch 39/100, step 56/62,  total step 2412/6200, training_loss = 0.2805\n",
      "epoch 39/100, step 57/62,  total step 2413/6200, training_loss = 0.2905\n",
      "epoch 39/100, step 58/62,  total step 2414/6200, training_loss = 0.3727\n",
      "epoch 39/100, step 59/62,  total step 2415/6200, training_loss = 0.1776\n",
      "epoch 39/100, step 60/62,  total step 2416/6200, training_loss = 0.3197\n",
      "epoch 39/100, step 61/62,  total step 2417/6200, training_loss = 0.1800\n",
      "epoch 39/100, step 62/62,  total step 2418/6200, training_loss = 0.2549 | avg loss: 0.2519 Dice Metric:   0.657\n",
      "epoch 40/100, step 1/62,  total step 2419/6200, training_loss = 0.2334\n",
      "epoch 40/100, step 2/62,  total step 2420/6200, training_loss = 0.2438\n",
      "epoch 40/100, step 3/62,  total step 2421/6200, training_loss = 0.2858\n",
      "epoch 40/100, step 4/62,  total step 2422/6200, training_loss = 0.3895\n",
      "epoch 40/100, step 5/62,  total step 2423/6200, training_loss = 0.1581\n",
      "epoch 40/100, step 6/62,  total step 2424/6200, training_loss = 0.2177\n",
      "epoch 40/100, step 7/62,  total step 2425/6200, training_loss = 0.2235\n",
      "epoch 40/100, step 8/62,  total step 2426/6200, training_loss = 0.2746\n",
      "epoch 40/100, step 9/62,  total step 2427/6200, training_loss = 0.2111\n",
      "epoch 40/100, step 10/62,  total step 2428/6200, training_loss = 0.2225\n",
      "epoch 40/100, step 11/62,  total step 2429/6200, training_loss = 0.1805\n",
      "epoch 40/100, step 12/62,  total step 2430/6200, training_loss = 0.1863\n",
      "epoch 40/100, step 13/62,  total step 2431/6200, training_loss = 0.2398\n",
      "epoch 40/100, step 14/62,  total step 2432/6200, training_loss = 0.2692\n",
      "epoch 40/100, step 15/62,  total step 2433/6200, training_loss = 0.4112\n",
      "epoch 40/100, step 16/62,  total step 2434/6200, training_loss = 0.3789\n",
      "epoch 40/100, step 17/62,  total step 2435/6200, training_loss = 0.2417\n",
      "epoch 40/100, step 18/62,  total step 2436/6200, training_loss = 0.2353\n",
      "epoch 40/100, step 19/62,  total step 2437/6200, training_loss = 0.3205\n",
      "epoch 40/100, step 20/62,  total step 2438/6200, training_loss = 0.2501\n",
      "epoch 40/100, step 21/62,  total step 2439/6200, training_loss = 0.2496\n",
      "epoch 40/100, step 22/62,  total step 2440/6200, training_loss = 0.4081\n",
      "epoch 40/100, step 23/62,  total step 2441/6200, training_loss = 0.2800\n",
      "epoch 40/100, step 24/62,  total step 2442/6200, training_loss = 0.1883\n",
      "epoch 40/100, step 25/62,  total step 2443/6200, training_loss = 0.2454\n",
      "epoch 40/100, step 26/62,  total step 2444/6200, training_loss = 0.1648\n",
      "epoch 40/100, step 27/62,  total step 2445/6200, training_loss = 0.2605\n",
      "epoch 40/100, step 28/62,  total step 2446/6200, training_loss = 0.4290\n",
      "epoch 40/100, step 29/62,  total step 2447/6200, training_loss = 0.2967\n",
      "epoch 40/100, step 30/62,  total step 2448/6200, training_loss = 0.2823\n",
      "epoch 40/100, step 31/62,  total step 2449/6200, training_loss = 0.1781\n",
      "epoch 40/100, step 32/62,  total step 2450/6200, training_loss = 0.2125\n",
      "epoch 40/100, step 33/62,  total step 2451/6200, training_loss = 0.3285\n",
      "epoch 40/100, step 34/62,  total step 2452/6200, training_loss = 0.2174\n",
      "epoch 40/100, step 35/62,  total step 2453/6200, training_loss = 0.2355\n",
      "epoch 40/100, step 36/62,  total step 2454/6200, training_loss = 0.3067\n",
      "epoch 40/100, step 37/62,  total step 2455/6200, training_loss = 0.2559\n",
      "epoch 40/100, step 38/62,  total step 2456/6200, training_loss = 0.2632\n",
      "epoch 40/100, step 39/62,  total step 2457/6200, training_loss = 0.3149\n",
      "epoch 40/100, step 40/62,  total step 2458/6200, training_loss = 0.2781\n",
      "epoch 40/100, step 41/62,  total step 2459/6200, training_loss = 0.1988\n",
      "epoch 40/100, step 42/62,  total step 2460/6200, training_loss = 0.2186\n",
      "epoch 40/100, step 43/62,  total step 2461/6200, training_loss = 0.2339\n",
      "epoch 40/100, step 44/62,  total step 2462/6200, training_loss = 0.2438\n",
      "epoch 40/100, step 45/62,  total step 2463/6200, training_loss = 0.1636\n",
      "epoch 40/100, step 46/62,  total step 2464/6200, training_loss = 0.1521\n",
      "epoch 40/100, step 47/62,  total step 2465/6200, training_loss = 0.2521\n",
      "epoch 40/100, step 48/62,  total step 2466/6200, training_loss = 0.1614\n",
      "epoch 40/100, step 49/62,  total step 2467/6200, training_loss = 0.1824\n",
      "epoch 40/100, step 50/62,  total step 2468/6200, training_loss = 0.2257\n",
      "epoch 40/100, step 51/62,  total step 2469/6200, training_loss = 0.2206\n",
      "epoch 40/100, step 52/62,  total step 2470/6200, training_loss = 0.1946\n",
      "epoch 40/100, step 53/62,  total step 2471/6200, training_loss = 0.2310\n",
      "epoch 40/100, step 54/62,  total step 2472/6200, training_loss = 0.1878\n",
      "epoch 40/100, step 55/62,  total step 2473/6200, training_loss = 0.1884\n",
      "epoch 40/100, step 56/62,  total step 2474/6200, training_loss = 0.2732\n",
      "epoch 40/100, step 57/62,  total step 2475/6200, training_loss = 0.2420\n",
      "epoch 40/100, step 58/62,  total step 2476/6200, training_loss = 0.3431\n",
      "epoch 40/100, step 59/62,  total step 2477/6200, training_loss = 0.1915\n",
      "epoch 40/100, step 60/62,  total step 2478/6200, training_loss = 0.3076\n",
      "epoch 40/100, step 61/62,  total step 2479/6200, training_loss = 0.1499\n",
      "epoch 40/100, step 62/62,  total step 2480/6200, training_loss = 0.2185 | avg loss: 0.2476 Dice Metric:   0.653\n",
      "epoch 41/100, step 1/62,  total step 2481/6200, training_loss = 0.2346\n",
      "epoch 41/100, step 2/62,  total step 2482/6200, training_loss = 0.2010\n",
      "epoch 41/100, step 3/62,  total step 2483/6200, training_loss = 0.2649\n",
      "epoch 41/100, step 4/62,  total step 2484/6200, training_loss = 0.3845\n",
      "epoch 41/100, step 5/62,  total step 2485/6200, training_loss = 0.1578\n",
      "epoch 41/100, step 6/62,  total step 2486/6200, training_loss = 0.2194\n",
      "epoch 41/100, step 7/62,  total step 2487/6200, training_loss = 0.2059\n",
      "epoch 41/100, step 8/62,  total step 2488/6200, training_loss = 0.2483\n",
      "epoch 41/100, step 9/62,  total step 2489/6200, training_loss = 0.1787\n",
      "epoch 41/100, step 10/62,  total step 2490/6200, training_loss = 0.2714\n",
      "epoch 41/100, step 11/62,  total step 2491/6200, training_loss = 0.1918\n",
      "epoch 41/100, step 12/62,  total step 2492/6200, training_loss = 0.1911\n",
      "epoch 41/100, step 13/62,  total step 2493/6200, training_loss = 0.2337\n",
      "epoch 41/100, step 14/62,  total step 2494/6200, training_loss = 0.2524\n",
      "epoch 41/100, step 15/62,  total step 2495/6200, training_loss = 0.4295\n",
      "epoch 41/100, step 16/62,  total step 2496/6200, training_loss = 0.3833\n",
      "epoch 41/100, step 17/62,  total step 2497/6200, training_loss = 0.2007\n",
      "epoch 41/100, step 18/62,  total step 2498/6200, training_loss = 0.2079\n",
      "epoch 41/100, step 19/62,  total step 2499/6200, training_loss = 0.2835\n",
      "epoch 41/100, step 20/62,  total step 2500/6200, training_loss = 0.2514\n",
      "epoch 41/100, step 21/62,  total step 2501/6200, training_loss = 0.2741\n",
      "epoch 41/100, step 22/62,  total step 2502/6200, training_loss = 0.3941\n",
      "epoch 41/100, step 23/62,  total step 2503/6200, training_loss = 0.2746\n",
      "epoch 41/100, step 24/62,  total step 2504/6200, training_loss = 0.2167\n",
      "epoch 41/100, step 25/62,  total step 2505/6200, training_loss = 0.2267\n",
      "epoch 41/100, step 26/62,  total step 2506/6200, training_loss = 0.1730\n",
      "epoch 41/100, step 27/62,  total step 2507/6200, training_loss = 0.2510\n",
      "epoch 41/100, step 28/62,  total step 2508/6200, training_loss = 0.3927\n",
      "epoch 41/100, step 29/62,  total step 2509/6200, training_loss = 0.2758\n",
      "epoch 41/100, step 30/62,  total step 2510/6200, training_loss = 0.2747\n",
      "epoch 41/100, step 31/62,  total step 2511/6200, training_loss = 0.1739\n",
      "epoch 41/100, step 32/62,  total step 2512/6200, training_loss = 0.2371\n",
      "epoch 41/100, step 33/62,  total step 2513/6200, training_loss = 0.3254\n",
      "epoch 41/100, step 34/62,  total step 2514/6200, training_loss = 0.2465\n",
      "epoch 41/100, step 35/62,  total step 2515/6200, training_loss = 0.2534\n",
      "epoch 41/100, step 36/62,  total step 2516/6200, training_loss = 0.2954\n",
      "epoch 41/100, step 37/62,  total step 2517/6200, training_loss = 0.2683\n",
      "epoch 41/100, step 38/62,  total step 2518/6200, training_loss = 0.2530\n",
      "epoch 41/100, step 39/62,  total step 2519/6200, training_loss = 0.2481\n",
      "epoch 41/100, step 40/62,  total step 2520/6200, training_loss = 0.2820\n",
      "epoch 41/100, step 41/62,  total step 2521/6200, training_loss = 0.1959\n",
      "epoch 41/100, step 42/62,  total step 2522/6200, training_loss = 0.2683\n",
      "epoch 41/100, step 43/62,  total step 2523/6200, training_loss = 0.2260\n",
      "epoch 41/100, step 44/62,  total step 2524/6200, training_loss = 0.2754\n",
      "epoch 41/100, step 45/62,  total step 2525/6200, training_loss = 0.1518\n",
      "epoch 41/100, step 46/62,  total step 2526/6200, training_loss = 0.1721\n",
      "epoch 41/100, step 47/62,  total step 2527/6200, training_loss = 0.2640\n",
      "epoch 41/100, step 48/62,  total step 2528/6200, training_loss = 0.1760\n",
      "epoch 41/100, step 49/62,  total step 2529/6200, training_loss = 0.1584\n",
      "epoch 41/100, step 50/62,  total step 2530/6200, training_loss = 0.2363\n",
      "epoch 41/100, step 51/62,  total step 2531/6200, training_loss = 0.2456\n",
      "epoch 41/100, step 52/62,  total step 2532/6200, training_loss = 0.1921\n",
      "epoch 41/100, step 53/62,  total step 2533/6200, training_loss = 0.2322\n",
      "epoch 41/100, step 54/62,  total step 2534/6200, training_loss = 0.1987\n",
      "epoch 41/100, step 55/62,  total step 2535/6200, training_loss = 0.2190\n",
      "epoch 41/100, step 56/62,  total step 2536/6200, training_loss = 0.2601\n",
      "epoch 41/100, step 57/62,  total step 2537/6200, training_loss = 0.2422\n",
      "epoch 41/100, step 58/62,  total step 2538/6200, training_loss = 0.3191\n",
      "epoch 41/100, step 59/62,  total step 2539/6200, training_loss = 0.1954\n",
      "epoch 41/100, step 60/62,  total step 2540/6200, training_loss = 0.2951\n",
      "epoch 41/100, step 61/62,  total step 2541/6200, training_loss = 0.1478\n",
      "epoch 41/100, step 62/62,  total step 2542/6200, training_loss = 0.2016 | avg loss: 0.2452 Dice Metric:   0.665\n",
      "epoch 42/100, step 1/62,  total step 2543/6200, training_loss = 0.2418\n",
      "epoch 42/100, step 2/62,  total step 2544/6200, training_loss = 0.2015\n",
      "epoch 42/100, step 3/62,  total step 2545/6200, training_loss = 0.2438\n",
      "epoch 42/100, step 4/62,  total step 2546/6200, training_loss = 0.3854\n",
      "epoch 42/100, step 5/62,  total step 2547/6200, training_loss = 0.1526\n",
      "epoch 42/100, step 6/62,  total step 2548/6200, training_loss = 0.2294\n",
      "epoch 42/100, step 7/62,  total step 2549/6200, training_loss = 0.2231\n",
      "epoch 42/100, step 8/62,  total step 2550/6200, training_loss = 0.3019\n",
      "epoch 42/100, step 9/62,  total step 2551/6200, training_loss = 0.2190\n",
      "epoch 42/100, step 10/62,  total step 2552/6200, training_loss = 0.2385\n",
      "epoch 42/100, step 11/62,  total step 2553/6200, training_loss = 0.1584\n",
      "epoch 42/100, step 12/62,  total step 2554/6200, training_loss = 0.1989\n",
      "epoch 42/100, step 13/62,  total step 2555/6200, training_loss = 0.2334\n",
      "epoch 42/100, step 14/62,  total step 2556/6200, training_loss = 0.2704\n",
      "epoch 42/100, step 15/62,  total step 2557/6200, training_loss = 0.4331\n",
      "epoch 42/100, step 16/62,  total step 2558/6200, training_loss = 0.3738\n",
      "epoch 42/100, step 17/62,  total step 2559/6200, training_loss = 0.2249\n",
      "epoch 42/100, step 18/62,  total step 2560/6200, training_loss = 0.2375\n",
      "epoch 42/100, step 19/62,  total step 2561/6200, training_loss = 0.2959\n",
      "epoch 42/100, step 20/62,  total step 2562/6200, training_loss = 0.2482\n",
      "epoch 42/100, step 21/62,  total step 2563/6200, training_loss = 0.2677\n",
      "epoch 42/100, step 22/62,  total step 2564/6200, training_loss = 0.3599\n",
      "epoch 42/100, step 23/62,  total step 2565/6200, training_loss = 0.2606\n",
      "epoch 42/100, step 24/62,  total step 2566/6200, training_loss = 0.2041\n",
      "epoch 42/100, step 25/62,  total step 2567/6200, training_loss = 0.2217\n",
      "epoch 42/100, step 26/62,  total step 2568/6200, training_loss = 0.1723\n",
      "epoch 42/100, step 27/62,  total step 2569/6200, training_loss = 0.2298\n",
      "epoch 42/100, step 28/62,  total step 2570/6200, training_loss = 0.3665\n",
      "epoch 42/100, step 29/62,  total step 2571/6200, training_loss = 0.2910\n",
      "epoch 42/100, step 30/62,  total step 2572/6200, training_loss = 0.2942\n",
      "epoch 42/100, step 31/62,  total step 2573/6200, training_loss = 0.1916\n",
      "epoch 42/100, step 32/62,  total step 2574/6200, training_loss = 0.2518\n",
      "epoch 42/100, step 33/62,  total step 2575/6200, training_loss = 0.3243\n",
      "epoch 42/100, step 34/62,  total step 2576/6200, training_loss = 0.2117\n",
      "epoch 42/100, step 35/62,  total step 2577/6200, training_loss = 0.2370\n",
      "epoch 42/100, step 36/62,  total step 2578/6200, training_loss = 0.3025\n",
      "epoch 42/100, step 37/62,  total step 2579/6200, training_loss = 0.3076\n",
      "epoch 42/100, step 38/62,  total step 2580/6200, training_loss = 0.2987\n",
      "epoch 42/100, step 39/62,  total step 2581/6200, training_loss = 0.2770\n",
      "epoch 42/100, step 40/62,  total step 2582/6200, training_loss = 0.2769\n",
      "epoch 42/100, step 41/62,  total step 2583/6200, training_loss = 0.2086\n",
      "epoch 42/100, step 42/62,  total step 2584/6200, training_loss = 0.2239\n",
      "epoch 42/100, step 43/62,  total step 2585/6200, training_loss = 0.2260\n",
      "epoch 42/100, step 44/62,  total step 2586/6200, training_loss = 0.2655\n",
      "epoch 42/100, step 45/62,  total step 2587/6200, training_loss = 0.1442\n",
      "epoch 42/100, step 46/62,  total step 2588/6200, training_loss = 0.2148\n",
      "epoch 42/100, step 47/62,  total step 2589/6200, training_loss = 0.2536\n",
      "epoch 42/100, step 48/62,  total step 2590/6200, training_loss = 0.1699\n",
      "epoch 42/100, step 49/62,  total step 2591/6200, training_loss = 0.1589\n",
      "epoch 42/100, step 50/62,  total step 2592/6200, training_loss = 0.2512\n",
      "epoch 42/100, step 51/62,  total step 2593/6200, training_loss = 0.2243\n",
      "epoch 42/100, step 52/62,  total step 2594/6200, training_loss = 0.1917\n",
      "epoch 42/100, step 53/62,  total step 2595/6200, training_loss = 0.1808\n",
      "epoch 42/100, step 54/62,  total step 2596/6200, training_loss = 0.1779\n",
      "epoch 42/100, step 55/62,  total step 2597/6200, training_loss = 0.2079\n",
      "epoch 42/100, step 56/62,  total step 2598/6200, training_loss = 0.3034\n",
      "epoch 42/100, step 57/62,  total step 2599/6200, training_loss = 0.2377\n",
      "epoch 42/100, step 58/62,  total step 2600/6200, training_loss = 0.3475\n",
      "epoch 42/100, step 59/62,  total step 2601/6200, training_loss = 0.2034\n",
      "epoch 42/100, step 60/62,  total step 2602/6200, training_loss = 0.3219\n",
      "epoch 42/100, step 61/62,  total step 2603/6200, training_loss = 0.1530\n",
      "epoch 42/100, step 62/62,  total step 2604/6200, training_loss = 0.2037 | avg loss: 0.2472 Dice Metric:   0.644\n",
      "epoch 43/100, step 1/62,  total step 2605/6200, training_loss = 0.2199\n",
      "epoch 43/100, step 2/62,  total step 2606/6200, training_loss = 0.2090\n",
      "epoch 43/100, step 3/62,  total step 2607/6200, training_loss = 0.2562\n",
      "epoch 43/100, step 4/62,  total step 2608/6200, training_loss = 0.3907\n",
      "epoch 43/100, step 5/62,  total step 2609/6200, training_loss = 0.1508\n",
      "epoch 43/100, step 6/62,  total step 2610/6200, training_loss = 0.2125\n",
      "epoch 43/100, step 7/62,  total step 2611/6200, training_loss = 0.1933\n",
      "epoch 43/100, step 8/62,  total step 2612/6200, training_loss = 0.2681\n",
      "epoch 43/100, step 9/62,  total step 2613/6200, training_loss = 0.2061\n",
      "epoch 43/100, step 10/62,  total step 2614/6200, training_loss = 0.2502\n",
      "epoch 43/100, step 11/62,  total step 2615/6200, training_loss = 0.1761\n",
      "epoch 43/100, step 12/62,  total step 2616/6200, training_loss = 0.1936\n",
      "epoch 43/100, step 13/62,  total step 2617/6200, training_loss = 0.2834\n",
      "epoch 43/100, step 14/62,  total step 2618/6200, training_loss = 0.2625\n",
      "epoch 43/100, step 15/62,  total step 2619/6200, training_loss = 0.3981\n",
      "epoch 43/100, step 16/62,  total step 2620/6200, training_loss = 0.3637\n",
      "epoch 43/100, step 17/62,  total step 2621/6200, training_loss = 0.2633\n",
      "epoch 43/100, step 18/62,  total step 2622/6200, training_loss = 0.2165\n",
      "epoch 43/100, step 19/62,  total step 2623/6200, training_loss = 0.2765\n",
      "epoch 43/100, step 20/62,  total step 2624/6200, training_loss = 0.2619\n",
      "epoch 43/100, step 21/62,  total step 2625/6200, training_loss = 0.2965\n",
      "epoch 43/100, step 22/62,  total step 2626/6200, training_loss = 0.3925\n",
      "epoch 43/100, step 23/62,  total step 2627/6200, training_loss = 0.2620\n",
      "epoch 43/100, step 24/62,  total step 2628/6200, training_loss = 0.1887\n",
      "epoch 43/100, step 25/62,  total step 2629/6200, training_loss = 0.2044\n",
      "epoch 43/100, step 26/62,  total step 2630/6200, training_loss = 0.1863\n",
      "epoch 43/100, step 27/62,  total step 2631/6200, training_loss = 0.2632\n",
      "epoch 43/100, step 28/62,  total step 2632/6200, training_loss = 0.3869\n",
      "epoch 43/100, step 29/62,  total step 2633/6200, training_loss = 0.2771\n",
      "epoch 43/100, step 30/62,  total step 2634/6200, training_loss = 0.2873\n",
      "epoch 43/100, step 31/62,  total step 2635/6200, training_loss = 0.1515\n",
      "epoch 43/100, step 32/62,  total step 2636/6200, training_loss = 0.2078\n",
      "epoch 43/100, step 33/62,  total step 2637/6200, training_loss = 0.3365\n",
      "epoch 43/100, step 34/62,  total step 2638/6200, training_loss = 0.2411\n",
      "epoch 43/100, step 35/62,  total step 2639/6200, training_loss = 0.2452\n",
      "epoch 43/100, step 36/62,  total step 2640/6200, training_loss = 0.2930\n",
      "epoch 43/100, step 37/62,  total step 2641/6200, training_loss = 0.2443\n",
      "epoch 43/100, step 38/62,  total step 2642/6200, training_loss = 0.2646\n",
      "epoch 43/100, step 39/62,  total step 2643/6200, training_loss = 0.2602\n",
      "epoch 43/100, step 40/62,  total step 2644/6200, training_loss = 0.2803\n",
      "epoch 43/100, step 41/62,  total step 2645/6200, training_loss = 0.2106\n",
      "epoch 43/100, step 42/62,  total step 2646/6200, training_loss = 0.2217\n",
      "epoch 43/100, step 43/62,  total step 2647/6200, training_loss = 0.2478\n",
      "epoch 43/100, step 44/62,  total step 2648/6200, training_loss = 0.2526\n",
      "epoch 43/100, step 45/62,  total step 2649/6200, training_loss = 0.1825\n",
      "epoch 43/100, step 46/62,  total step 2650/6200, training_loss = 0.1951\n",
      "epoch 43/100, step 47/62,  total step 2651/6200, training_loss = 0.2752\n",
      "epoch 43/100, step 48/62,  total step 2652/6200, training_loss = 0.1754\n",
      "epoch 43/100, step 49/62,  total step 2653/6200, training_loss = 0.1856\n",
      "epoch 43/100, step 50/62,  total step 2654/6200, training_loss = 0.2812\n",
      "epoch 43/100, step 51/62,  total step 2655/6200, training_loss = 0.2136\n",
      "epoch 43/100, step 52/62,  total step 2656/6200, training_loss = 0.1806\n",
      "epoch 43/100, step 53/62,  total step 2657/6200, training_loss = 0.2474\n",
      "epoch 43/100, step 54/62,  total step 2658/6200, training_loss = 0.1791\n",
      "epoch 43/100, step 55/62,  total step 2659/6200, training_loss = 0.1806\n",
      "epoch 43/100, step 56/62,  total step 2660/6200, training_loss = 0.2671\n",
      "epoch 43/100, step 57/62,  total step 2661/6200, training_loss = 0.2726\n",
      "epoch 43/100, step 58/62,  total step 2662/6200, training_loss = 0.3564\n",
      "epoch 43/100, step 59/62,  total step 2663/6200, training_loss = 0.2080\n",
      "epoch 43/100, step 60/62,  total step 2664/6200, training_loss = 0.3168\n",
      "epoch 43/100, step 61/62,  total step 2665/6200, training_loss = 0.1662\n",
      "epoch 43/100, step 62/62,  total step 2666/6200, training_loss = 0.2014 | avg loss: 0.2474 Dice Metric:   0.669\n",
      "epoch 44/100, step 1/62,  total step 2667/6200, training_loss = 0.2336\n",
      "epoch 44/100, step 2/62,  total step 2668/6200, training_loss = 0.2112\n",
      "epoch 44/100, step 3/62,  total step 2669/6200, training_loss = 0.2866\n",
      "epoch 44/100, step 4/62,  total step 2670/6200, training_loss = 0.3833\n",
      "epoch 44/100, step 5/62,  total step 2671/6200, training_loss = 0.1640\n",
      "epoch 44/100, step 6/62,  total step 2672/6200, training_loss = 0.2199\n",
      "epoch 44/100, step 7/62,  total step 2673/6200, training_loss = 0.2070\n",
      "epoch 44/100, step 8/62,  total step 2674/6200, training_loss = 0.2821\n",
      "epoch 44/100, step 9/62,  total step 2675/6200, training_loss = 0.1845\n",
      "epoch 44/100, step 10/62,  total step 2676/6200, training_loss = 0.2255\n",
      "epoch 44/100, step 11/62,  total step 2677/6200, training_loss = 0.1676\n",
      "epoch 44/100, step 12/62,  total step 2678/6200, training_loss = 0.2049\n",
      "epoch 44/100, step 13/62,  total step 2679/6200, training_loss = 0.2331\n",
      "epoch 44/100, step 14/62,  total step 2680/6200, training_loss = 0.2644\n",
      "epoch 44/100, step 15/62,  total step 2681/6200, training_loss = 0.4093\n",
      "epoch 44/100, step 16/62,  total step 2682/6200, training_loss = 0.3473\n",
      "epoch 44/100, step 17/62,  total step 2683/6200, training_loss = 0.2052\n",
      "epoch 44/100, step 18/62,  total step 2684/6200, training_loss = 0.2235\n",
      "epoch 44/100, step 19/62,  total step 2685/6200, training_loss = 0.2840\n",
      "epoch 44/100, step 20/62,  total step 2686/6200, training_loss = 0.2747\n",
      "epoch 44/100, step 21/62,  total step 2687/6200, training_loss = 0.2713\n",
      "epoch 44/100, step 22/62,  total step 2688/6200, training_loss = 0.3806\n",
      "epoch 44/100, step 23/62,  total step 2689/6200, training_loss = 0.2743\n",
      "epoch 44/100, step 24/62,  total step 2690/6200, training_loss = 0.1893\n",
      "epoch 44/100, step 25/62,  total step 2691/6200, training_loss = 0.2135\n",
      "epoch 44/100, step 26/62,  total step 2692/6200, training_loss = 0.1582\n",
      "epoch 44/100, step 27/62,  total step 2693/6200, training_loss = 0.2345\n",
      "epoch 44/100, step 28/62,  total step 2694/6200, training_loss = 0.4063\n",
      "epoch 44/100, step 29/62,  total step 2695/6200, training_loss = 0.2977\n",
      "epoch 44/100, step 30/62,  total step 2696/6200, training_loss = 0.2698\n",
      "epoch 44/100, step 31/62,  total step 2697/6200, training_loss = 0.1638\n",
      "epoch 44/100, step 32/62,  total step 2698/6200, training_loss = 0.2022\n",
      "epoch 44/100, step 33/62,  total step 2699/6200, training_loss = 0.3507\n",
      "epoch 44/100, step 34/62,  total step 2700/6200, training_loss = 0.2300\n",
      "epoch 44/100, step 35/62,  total step 2701/6200, training_loss = 0.2151\n",
      "epoch 44/100, step 36/62,  total step 2702/6200, training_loss = 0.2924\n",
      "epoch 44/100, step 37/62,  total step 2703/6200, training_loss = 0.2446\n",
      "epoch 44/100, step 38/62,  total step 2704/6200, training_loss = 0.2688\n",
      "epoch 44/100, step 39/62,  total step 2705/6200, training_loss = 0.2842\n",
      "epoch 44/100, step 40/62,  total step 2706/6200, training_loss = 0.2816\n",
      "epoch 44/100, step 41/62,  total step 2707/6200, training_loss = 0.1929\n",
      "epoch 44/100, step 42/62,  total step 2708/6200, training_loss = 0.2396\n",
      "epoch 44/100, step 43/62,  total step 2709/6200, training_loss = 0.2325\n",
      "epoch 44/100, step 44/62,  total step 2710/6200, training_loss = 0.2495\n",
      "epoch 44/100, step 45/62,  total step 2711/6200, training_loss = 0.1497\n",
      "epoch 44/100, step 46/62,  total step 2712/6200, training_loss = 0.1843\n",
      "epoch 44/100, step 47/62,  total step 2713/6200, training_loss = 0.2394\n",
      "epoch 44/100, step 48/62,  total step 2714/6200, training_loss = 0.2048\n",
      "epoch 44/100, step 49/62,  total step 2715/6200, training_loss = 0.1867\n",
      "epoch 44/100, step 50/62,  total step 2716/6200, training_loss = 0.2228\n",
      "epoch 44/100, step 51/62,  total step 2717/6200, training_loss = 0.2054\n",
      "epoch 44/100, step 52/62,  total step 2718/6200, training_loss = 0.1737\n",
      "epoch 44/100, step 53/62,  total step 2719/6200, training_loss = 0.2155\n",
      "epoch 44/100, step 54/62,  total step 2720/6200, training_loss = 0.1709\n",
      "epoch 44/100, step 55/62,  total step 2721/6200, training_loss = 0.1812\n",
      "epoch 44/100, step 56/62,  total step 2722/6200, training_loss = 0.2473\n",
      "epoch 44/100, step 57/62,  total step 2723/6200, training_loss = 0.2462\n",
      "epoch 44/100, step 58/62,  total step 2724/6200, training_loss = 0.3427\n",
      "epoch 44/100, step 59/62,  total step 2725/6200, training_loss = 0.1795\n",
      "epoch 44/100, step 60/62,  total step 2726/6200, training_loss = 0.3248\n",
      "epoch 44/100, step 61/62,  total step 2727/6200, training_loss = 0.1544\n",
      "epoch 44/100, step 62/62,  total step 2728/6200, training_loss = 0.2007 | avg loss: 0.2417 Dice Metric:   0.666\n",
      "epoch 45/100, step 1/62,  total step 2729/6200, training_loss = 0.2186\n",
      "epoch 45/100, step 2/62,  total step 2730/6200, training_loss = 0.1955\n",
      "epoch 45/100, step 3/62,  total step 2731/6200, training_loss = 0.2618\n",
      "epoch 45/100, step 4/62,  total step 2732/6200, training_loss = 0.3722\n",
      "epoch 45/100, step 5/62,  total step 2733/6200, training_loss = 0.1553\n",
      "epoch 45/100, step 6/62,  total step 2734/6200, training_loss = 0.2100\n",
      "epoch 45/100, step 7/62,  total step 2735/6200, training_loss = 0.2051\n",
      "epoch 45/100, step 8/62,  total step 2736/6200, training_loss = 0.2746\n",
      "epoch 45/100, step 9/62,  total step 2737/6200, training_loss = 0.2033\n",
      "epoch 45/100, step 10/62,  total step 2738/6200, training_loss = 0.2498\n",
      "epoch 45/100, step 11/62,  total step 2739/6200, training_loss = 0.1602\n",
      "epoch 45/100, step 12/62,  total step 2740/6200, training_loss = 0.1933\n",
      "epoch 45/100, step 13/62,  total step 2741/6200, training_loss = 0.2258\n",
      "epoch 45/100, step 14/62,  total step 2742/6200, training_loss = 0.2788\n",
      "epoch 45/100, step 15/62,  total step 2743/6200, training_loss = 0.3580\n",
      "epoch 45/100, step 16/62,  total step 2744/6200, training_loss = 0.3127\n",
      "epoch 45/100, step 17/62,  total step 2745/6200, training_loss = 0.1935\n",
      "epoch 45/100, step 18/62,  total step 2746/6200, training_loss = 0.2018\n",
      "epoch 45/100, step 19/62,  total step 2747/6200, training_loss = 0.3130\n",
      "epoch 45/100, step 20/62,  total step 2748/6200, training_loss = 0.2615\n",
      "epoch 45/100, step 21/62,  total step 2749/6200, training_loss = 0.2700\n",
      "epoch 45/100, step 22/62,  total step 2750/6200, training_loss = 0.3920\n",
      "epoch 45/100, step 23/62,  total step 2751/6200, training_loss = 0.2403\n",
      "epoch 45/100, step 24/62,  total step 2752/6200, training_loss = 0.1912\n",
      "epoch 45/100, step 25/62,  total step 2753/6200, training_loss = 0.2080\n",
      "epoch 45/100, step 26/62,  total step 2754/6200, training_loss = 0.1723\n",
      "epoch 45/100, step 27/62,  total step 2755/6200, training_loss = 0.2160\n",
      "epoch 45/100, step 28/62,  total step 2756/6200, training_loss = 0.3478\n",
      "epoch 45/100, step 29/62,  total step 2757/6200, training_loss = 0.2604\n",
      "epoch 45/100, step 30/62,  total step 2758/6200, training_loss = 0.2904\n",
      "epoch 45/100, step 31/62,  total step 2759/6200, training_loss = 0.1728\n",
      "epoch 45/100, step 32/62,  total step 2760/6200, training_loss = 0.1918\n",
      "epoch 45/100, step 33/62,  total step 2761/6200, training_loss = 0.3037\n",
      "epoch 45/100, step 34/62,  total step 2762/6200, training_loss = 0.2180\n",
      "epoch 45/100, step 35/62,  total step 2763/6200, training_loss = 0.2611\n",
      "epoch 45/100, step 36/62,  total step 2764/6200, training_loss = 0.3211\n",
      "epoch 45/100, step 37/62,  total step 2765/6200, training_loss = 0.2414\n",
      "epoch 45/100, step 38/62,  total step 2766/6200, training_loss = 0.2543\n",
      "epoch 45/100, step 39/62,  total step 2767/6200, training_loss = 0.3017\n",
      "epoch 45/100, step 40/62,  total step 2768/6200, training_loss = 0.2794\n",
      "epoch 45/100, step 41/62,  total step 2769/6200, training_loss = 0.2208\n",
      "epoch 45/100, step 42/62,  total step 2770/6200, training_loss = 0.2293\n",
      "epoch 45/100, step 43/62,  total step 2771/6200, training_loss = 0.2209\n",
      "epoch 45/100, step 44/62,  total step 2772/6200, training_loss = 0.2463\n",
      "epoch 45/100, step 45/62,  total step 2773/6200, training_loss = 0.1567\n",
      "epoch 45/100, step 46/62,  total step 2774/6200, training_loss = 0.1864\n",
      "epoch 45/100, step 47/62,  total step 2775/6200, training_loss = 0.2760\n",
      "epoch 45/100, step 48/62,  total step 2776/6200, training_loss = 0.1558\n",
      "epoch 45/100, step 49/62,  total step 2777/6200, training_loss = 0.1696\n",
      "epoch 45/100, step 50/62,  total step 2778/6200, training_loss = 0.2433\n",
      "epoch 45/100, step 51/62,  total step 2779/6200, training_loss = 0.2242\n",
      "epoch 45/100, step 52/62,  total step 2780/6200, training_loss = 0.1753\n",
      "epoch 45/100, step 53/62,  total step 2781/6200, training_loss = 0.2161\n",
      "epoch 45/100, step 54/62,  total step 2782/6200, training_loss = 0.1522\n",
      "epoch 45/100, step 55/62,  total step 2783/6200, training_loss = 0.1968\n",
      "epoch 45/100, step 56/62,  total step 2784/6200, training_loss = 0.2515\n",
      "epoch 45/100, step 57/62,  total step 2785/6200, training_loss = 0.2572\n",
      "epoch 45/100, step 58/62,  total step 2786/6200, training_loss = 0.3647\n",
      "epoch 45/100, step 59/62,  total step 2787/6200, training_loss = 0.1939\n",
      "epoch 45/100, step 60/62,  total step 2788/6200, training_loss = 0.3816\n",
      "epoch 45/100, step 61/62,  total step 2789/6200, training_loss = 0.1417\n",
      "epoch 45/100, step 62/62,  total step 2790/6200, training_loss = 0.1669 | avg loss: 0.2388 Dice Metric:    0.67\n",
      "epoch 46/100, step 1/62,  total step 2791/6200, training_loss = 0.2412\n",
      "epoch 46/100, step 2/62,  total step 2792/6200, training_loss = 0.2140\n",
      "epoch 46/100, step 3/62,  total step 2793/6200, training_loss = 0.2567\n",
      "epoch 46/100, step 4/62,  total step 2794/6200, training_loss = 0.3571\n",
      "epoch 46/100, step 5/62,  total step 2795/6200, training_loss = 0.1574\n",
      "epoch 46/100, step 6/62,  total step 2796/6200, training_loss = 0.2481\n",
      "epoch 46/100, step 7/62,  total step 2797/6200, training_loss = 0.2007\n",
      "epoch 46/100, step 8/62,  total step 2798/6200, training_loss = 0.2406\n",
      "epoch 46/100, step 9/62,  total step 2799/6200, training_loss = 0.2068\n",
      "epoch 46/100, step 10/62,  total step 2800/6200, training_loss = 0.2407\n",
      "epoch 46/100, step 11/62,  total step 2801/6200, training_loss = 0.1690\n",
      "epoch 46/100, step 12/62,  total step 2802/6200, training_loss = 0.1808\n",
      "epoch 46/100, step 13/62,  total step 2803/6200, training_loss = 0.2503\n",
      "epoch 46/100, step 14/62,  total step 2804/6200, training_loss = 0.2725\n",
      "epoch 46/100, step 15/62,  total step 2805/6200, training_loss = 0.3784\n",
      "epoch 46/100, step 16/62,  total step 2806/6200, training_loss = 0.3604\n",
      "epoch 46/100, step 17/62,  total step 2807/6200, training_loss = 0.2124\n",
      "epoch 46/100, step 18/62,  total step 2808/6200, training_loss = 0.2052\n",
      "epoch 46/100, step 19/62,  total step 2809/6200, training_loss = 0.2881\n",
      "epoch 46/100, step 20/62,  total step 2810/6200, training_loss = 0.2352\n",
      "epoch 46/100, step 21/62,  total step 2811/6200, training_loss = 0.2535\n",
      "epoch 46/100, step 22/62,  total step 2812/6200, training_loss = 0.4360\n",
      "epoch 46/100, step 23/62,  total step 2813/6200, training_loss = 0.2585\n",
      "epoch 46/100, step 24/62,  total step 2814/6200, training_loss = 0.1963\n",
      "epoch 46/100, step 25/62,  total step 2815/6200, training_loss = 0.2426\n",
      "epoch 46/100, step 26/62,  total step 2816/6200, training_loss = 0.1762\n",
      "epoch 46/100, step 27/62,  total step 2817/6200, training_loss = 0.2448\n",
      "epoch 46/100, step 28/62,  total step 2818/6200, training_loss = 0.3881\n",
      "epoch 46/100, step 29/62,  total step 2819/6200, training_loss = 0.2862\n",
      "epoch 46/100, step 30/62,  total step 2820/6200, training_loss = 0.2706\n",
      "epoch 46/100, step 31/62,  total step 2821/6200, training_loss = 0.1931\n",
      "epoch 46/100, step 32/62,  total step 2822/6200, training_loss = 0.2156\n",
      "epoch 46/100, step 33/62,  total step 2823/6200, training_loss = 0.3126\n",
      "epoch 46/100, step 34/62,  total step 2824/6200, training_loss = 0.2272\n",
      "epoch 46/100, step 35/62,  total step 2825/6200, training_loss = 0.2240\n",
      "epoch 46/100, step 36/62,  total step 2826/6200, training_loss = 0.3026\n",
      "epoch 46/100, step 37/62,  total step 2827/6200, training_loss = 0.2957\n",
      "epoch 46/100, step 38/62,  total step 2828/6200, training_loss = 0.2733\n",
      "epoch 46/100, step 39/62,  total step 2829/6200, training_loss = 0.2751\n",
      "epoch 46/100, step 40/62,  total step 2830/6200, training_loss = 0.2791\n",
      "epoch 46/100, step 41/62,  total step 2831/6200, training_loss = 0.2102\n",
      "epoch 46/100, step 42/62,  total step 2832/6200, training_loss = 0.2342\n",
      "epoch 46/100, step 43/62,  total step 2833/6200, training_loss = 0.2279\n",
      "epoch 46/100, step 44/62,  total step 2834/6200, training_loss = 0.2299\n",
      "epoch 46/100, step 45/62,  total step 2835/6200, training_loss = 0.1526\n",
      "epoch 46/100, step 46/62,  total step 2836/6200, training_loss = 0.1587\n",
      "epoch 46/100, step 47/62,  total step 2837/6200, training_loss = 0.2678\n",
      "epoch 46/100, step 48/62,  total step 2838/6200, training_loss = 0.1843\n",
      "epoch 46/100, step 49/62,  total step 2839/6200, training_loss = 0.1746\n",
      "epoch 46/100, step 50/62,  total step 2840/6200, training_loss = 0.2296\n",
      "epoch 46/100, step 51/62,  total step 2841/6200, training_loss = 0.2186\n",
      "epoch 46/100, step 52/62,  total step 2842/6200, training_loss = 0.1864\n",
      "epoch 46/100, step 53/62,  total step 2843/6200, training_loss = 0.2036\n",
      "epoch 46/100, step 54/62,  total step 2844/6200, training_loss = 0.1542\n",
      "epoch 46/100, step 55/62,  total step 2845/6200, training_loss = 0.2077\n",
      "epoch 46/100, step 56/62,  total step 2846/6200, training_loss = 0.2659\n",
      "epoch 46/100, step 57/62,  total step 2847/6200, training_loss = 0.2605\n",
      "epoch 46/100, step 58/62,  total step 2848/6200, training_loss = 0.3557\n",
      "epoch 46/100, step 59/62,  total step 2849/6200, training_loss = 0.2119\n",
      "epoch 46/100, step 60/62,  total step 2850/6200, training_loss = 0.3168\n",
      "epoch 46/100, step 61/62,  total step 2851/6200, training_loss = 0.1520\n",
      "epoch 46/100, step 62/62,  total step 2852/6200, training_loss = 0.1985 | avg loss: 0.2430 Dice Metric:   0.678\n",
      "epoch 47/100, step 1/62,  total step 2853/6200, training_loss = 0.2220\n",
      "epoch 47/100, step 2/62,  total step 2854/6200, training_loss = 0.2120\n",
      "epoch 47/100, step 3/62,  total step 2855/6200, training_loss = 0.2402\n",
      "epoch 47/100, step 4/62,  total step 2856/6200, training_loss = 0.4114\n",
      "epoch 47/100, step 5/62,  total step 2857/6200, training_loss = 0.1475\n",
      "epoch 47/100, step 6/62,  total step 2858/6200, training_loss = 0.2097\n",
      "epoch 47/100, step 7/62,  total step 2859/6200, training_loss = 0.1975\n",
      "epoch 47/100, step 8/62,  total step 2860/6200, training_loss = 0.2351\n",
      "epoch 47/100, step 9/62,  total step 2861/6200, training_loss = 0.2274\n",
      "epoch 47/100, step 10/62,  total step 2862/6200, training_loss = 0.2388\n",
      "epoch 47/100, step 11/62,  total step 2863/6200, training_loss = 0.1715\n",
      "epoch 47/100, step 12/62,  total step 2864/6200, training_loss = 0.1784\n",
      "epoch 47/100, step 13/62,  total step 2865/6200, training_loss = 0.2208\n",
      "epoch 47/100, step 14/62,  total step 2866/6200, training_loss = 0.2422\n",
      "epoch 47/100, step 15/62,  total step 2867/6200, training_loss = 0.3871\n",
      "epoch 47/100, step 16/62,  total step 2868/6200, training_loss = 0.3915\n",
      "epoch 47/100, step 17/62,  total step 2869/6200, training_loss = 0.2124\n",
      "epoch 47/100, step 18/62,  total step 2870/6200, training_loss = 0.2077\n",
      "epoch 47/100, step 19/62,  total step 2871/6200, training_loss = 0.2670\n",
      "epoch 47/100, step 20/62,  total step 2872/6200, training_loss = 0.2328\n",
      "epoch 47/100, step 21/62,  total step 2873/6200, training_loss = 0.2919\n",
      "epoch 47/100, step 22/62,  total step 2874/6200, training_loss = 0.4110\n",
      "epoch 47/100, step 23/62,  total step 2875/6200, training_loss = 0.2773\n",
      "epoch 47/100, step 24/62,  total step 2876/6200, training_loss = 0.2013\n",
      "epoch 47/100, step 25/62,  total step 2877/6200, training_loss = 0.2308\n",
      "epoch 47/100, step 26/62,  total step 2878/6200, training_loss = 0.1716\n",
      "epoch 47/100, step 27/62,  total step 2879/6200, training_loss = 0.2414\n",
      "epoch 47/100, step 28/62,  total step 2880/6200, training_loss = 0.3554\n",
      "epoch 47/100, step 29/62,  total step 2881/6200, training_loss = 0.2671\n",
      "epoch 47/100, step 30/62,  total step 2882/6200, training_loss = 0.2765\n",
      "epoch 47/100, step 31/62,  total step 2883/6200, training_loss = 0.1575\n",
      "epoch 47/100, step 32/62,  total step 2884/6200, training_loss = 0.2172\n",
      "epoch 47/100, step 33/62,  total step 2885/6200, training_loss = 0.2714\n",
      "epoch 47/100, step 34/62,  total step 2886/6200, training_loss = 0.1957\n",
      "epoch 47/100, step 35/62,  total step 2887/6200, training_loss = 0.2649\n",
      "epoch 47/100, step 36/62,  total step 2888/6200, training_loss = 0.2924\n",
      "epoch 47/100, step 37/62,  total step 2889/6200, training_loss = 0.2304\n",
      "epoch 47/100, step 38/62,  total step 2890/6200, training_loss = 0.2557\n",
      "epoch 47/100, step 39/62,  total step 2891/6200, training_loss = 0.2713\n",
      "epoch 47/100, step 40/62,  total step 2892/6200, training_loss = 0.2761\n",
      "epoch 47/100, step 41/62,  total step 2893/6200, training_loss = 0.1775\n",
      "epoch 47/100, step 42/62,  total step 2894/6200, training_loss = 0.2164\n",
      "epoch 47/100, step 43/62,  total step 2895/6200, training_loss = 0.2098\n",
      "epoch 47/100, step 44/62,  total step 2896/6200, training_loss = 0.2472\n",
      "epoch 47/100, step 45/62,  total step 2897/6200, training_loss = 0.1602\n",
      "epoch 47/100, step 46/62,  total step 2898/6200, training_loss = 0.1714\n",
      "epoch 47/100, step 47/62,  total step 2899/6200, training_loss = 0.2821\n",
      "epoch 47/100, step 48/62,  total step 2900/6200, training_loss = 0.1731\n",
      "epoch 47/100, step 49/62,  total step 2901/6200, training_loss = 0.1664\n",
      "epoch 47/100, step 50/62,  total step 2902/6200, training_loss = 0.2298\n",
      "epoch 47/100, step 51/62,  total step 2903/6200, training_loss = 0.2291\n",
      "epoch 47/100, step 52/62,  total step 2904/6200, training_loss = 0.2002\n",
      "epoch 47/100, step 53/62,  total step 2905/6200, training_loss = 0.1783\n",
      "epoch 47/100, step 54/62,  total step 2906/6200, training_loss = 0.1949\n",
      "epoch 47/100, step 55/62,  total step 2907/6200, training_loss = 0.2008\n",
      "epoch 47/100, step 56/62,  total step 2908/6200, training_loss = 0.2587\n",
      "epoch 47/100, step 57/62,  total step 2909/6200, training_loss = 0.2593\n",
      "epoch 47/100, step 58/62,  total step 2910/6200, training_loss = 0.3589\n",
      "epoch 47/100, step 59/62,  total step 2911/6200, training_loss = 0.2252\n",
      "epoch 47/100, step 60/62,  total step 2912/6200, training_loss = 0.2828\n",
      "epoch 47/100, step 61/62,  total step 2913/6200, training_loss = 0.1607\n",
      "epoch 47/100, step 62/62,  total step 2914/6200, training_loss = 0.2193 | avg loss: 0.2389 Dice Metric:   0.646\n",
      "epoch 48/100, step 1/62,  total step 2915/6200, training_loss = 0.2630\n",
      "epoch 48/100, step 2/62,  total step 2916/6200, training_loss = 0.2280\n",
      "epoch 48/100, step 3/62,  total step 2917/6200, training_loss = 0.2620\n",
      "epoch 48/100, step 4/62,  total step 2918/6200, training_loss = 0.3627\n",
      "epoch 48/100, step 5/62,  total step 2919/6200, training_loss = 0.1495\n",
      "epoch 48/100, step 6/62,  total step 2920/6200, training_loss = 0.2261\n",
      "epoch 48/100, step 7/62,  total step 2921/6200, training_loss = 0.2077\n",
      "epoch 48/100, step 8/62,  total step 2922/6200, training_loss = 0.2487\n",
      "epoch 48/100, step 9/62,  total step 2923/6200, training_loss = 0.1875\n",
      "epoch 48/100, step 10/62,  total step 2924/6200, training_loss = 0.2116\n",
      "epoch 48/100, step 11/62,  total step 2925/6200, training_loss = 0.1590\n",
      "epoch 48/100, step 12/62,  total step 2926/6200, training_loss = 0.1897\n",
      "epoch 48/100, step 13/62,  total step 2927/6200, training_loss = 0.2382\n",
      "epoch 48/100, step 14/62,  total step 2928/6200, training_loss = 0.2790\n",
      "epoch 48/100, step 15/62,  total step 2929/6200, training_loss = 0.3699\n",
      "epoch 48/100, step 16/62,  total step 2930/6200, training_loss = 0.3301\n",
      "epoch 48/100, step 17/62,  total step 2931/6200, training_loss = 0.2001\n",
      "epoch 48/100, step 18/62,  total step 2932/6200, training_loss = 0.2002\n",
      "epoch 48/100, step 19/62,  total step 2933/6200, training_loss = 0.3172\n",
      "epoch 48/100, step 20/62,  total step 2934/6200, training_loss = 0.2558\n",
      "epoch 48/100, step 21/62,  total step 2935/6200, training_loss = 0.2474\n",
      "epoch 48/100, step 22/62,  total step 2936/6200, training_loss = 0.3687\n",
      "epoch 48/100, step 23/62,  total step 2937/6200, training_loss = 0.2788\n",
      "epoch 48/100, step 24/62,  total step 2938/6200, training_loss = 0.2173\n",
      "epoch 48/100, step 25/62,  total step 2939/6200, training_loss = 0.2589\n",
      "epoch 48/100, step 26/62,  total step 2940/6200, training_loss = 0.1775\n",
      "epoch 48/100, step 27/62,  total step 2941/6200, training_loss = 0.2632\n",
      "epoch 48/100, step 28/62,  total step 2942/6200, training_loss = 0.3104\n",
      "epoch 48/100, step 29/62,  total step 2943/6200, training_loss = 0.2931\n",
      "epoch 48/100, step 30/62,  total step 2944/6200, training_loss = 0.2959\n",
      "epoch 48/100, step 31/62,  total step 2945/6200, training_loss = 0.1719\n",
      "epoch 48/100, step 32/62,  total step 2946/6200, training_loss = 0.1668\n",
      "epoch 48/100, step 33/62,  total step 2947/6200, training_loss = 0.3242\n",
      "epoch 48/100, step 34/62,  total step 2948/6200, training_loss = 0.2141\n",
      "epoch 48/100, step 35/62,  total step 2949/6200, training_loss = 0.2314\n",
      "epoch 48/100, step 36/62,  total step 2950/6200, training_loss = 0.2796\n",
      "epoch 48/100, step 37/62,  total step 2951/6200, training_loss = 0.2276\n",
      "epoch 48/100, step 38/62,  total step 2952/6200, training_loss = 0.2857\n",
      "epoch 48/100, step 39/62,  total step 2953/6200, training_loss = 0.2750\n",
      "epoch 48/100, step 40/62,  total step 2954/6200, training_loss = 0.2765\n",
      "epoch 48/100, step 41/62,  total step 2955/6200, training_loss = 0.2000\n",
      "epoch 48/100, step 42/62,  total step 2956/6200, training_loss = 0.2345\n",
      "epoch 48/100, step 43/62,  total step 2957/6200, training_loss = 0.2134\n",
      "epoch 48/100, step 44/62,  total step 2958/6200, training_loss = 0.2116\n",
      "epoch 48/100, step 45/62,  total step 2959/6200, training_loss = 0.1709\n",
      "epoch 48/100, step 46/62,  total step 2960/6200, training_loss = 0.1841\n",
      "epoch 48/100, step 47/62,  total step 2961/6200, training_loss = 0.2579\n",
      "epoch 48/100, step 48/62,  total step 2962/6200, training_loss = 0.1687\n",
      "epoch 48/100, step 49/62,  total step 2963/6200, training_loss = 0.1629\n",
      "epoch 48/100, step 50/62,  total step 2964/6200, training_loss = 0.2152\n",
      "epoch 48/100, step 51/62,  total step 2965/6200, training_loss = 0.2388\n",
      "epoch 48/100, step 52/62,  total step 2966/6200, training_loss = 0.1961\n",
      "epoch 48/100, step 53/62,  total step 2967/6200, training_loss = 0.2216\n",
      "epoch 48/100, step 54/62,  total step 2968/6200, training_loss = 0.2077\n",
      "epoch 48/100, step 55/62,  total step 2969/6200, training_loss = 0.2184\n",
      "epoch 48/100, step 56/62,  total step 2970/6200, training_loss = 0.2730\n",
      "epoch 48/100, step 57/62,  total step 2971/6200, training_loss = 0.2552\n",
      "epoch 48/100, step 58/62,  total step 2972/6200, training_loss = 0.3644\n",
      "epoch 48/100, step 59/62,  total step 2973/6200, training_loss = 0.1924\n",
      "epoch 48/100, step 60/62,  total step 2974/6200, training_loss = 0.3273\n",
      "epoch 48/100, step 61/62,  total step 2975/6200, training_loss = 0.1354\n",
      "epoch 48/100, step 62/62,  total step 2976/6200, training_loss = 0.1852 | avg loss: 0.2401 Dice Metric:   0.651\n",
      "epoch 49/100, step 1/62,  total step 2977/6200, training_loss = 0.2478\n",
      "epoch 49/100, step 2/62,  total step 2978/6200, training_loss = 0.2071\n",
      "epoch 49/100, step 3/62,  total step 2979/6200, training_loss = 0.2492\n",
      "epoch 49/100, step 4/62,  total step 2980/6200, training_loss = 0.3525\n",
      "epoch 49/100, step 5/62,  total step 2981/6200, training_loss = 0.1380\n",
      "epoch 49/100, step 6/62,  total step 2982/6200, training_loss = 0.2280\n",
      "epoch 49/100, step 7/62,  total step 2983/6200, training_loss = 0.1810\n",
      "epoch 49/100, step 8/62,  total step 2984/6200, training_loss = 0.2389\n",
      "epoch 49/100, step 9/62,  total step 2985/6200, training_loss = 0.2297\n",
      "epoch 49/100, step 10/62,  total step 2986/6200, training_loss = 0.2311\n",
      "epoch 49/100, step 11/62,  total step 2987/6200, training_loss = 0.1804\n",
      "epoch 49/100, step 12/62,  total step 2988/6200, training_loss = 0.2002\n",
      "epoch 49/100, step 13/62,  total step 2989/6200, training_loss = 0.2163\n",
      "epoch 49/100, step 14/62,  total step 2990/6200, training_loss = 0.2473\n",
      "epoch 49/100, step 15/62,  total step 2991/6200, training_loss = 0.3844\n",
      "epoch 49/100, step 16/62,  total step 2992/6200, training_loss = 0.3725\n",
      "epoch 49/100, step 17/62,  total step 2993/6200, training_loss = 0.2341\n",
      "epoch 49/100, step 18/62,  total step 2994/6200, training_loss = 0.2691\n",
      "epoch 49/100, step 19/62,  total step 2995/6200, training_loss = 0.2788\n",
      "epoch 49/100, step 20/62,  total step 2996/6200, training_loss = 0.2346\n",
      "epoch 49/100, step 21/62,  total step 2997/6200, training_loss = 0.2512\n",
      "epoch 49/100, step 22/62,  total step 2998/6200, training_loss = 0.3558\n",
      "epoch 49/100, step 23/62,  total step 2999/6200, training_loss = 0.2806\n",
      "epoch 49/100, step 24/62,  total step 3000/6200, training_loss = 0.1889\n",
      "epoch 49/100, step 25/62,  total step 3001/6200, training_loss = 0.2343\n",
      "epoch 49/100, step 26/62,  total step 3002/6200, training_loss = 0.1589\n",
      "epoch 49/100, step 27/62,  total step 3003/6200, training_loss = 0.2658\n",
      "epoch 49/100, step 28/62,  total step 3004/6200, training_loss = 0.3921\n",
      "epoch 49/100, step 29/62,  total step 3005/6200, training_loss = 0.2925\n",
      "epoch 49/100, step 30/62,  total step 3006/6200, training_loss = 0.2962\n",
      "epoch 49/100, step 31/62,  total step 3007/6200, training_loss = 0.1847\n",
      "epoch 49/100, step 32/62,  total step 3008/6200, training_loss = 0.1986\n",
      "epoch 49/100, step 33/62,  total step 3009/6200, training_loss = 0.3028\n",
      "epoch 49/100, step 34/62,  total step 3010/6200, training_loss = 0.2234\n",
      "epoch 49/100, step 35/62,  total step 3011/6200, training_loss = 0.2628\n",
      "epoch 49/100, step 36/62,  total step 3012/6200, training_loss = 0.3063\n",
      "epoch 49/100, step 37/62,  total step 3013/6200, training_loss = 0.2564\n",
      "epoch 49/100, step 38/62,  total step 3014/6200, training_loss = 0.2617\n",
      "epoch 49/100, step 39/62,  total step 3015/6200, training_loss = 0.2757\n",
      "epoch 49/100, step 40/62,  total step 3016/6200, training_loss = 0.2876\n",
      "epoch 49/100, step 41/62,  total step 3017/6200, training_loss = 0.1973\n",
      "epoch 49/100, step 42/62,  total step 3018/6200, training_loss = 0.2123\n",
      "epoch 49/100, step 43/62,  total step 3019/6200, training_loss = 0.2182\n",
      "epoch 49/100, step 44/62,  total step 3020/6200, training_loss = 0.2479\n",
      "epoch 49/100, step 45/62,  total step 3021/6200, training_loss = 0.1585\n",
      "epoch 49/100, step 46/62,  total step 3022/6200, training_loss = 0.1828\n",
      "epoch 49/100, step 47/62,  total step 3023/6200, training_loss = 0.2628\n",
      "epoch 49/100, step 48/62,  total step 3024/6200, training_loss = 0.1778\n",
      "epoch 49/100, step 49/62,  total step 3025/6200, training_loss = 0.1722\n",
      "epoch 49/100, step 50/62,  total step 3026/6200, training_loss = 0.2313\n",
      "epoch 49/100, step 51/62,  total step 3027/6200, training_loss = 0.2356\n",
      "epoch 49/100, step 52/62,  total step 3028/6200, training_loss = 0.1938\n",
      "epoch 49/100, step 53/62,  total step 3029/6200, training_loss = 0.2792\n",
      "epoch 49/100, step 54/62,  total step 3030/6200, training_loss = 0.2143\n",
      "epoch 49/100, step 55/62,  total step 3031/6200, training_loss = 0.2043\n",
      "epoch 49/100, step 56/62,  total step 3032/6200, training_loss = 0.2526\n",
      "epoch 49/100, step 57/62,  total step 3033/6200, training_loss = 0.2460\n",
      "epoch 49/100, step 58/62,  total step 3034/6200, training_loss = 0.3403\n",
      "epoch 49/100, step 59/62,  total step 3035/6200, training_loss = 0.1871\n",
      "epoch 49/100, step 60/62,  total step 3036/6200, training_loss = 0.3858\n",
      "epoch 49/100, step 61/62,  total step 3037/6200, training_loss = 0.1672\n",
      "epoch 49/100, step 62/62,  total step 3038/6200, training_loss = 0.2369 | avg loss: 0.2452 Dice Metric:   0.642\n",
      "epoch 50/100, step 1/62,  total step 3039/6200, training_loss = 0.2107\n",
      "epoch 50/100, step 2/62,  total step 3040/6200, training_loss = 0.1961\n",
      "epoch 50/100, step 3/62,  total step 3041/6200, training_loss = 0.2702\n",
      "epoch 50/100, step 4/62,  total step 3042/6200, training_loss = 0.3567\n",
      "epoch 50/100, step 5/62,  total step 3043/6200, training_loss = 0.1466\n",
      "epoch 50/100, step 6/62,  total step 3044/6200, training_loss = 0.2112\n",
      "epoch 50/100, step 7/62,  total step 3045/6200, training_loss = 0.1944\n",
      "epoch 50/100, step 8/62,  total step 3046/6200, training_loss = 0.2740\n",
      "epoch 50/100, step 9/62,  total step 3047/6200, training_loss = 0.1890\n",
      "epoch 50/100, step 10/62,  total step 3048/6200, training_loss = 0.2371\n",
      "epoch 50/100, step 11/62,  total step 3049/6200, training_loss = 0.1343\n",
      "epoch 50/100, step 12/62,  total step 3050/6200, training_loss = 0.1955\n",
      "epoch 50/100, step 13/62,  total step 3051/6200, training_loss = 0.2422\n",
      "epoch 50/100, step 14/62,  total step 3052/6200, training_loss = 0.2702\n",
      "epoch 50/100, step 15/62,  total step 3053/6200, training_loss = 0.3966\n",
      "epoch 50/100, step 16/62,  total step 3054/6200, training_loss = 0.3273\n",
      "epoch 50/100, step 17/62,  total step 3055/6200, training_loss = 0.2255\n",
      "epoch 50/100, step 18/62,  total step 3056/6200, training_loss = 0.2164\n",
      "epoch 50/100, step 19/62,  total step 3057/6200, training_loss = 0.2841\n",
      "epoch 50/100, step 20/62,  total step 3058/6200, training_loss = 0.2484\n",
      "epoch 50/100, step 21/62,  total step 3059/6200, training_loss = 0.2646\n",
      "epoch 50/100, step 22/62,  total step 3060/6200, training_loss = 0.3488\n",
      "epoch 50/100, step 23/62,  total step 3061/6200, training_loss = 0.2375\n",
      "epoch 50/100, step 24/62,  total step 3062/6200, training_loss = 0.1925\n",
      "epoch 50/100, step 25/62,  total step 3063/6200, training_loss = 0.2113\n",
      "epoch 50/100, step 26/62,  total step 3064/6200, training_loss = 0.1687\n",
      "epoch 50/100, step 27/62,  total step 3065/6200, training_loss = 0.2386\n",
      "epoch 50/100, step 28/62,  total step 3066/6200, training_loss = 0.3437\n",
      "epoch 50/100, step 29/62,  total step 3067/6200, training_loss = 0.2517\n",
      "epoch 50/100, step 30/62,  total step 3068/6200, training_loss = 0.2652\n",
      "epoch 50/100, step 31/62,  total step 3069/6200, training_loss = 0.1749\n",
      "epoch 50/100, step 32/62,  total step 3070/6200, training_loss = 0.2154\n",
      "epoch 50/100, step 33/62,  total step 3071/6200, training_loss = 0.3382\n",
      "epoch 50/100, step 34/62,  total step 3072/6200, training_loss = 0.2082\n",
      "epoch 50/100, step 35/62,  total step 3073/6200, training_loss = 0.2223\n",
      "epoch 50/100, step 36/62,  total step 3074/6200, training_loss = 0.3031\n",
      "epoch 50/100, step 37/62,  total step 3075/6200, training_loss = 0.2165\n",
      "epoch 50/100, step 38/62,  total step 3076/6200, training_loss = 0.2389\n",
      "epoch 50/100, step 39/62,  total step 3077/6200, training_loss = 0.2491\n",
      "epoch 50/100, step 40/62,  total step 3078/6200, training_loss = 0.2567\n",
      "epoch 50/100, step 41/62,  total step 3079/6200, training_loss = 0.2032\n",
      "epoch 50/100, step 42/62,  total step 3080/6200, training_loss = 0.2212\n",
      "epoch 50/100, step 43/62,  total step 3081/6200, training_loss = 0.2105\n",
      "epoch 50/100, step 44/62,  total step 3082/6200, training_loss = 0.3252\n",
      "epoch 50/100, step 45/62,  total step 3083/6200, training_loss = 0.1601\n",
      "epoch 50/100, step 46/62,  total step 3084/6200, training_loss = 0.1899\n",
      "epoch 50/100, step 47/62,  total step 3085/6200, training_loss = 0.2648\n",
      "epoch 50/100, step 48/62,  total step 3086/6200, training_loss = 0.1832\n",
      "epoch 50/100, step 49/62,  total step 3087/6200, training_loss = 0.1768\n",
      "epoch 50/100, step 50/62,  total step 3088/6200, training_loss = 0.2341\n",
      "epoch 50/100, step 51/62,  total step 3089/6200, training_loss = 0.2384\n",
      "epoch 50/100, step 52/62,  total step 3090/6200, training_loss = 0.1737\n",
      "epoch 50/100, step 53/62,  total step 3091/6200, training_loss = 0.2081\n",
      "epoch 50/100, step 54/62,  total step 3092/6200, training_loss = 0.1486\n",
      "epoch 50/100, step 55/62,  total step 3093/6200, training_loss = 0.1827\n",
      "epoch 50/100, step 56/62,  total step 3094/6200, training_loss = 0.2557\n",
      "epoch 50/100, step 57/62,  total step 3095/6200, training_loss = 0.2739\n",
      "epoch 50/100, step 58/62,  total step 3096/6200, training_loss = 0.3648\n",
      "epoch 50/100, step 59/62,  total step 3097/6200, training_loss = 0.1926\n",
      "epoch 50/100, step 60/62,  total step 3098/6200, training_loss = 0.3040\n",
      "epoch 50/100, step 61/62,  total step 3099/6200, training_loss = 0.1512\n",
      "epoch 50/100, step 62/62,  total step 3100/6200, training_loss = 0.2771 | avg loss: 0.2373 Dice Metric:   0.663\n",
      "epoch 51/100, step 1/62,  total step 3101/6200, training_loss = 0.2349\n",
      "epoch 51/100, step 2/62,  total step 3102/6200, training_loss = 0.2040\n",
      "epoch 51/100, step 3/62,  total step 3103/6200, training_loss = 0.2436\n",
      "epoch 51/100, step 4/62,  total step 3104/6200, training_loss = 0.3749\n",
      "epoch 51/100, step 5/62,  total step 3105/6200, training_loss = 0.1487\n",
      "epoch 51/100, step 6/62,  total step 3106/6200, training_loss = 0.1935\n",
      "epoch 51/100, step 7/62,  total step 3107/6200, training_loss = 0.2008\n",
      "epoch 51/100, step 8/62,  total step 3108/6200, training_loss = 0.2688\n",
      "epoch 51/100, step 9/62,  total step 3109/6200, training_loss = 0.1876\n",
      "epoch 51/100, step 10/62,  total step 3110/6200, training_loss = 0.2421\n",
      "epoch 51/100, step 11/62,  total step 3111/6200, training_loss = 0.1592\n",
      "epoch 51/100, step 12/62,  total step 3112/6200, training_loss = 0.1719\n",
      "epoch 51/100, step 13/62,  total step 3113/6200, training_loss = 0.2167\n",
      "epoch 51/100, step 14/62,  total step 3114/6200, training_loss = 0.2486\n",
      "epoch 51/100, step 15/62,  total step 3115/6200, training_loss = 0.3614\n",
      "epoch 51/100, step 16/62,  total step 3116/6200, training_loss = 0.3467\n",
      "epoch 51/100, step 17/62,  total step 3117/6200, training_loss = 0.2546\n",
      "epoch 51/100, step 18/62,  total step 3118/6200, training_loss = 0.2335\n",
      "epoch 51/100, step 19/62,  total step 3119/6200, training_loss = 0.2975\n",
      "epoch 51/100, step 20/62,  total step 3120/6200, training_loss = 0.2156\n",
      "epoch 51/100, step 21/62,  total step 3121/6200, training_loss = 0.2542\n",
      "epoch 51/100, step 22/62,  total step 3122/6200, training_loss = 0.3698\n",
      "epoch 51/100, step 23/62,  total step 3123/6200, training_loss = 0.2541\n",
      "epoch 51/100, step 24/62,  total step 3124/6200, training_loss = 0.1893\n",
      "epoch 51/100, step 25/62,  total step 3125/6200, training_loss = 0.2236\n",
      "epoch 51/100, step 26/62,  total step 3126/6200, training_loss = 0.1690\n",
      "epoch 51/100, step 27/62,  total step 3127/6200, training_loss = 0.2311\n",
      "epoch 51/100, step 28/62,  total step 3128/6200, training_loss = 0.3178\n",
      "epoch 51/100, step 29/62,  total step 3129/6200, training_loss = 0.2522\n",
      "epoch 51/100, step 30/62,  total step 3130/6200, training_loss = 0.2738\n",
      "epoch 51/100, step 31/62,  total step 3131/6200, training_loss = 0.1930\n",
      "epoch 51/100, step 32/62,  total step 3132/6200, training_loss = 0.2139\n",
      "epoch 51/100, step 33/62,  total step 3133/6200, training_loss = 0.2970\n",
      "epoch 51/100, step 34/62,  total step 3134/6200, training_loss = 0.2158\n",
      "epoch 51/100, step 35/62,  total step 3135/6200, training_loss = 0.2674\n",
      "epoch 51/100, step 36/62,  total step 3136/6200, training_loss = 0.2891\n",
      "epoch 51/100, step 37/62,  total step 3137/6200, training_loss = 0.2122\n",
      "epoch 51/100, step 38/62,  total step 3138/6200, training_loss = 0.2321\n",
      "epoch 51/100, step 39/62,  total step 3139/6200, training_loss = 0.2543\n",
      "epoch 51/100, step 40/62,  total step 3140/6200, training_loss = 0.2831\n",
      "epoch 51/100, step 41/62,  total step 3141/6200, training_loss = 0.1933\n",
      "epoch 51/100, step 42/62,  total step 3142/6200, training_loss = 0.2136\n",
      "epoch 51/100, step 43/62,  total step 3143/6200, training_loss = 0.2185\n",
      "epoch 51/100, step 44/62,  total step 3144/6200, training_loss = 0.2453\n",
      "epoch 51/100, step 45/62,  total step 3145/6200, training_loss = 0.1601\n",
      "epoch 51/100, step 46/62,  total step 3146/6200, training_loss = 0.1895\n",
      "epoch 51/100, step 47/62,  total step 3147/6200, training_loss = 0.2591\n",
      "epoch 51/100, step 48/62,  total step 3148/6200, training_loss = 0.1836\n",
      "epoch 51/100, step 49/62,  total step 3149/6200, training_loss = 0.1795\n",
      "epoch 51/100, step 50/62,  total step 3150/6200, training_loss = 0.2193\n",
      "epoch 51/100, step 51/62,  total step 3151/6200, training_loss = 0.2321\n",
      "epoch 51/100, step 52/62,  total step 3152/6200, training_loss = 0.1999\n",
      "epoch 51/100, step 53/62,  total step 3153/6200, training_loss = 0.2188\n",
      "epoch 51/100, step 54/62,  total step 3154/6200, training_loss = 0.1688\n",
      "epoch 51/100, step 55/62,  total step 3155/6200, training_loss = 0.2025\n",
      "epoch 51/100, step 56/62,  total step 3156/6200, training_loss = 0.2591\n",
      "epoch 51/100, step 57/62,  total step 3157/6200, training_loss = 0.2460\n",
      "epoch 51/100, step 58/62,  total step 3158/6200, training_loss = 0.3623\n",
      "epoch 51/100, step 59/62,  total step 3159/6200, training_loss = 0.1967\n",
      "epoch 51/100, step 60/62,  total step 3160/6200, training_loss = 0.3023\n",
      "epoch 51/100, step 61/62,  total step 3161/6200, training_loss = 0.1630\n",
      "epoch 51/100, step 62/62,  total step 3162/6200, training_loss = 0.1940 | avg loss: 0.2356 Dice Metric:   0.638\n",
      "epoch 52/100, step 1/62,  total step 3163/6200, training_loss = 0.2277\n",
      "epoch 52/100, step 2/62,  total step 3164/6200, training_loss = 0.2197\n",
      "epoch 52/100, step 3/62,  total step 3165/6200, training_loss = 0.2580\n",
      "epoch 52/100, step 4/62,  total step 3166/6200, training_loss = 0.3902\n",
      "epoch 52/100, step 5/62,  total step 3167/6200, training_loss = 0.1535\n",
      "epoch 52/100, step 6/62,  total step 3168/6200, training_loss = 0.2215\n",
      "epoch 52/100, step 7/62,  total step 3169/6200, training_loss = 0.1950\n",
      "epoch 52/100, step 8/62,  total step 3170/6200, training_loss = 0.2550\n",
      "epoch 52/100, step 9/62,  total step 3171/6200, training_loss = 0.1834\n",
      "epoch 52/100, step 10/62,  total step 3172/6200, training_loss = 0.2256\n",
      "epoch 52/100, step 11/62,  total step 3173/6200, training_loss = 0.1660\n",
      "epoch 52/100, step 12/62,  total step 3174/6200, training_loss = 0.1947\n",
      "epoch 52/100, step 13/62,  total step 3175/6200, training_loss = 0.2139\n",
      "epoch 52/100, step 14/62,  total step 3176/6200, training_loss = 0.2539\n",
      "epoch 52/100, step 15/62,  total step 3177/6200, training_loss = 0.3468\n",
      "epoch 52/100, step 16/62,  total step 3178/6200, training_loss = 0.3514\n",
      "epoch 52/100, step 17/62,  total step 3179/6200, training_loss = 0.2163\n",
      "epoch 52/100, step 18/62,  total step 3180/6200, training_loss = 0.2195\n",
      "epoch 52/100, step 19/62,  total step 3181/6200, training_loss = 0.2725\n",
      "epoch 52/100, step 20/62,  total step 3182/6200, training_loss = 0.2362\n",
      "epoch 52/100, step 21/62,  total step 3183/6200, training_loss = 0.2417\n",
      "epoch 52/100, step 22/62,  total step 3184/6200, training_loss = 0.3848\n",
      "epoch 52/100, step 23/62,  total step 3185/6200, training_loss = 0.2517\n",
      "epoch 52/100, step 24/62,  total step 3186/6200, training_loss = 0.1884\n",
      "epoch 52/100, step 25/62,  total step 3187/6200, training_loss = 0.2219\n",
      "epoch 52/100, step 26/62,  total step 3188/6200, training_loss = 0.1687\n",
      "epoch 52/100, step 27/62,  total step 3189/6200, training_loss = 0.2379\n",
      "epoch 52/100, step 28/62,  total step 3190/6200, training_loss = 0.3280\n",
      "epoch 52/100, step 29/62,  total step 3191/6200, training_loss = 0.3103\n",
      "epoch 52/100, step 30/62,  total step 3192/6200, training_loss = 0.2691\n",
      "epoch 52/100, step 31/62,  total step 3193/6200, training_loss = 0.1574\n",
      "epoch 52/100, step 32/62,  total step 3194/6200, training_loss = 0.1850\n",
      "epoch 52/100, step 33/62,  total step 3195/6200, training_loss = 0.3184\n",
      "epoch 52/100, step 34/62,  total step 3196/6200, training_loss = 0.2118\n",
      "epoch 52/100, step 35/62,  total step 3197/6200, training_loss = 0.2335\n",
      "epoch 52/100, step 36/62,  total step 3198/6200, training_loss = 0.2779\n",
      "epoch 52/100, step 37/62,  total step 3199/6200, training_loss = 0.2280\n",
      "epoch 52/100, step 38/62,  total step 3200/6200, training_loss = 0.2467\n",
      "epoch 52/100, step 39/62,  total step 3201/6200, training_loss = 0.2677\n",
      "epoch 52/100, step 40/62,  total step 3202/6200, training_loss = 0.2963\n",
      "epoch 52/100, step 41/62,  total step 3203/6200, training_loss = 0.2139\n",
      "epoch 52/100, step 42/62,  total step 3204/6200, training_loss = 0.2261\n",
      "epoch 52/100, step 43/62,  total step 3205/6200, training_loss = 0.2038\n",
      "epoch 52/100, step 44/62,  total step 3206/6200, training_loss = 0.2310\n",
      "epoch 52/100, step 45/62,  total step 3207/6200, training_loss = 0.1550\n",
      "epoch 52/100, step 46/62,  total step 3208/6200, training_loss = 0.1601\n",
      "epoch 52/100, step 47/62,  total step 3209/6200, training_loss = 0.2587\n",
      "epoch 52/100, step 48/62,  total step 3210/6200, training_loss = 0.1831\n",
      "epoch 52/100, step 49/62,  total step 3211/6200, training_loss = 0.1636\n",
      "epoch 52/100, step 50/62,  total step 3212/6200, training_loss = 0.2086\n",
      "epoch 52/100, step 51/62,  total step 3213/6200, training_loss = 0.2160\n",
      "epoch 52/100, step 52/62,  total step 3214/6200, training_loss = 0.1619\n",
      "epoch 52/100, step 53/62,  total step 3215/6200, training_loss = 0.2331\n",
      "epoch 52/100, step 54/62,  total step 3216/6200, training_loss = 0.1948\n",
      "epoch 52/100, step 55/62,  total step 3217/6200, training_loss = 0.1849\n",
      "epoch 52/100, step 56/62,  total step 3218/6200, training_loss = 0.2564\n",
      "epoch 52/100, step 57/62,  total step 3219/6200, training_loss = 0.2674\n",
      "epoch 52/100, step 58/62,  total step 3220/6200, training_loss = 0.3439\n",
      "epoch 52/100, step 59/62,  total step 3221/6200, training_loss = 0.2029\n",
      "epoch 52/100, step 60/62,  total step 3222/6200, training_loss = 0.3611\n",
      "epoch 52/100, step 61/62,  total step 3223/6200, training_loss = 0.1507\n",
      "epoch 52/100, step 62/62,  total step 3224/6200, training_loss = 0.1996 | avg loss: 0.2355 Dice Metric:   0.649\n",
      "epoch 53/100, step 1/62,  total step 3225/6200, training_loss = 0.2394\n",
      "epoch 53/100, step 2/62,  total step 3226/6200, training_loss = 0.2223\n",
      "epoch 53/100, step 3/62,  total step 3227/6200, training_loss = 0.2805\n",
      "epoch 53/100, step 4/62,  total step 3228/6200, training_loss = 0.3672\n",
      "epoch 53/100, step 5/62,  total step 3229/6200, training_loss = 0.1571\n",
      "epoch 53/100, step 6/62,  total step 3230/6200, training_loss = 0.2465\n",
      "epoch 53/100, step 7/62,  total step 3231/6200, training_loss = 0.1899\n",
      "epoch 53/100, step 8/62,  total step 3232/6200, training_loss = 0.2493\n",
      "epoch 53/100, step 9/62,  total step 3233/6200, training_loss = 0.2007\n",
      "epoch 53/100, step 10/62,  total step 3234/6200, training_loss = 0.2911\n",
      "epoch 53/100, step 11/62,  total step 3235/6200, training_loss = 0.1884\n",
      "epoch 53/100, step 12/62,  total step 3236/6200, training_loss = 0.1795\n",
      "epoch 53/100, step 13/62,  total step 3237/6200, training_loss = 0.2169\n",
      "epoch 53/100, step 14/62,  total step 3238/6200, training_loss = 0.2674\n",
      "epoch 53/100, step 15/62,  total step 3239/6200, training_loss = 0.3693\n",
      "epoch 53/100, step 16/62,  total step 3240/6200, training_loss = 0.3551\n",
      "epoch 53/100, step 17/62,  total step 3241/6200, training_loss = 0.2294\n",
      "epoch 53/100, step 18/62,  total step 3242/6200, training_loss = 0.2095\n",
      "epoch 53/100, step 19/62,  total step 3243/6200, training_loss = 0.2724\n",
      "epoch 53/100, step 20/62,  total step 3244/6200, training_loss = 0.2419\n",
      "epoch 53/100, step 21/62,  total step 3245/6200, training_loss = 0.2461\n",
      "epoch 53/100, step 22/62,  total step 3246/6200, training_loss = 0.3504\n",
      "epoch 53/100, step 23/62,  total step 3247/6200, training_loss = 0.2384\n",
      "epoch 53/100, step 24/62,  total step 3248/6200, training_loss = 0.1813\n",
      "epoch 53/100, step 25/62,  total step 3249/6200, training_loss = 0.2264\n",
      "epoch 53/100, step 26/62,  total step 3250/6200, training_loss = 0.1560\n",
      "epoch 53/100, step 27/62,  total step 3251/6200, training_loss = 0.2603\n",
      "epoch 53/100, step 28/62,  total step 3252/6200, training_loss = 0.3488\n",
      "epoch 53/100, step 29/62,  total step 3253/6200, training_loss = 0.2829\n",
      "epoch 53/100, step 30/62,  total step 3254/6200, training_loss = 0.2581\n",
      "epoch 53/100, step 31/62,  total step 3255/6200, training_loss = 0.1681\n",
      "epoch 53/100, step 32/62,  total step 3256/6200, training_loss = 0.2266\n",
      "epoch 53/100, step 33/62,  total step 3257/6200, training_loss = 0.3706\n",
      "epoch 53/100, step 34/62,  total step 3258/6200, training_loss = 0.2067\n",
      "epoch 53/100, step 35/62,  total step 3259/6200, training_loss = 0.2086\n",
      "epoch 53/100, step 36/62,  total step 3260/6200, training_loss = 0.2761\n",
      "epoch 53/100, step 37/62,  total step 3261/6200, training_loss = 0.2271\n",
      "epoch 53/100, step 38/62,  total step 3262/6200, training_loss = 0.2509\n",
      "epoch 53/100, step 39/62,  total step 3263/6200, training_loss = 0.2697\n",
      "epoch 53/100, step 40/62,  total step 3264/6200, training_loss = 0.2525\n",
      "epoch 53/100, step 41/62,  total step 3265/6200, training_loss = 0.2223\n",
      "epoch 53/100, step 42/62,  total step 3266/6200, training_loss = 0.2008\n",
      "epoch 53/100, step 43/62,  total step 3267/6200, training_loss = 0.2194\n",
      "epoch 53/100, step 44/62,  total step 3268/6200, training_loss = 0.2436\n",
      "epoch 53/100, step 45/62,  total step 3269/6200, training_loss = 0.1433\n",
      "epoch 53/100, step 46/62,  total step 3270/6200, training_loss = 0.1804\n",
      "epoch 53/100, step 47/62,  total step 3271/6200, training_loss = 0.2686\n",
      "epoch 53/100, step 48/62,  total step 3272/6200, training_loss = 0.1624\n",
      "epoch 53/100, step 49/62,  total step 3273/6200, training_loss = 0.1799\n",
      "epoch 53/100, step 50/62,  total step 3274/6200, training_loss = 0.1980\n",
      "epoch 53/100, step 51/62,  total step 3275/6200, training_loss = 0.2230\n",
      "epoch 53/100, step 52/62,  total step 3276/6200, training_loss = 0.1468\n",
      "epoch 53/100, step 53/62,  total step 3277/6200, training_loss = 0.2181\n",
      "epoch 53/100, step 54/62,  total step 3278/6200, training_loss = 0.1723\n",
      "epoch 53/100, step 55/62,  total step 3279/6200, training_loss = 0.1997\n",
      "epoch 53/100, step 56/62,  total step 3280/6200, training_loss = 0.2414\n",
      "epoch 53/100, step 57/62,  total step 3281/6200, training_loss = 0.2349\n",
      "epoch 53/100, step 58/62,  total step 3282/6200, training_loss = 0.3843\n",
      "epoch 53/100, step 59/62,  total step 3283/6200, training_loss = 0.2091\n",
      "epoch 53/100, step 60/62,  total step 3284/6200, training_loss = 0.3325\n",
      "epoch 53/100, step 61/62,  total step 3285/6200, training_loss = 0.1618\n",
      "epoch 53/100, step 62/62,  total step 3286/6200, training_loss = 0.2155 | avg loss: 0.2377 Dice Metric:   0.643\n",
      "epoch 54/100, step 1/62,  total step 3287/6200, training_loss = 0.2269\n",
      "epoch 54/100, step 2/62,  total step 3288/6200, training_loss = 0.2089\n",
      "epoch 54/100, step 3/62,  total step 3289/6200, training_loss = 0.2475\n",
      "epoch 54/100, step 4/62,  total step 3290/6200, training_loss = 0.3884\n",
      "epoch 54/100, step 5/62,  total step 3291/6200, training_loss = 0.1635\n",
      "epoch 54/100, step 6/62,  total step 3292/6200, training_loss = 0.2189\n",
      "epoch 54/100, step 7/62,  total step 3293/6200, training_loss = 0.1944\n",
      "epoch 54/100, step 8/62,  total step 3294/6200, training_loss = 0.2519\n",
      "epoch 54/100, step 9/62,  total step 3295/6200, training_loss = 0.2084\n",
      "epoch 54/100, step 10/62,  total step 3296/6200, training_loss = 0.2448\n",
      "epoch 54/100, step 11/62,  total step 3297/6200, training_loss = 0.1614\n",
      "epoch 54/100, step 12/62,  total step 3298/6200, training_loss = 0.1850\n",
      "epoch 54/100, step 13/62,  total step 3299/6200, training_loss = 0.2505\n",
      "epoch 54/100, step 14/62,  total step 3300/6200, training_loss = 0.2424\n",
      "epoch 54/100, step 15/62,  total step 3301/6200, training_loss = 0.3733\n",
      "epoch 54/100, step 16/62,  total step 3302/6200, training_loss = 0.3302\n",
      "epoch 54/100, step 17/62,  total step 3303/6200, training_loss = 0.1853\n",
      "epoch 54/100, step 18/62,  total step 3304/6200, training_loss = 0.2211\n",
      "epoch 54/100, step 19/62,  total step 3305/6200, training_loss = 0.2783\n",
      "epoch 54/100, step 20/62,  total step 3306/6200, training_loss = 0.2684\n",
      "epoch 54/100, step 21/62,  total step 3307/6200, training_loss = 0.2741\n",
      "epoch 54/100, step 22/62,  total step 3308/6200, training_loss = 0.3575\n",
      "epoch 54/100, step 23/62,  total step 3309/6200, training_loss = 0.2349\n",
      "epoch 54/100, step 24/62,  total step 3310/6200, training_loss = 0.1631\n",
      "epoch 54/100, step 25/62,  total step 3311/6200, training_loss = 0.2149\n",
      "epoch 54/100, step 26/62,  total step 3312/6200, training_loss = 0.1691\n",
      "epoch 54/100, step 27/62,  total step 3313/6200, training_loss = 0.2214\n",
      "epoch 54/100, step 28/62,  total step 3314/6200, training_loss = 0.3334\n",
      "epoch 54/100, step 29/62,  total step 3315/6200, training_loss = 0.2988\n",
      "epoch 54/100, step 30/62,  total step 3316/6200, training_loss = 0.2607\n",
      "epoch 54/100, step 31/62,  total step 3317/6200, training_loss = 0.1632\n",
      "epoch 54/100, step 32/62,  total step 3318/6200, training_loss = 0.2076\n",
      "epoch 54/100, step 33/62,  total step 3319/6200, training_loss = 0.2959\n",
      "epoch 54/100, step 34/62,  total step 3320/6200, training_loss = 0.2149\n",
      "epoch 54/100, step 35/62,  total step 3321/6200, training_loss = 0.2079\n",
      "epoch 54/100, step 36/62,  total step 3322/6200, training_loss = 0.2848\n",
      "epoch 54/100, step 37/62,  total step 3323/6200, training_loss = 0.2303\n",
      "epoch 54/100, step 38/62,  total step 3324/6200, training_loss = 0.2583\n",
      "epoch 54/100, step 39/62,  total step 3325/6200, training_loss = 0.2503\n",
      "epoch 54/100, step 40/62,  total step 3326/6200, training_loss = 0.2927\n",
      "epoch 54/100, step 41/62,  total step 3327/6200, training_loss = 0.1947\n",
      "epoch 54/100, step 42/62,  total step 3328/6200, training_loss = 0.2209\n",
      "epoch 54/100, step 43/62,  total step 3329/6200, training_loss = 0.1954\n",
      "epoch 54/100, step 44/62,  total step 3330/6200, training_loss = 0.2408\n",
      "epoch 54/100, step 45/62,  total step 3331/6200, training_loss = 0.1350\n",
      "epoch 54/100, step 46/62,  total step 3332/6200, training_loss = 0.2083\n",
      "epoch 54/100, step 47/62,  total step 3333/6200, training_loss = 0.2862\n",
      "epoch 54/100, step 48/62,  total step 3334/6200, training_loss = 0.1864\n",
      "epoch 54/100, step 49/62,  total step 3335/6200, training_loss = 0.1678\n",
      "epoch 54/100, step 50/62,  total step 3336/6200, training_loss = 0.2002\n",
      "epoch 54/100, step 51/62,  total step 3337/6200, training_loss = 0.2588\n",
      "epoch 54/100, step 52/62,  total step 3338/6200, training_loss = 0.1744\n",
      "epoch 54/100, step 53/62,  total step 3339/6200, training_loss = 0.2210\n",
      "epoch 54/100, step 54/62,  total step 3340/6200, training_loss = 0.1851\n",
      "epoch 54/100, step 55/62,  total step 3341/6200, training_loss = 0.1910\n",
      "epoch 54/100, step 56/62,  total step 3342/6200, training_loss = 0.2519\n",
      "epoch 54/100, step 57/62,  total step 3343/6200, training_loss = 0.2502\n",
      "epoch 54/100, step 58/62,  total step 3344/6200, training_loss = 0.3360\n",
      "epoch 54/100, step 59/62,  total step 3345/6200, training_loss = 0.1876\n",
      "epoch 54/100, step 60/62,  total step 3346/6200, training_loss = 0.3067\n",
      "epoch 54/100, step 61/62,  total step 3347/6200, training_loss = 0.1441\n",
      "epoch 54/100, step 62/62,  total step 3348/6200, training_loss = 0.2332 | avg loss: 0.2348 Dice Metric:   0.644\n",
      "epoch 55/100, step 1/62,  total step 3349/6200, training_loss = 0.2387\n",
      "epoch 55/100, step 2/62,  total step 3350/6200, training_loss = 0.2095\n",
      "epoch 55/100, step 3/62,  total step 3351/6200, training_loss = 0.2429\n",
      "epoch 55/100, step 4/62,  total step 3352/6200, training_loss = 0.3604\n",
      "epoch 55/100, step 5/62,  total step 3353/6200, training_loss = 0.1569\n",
      "epoch 55/100, step 6/62,  total step 3354/6200, training_loss = 0.2159\n",
      "epoch 55/100, step 7/62,  total step 3355/6200, training_loss = 0.1917\n",
      "epoch 55/100, step 8/62,  total step 3356/6200, training_loss = 0.2353\n",
      "epoch 55/100, step 9/62,  total step 3357/6200, training_loss = 0.1849\n",
      "epoch 55/100, step 10/62,  total step 3358/6200, training_loss = 0.2303\n",
      "epoch 55/100, step 11/62,  total step 3359/6200, training_loss = 0.1609\n",
      "epoch 55/100, step 12/62,  total step 3360/6200, training_loss = 0.1554\n",
      "epoch 55/100, step 13/62,  total step 3361/6200, training_loss = 0.2063\n",
      "epoch 55/100, step 14/62,  total step 3362/6200, training_loss = 0.2469\n",
      "epoch 55/100, step 15/62,  total step 3363/6200, training_loss = 0.3434\n",
      "epoch 55/100, step 16/62,  total step 3364/6200, training_loss = 0.3488\n",
      "epoch 55/100, step 17/62,  total step 3365/6200, training_loss = 0.1849\n",
      "epoch 55/100, step 18/62,  total step 3366/6200, training_loss = 0.2161\n",
      "epoch 55/100, step 19/62,  total step 3367/6200, training_loss = 0.2600\n",
      "epoch 55/100, step 20/62,  total step 3368/6200, training_loss = 0.2369\n",
      "epoch 55/100, step 21/62,  total step 3369/6200, training_loss = 0.2569\n",
      "epoch 55/100, step 22/62,  total step 3370/6200, training_loss = 0.3618\n",
      "epoch 55/100, step 23/62,  total step 3371/6200, training_loss = 0.2295\n",
      "epoch 55/100, step 24/62,  total step 3372/6200, training_loss = 0.1826\n",
      "epoch 55/100, step 25/62,  total step 3373/6200, training_loss = 0.2367\n",
      "epoch 55/100, step 26/62,  total step 3374/6200, training_loss = 0.1623\n",
      "epoch 55/100, step 27/62,  total step 3375/6200, training_loss = 0.2856\n",
      "epoch 55/100, step 28/62,  total step 3376/6200, training_loss = 0.3592\n",
      "epoch 55/100, step 29/62,  total step 3377/6200, training_loss = 0.2735\n",
      "epoch 55/100, step 30/62,  total step 3378/6200, training_loss = 0.2407\n",
      "epoch 55/100, step 31/62,  total step 3379/6200, training_loss = 0.1561\n",
      "epoch 55/100, step 32/62,  total step 3380/6200, training_loss = 0.2222\n",
      "epoch 55/100, step 33/62,  total step 3381/6200, training_loss = 0.2856\n",
      "epoch 55/100, step 34/62,  total step 3382/6200, training_loss = 0.1970\n",
      "epoch 55/100, step 35/62,  total step 3383/6200, training_loss = 0.2363\n",
      "epoch 55/100, step 36/62,  total step 3384/6200, training_loss = 0.2812\n",
      "epoch 55/100, step 37/62,  total step 3385/6200, training_loss = 0.2158\n",
      "epoch 55/100, step 38/62,  total step 3386/6200, training_loss = 0.2896\n",
      "epoch 55/100, step 39/62,  total step 3387/6200, training_loss = 0.2593\n",
      "epoch 55/100, step 40/62,  total step 3388/6200, training_loss = 0.3092\n",
      "epoch 55/100, step 41/62,  total step 3389/6200, training_loss = 0.1796\n",
      "epoch 55/100, step 42/62,  total step 3390/6200, training_loss = 0.2236\n",
      "epoch 55/100, step 43/62,  total step 3391/6200, training_loss = 0.2103\n",
      "epoch 55/100, step 44/62,  total step 3392/6200, training_loss = 0.2499\n",
      "epoch 55/100, step 45/62,  total step 3393/6200, training_loss = 0.1549\n",
      "epoch 55/100, step 46/62,  total step 3394/6200, training_loss = 0.1645\n",
      "epoch 55/100, step 47/62,  total step 3395/6200, training_loss = 0.2759\n",
      "epoch 55/100, step 48/62,  total step 3396/6200, training_loss = 0.1858\n",
      "epoch 55/100, step 49/62,  total step 3397/6200, training_loss = 0.1669\n",
      "epoch 55/100, step 50/62,  total step 3398/6200, training_loss = 0.2241\n",
      "epoch 55/100, step 51/62,  total step 3399/6200, training_loss = 0.2230\n",
      "epoch 55/100, step 52/62,  total step 3400/6200, training_loss = 0.1775\n",
      "epoch 55/100, step 53/62,  total step 3401/6200, training_loss = 0.1804\n",
      "epoch 55/100, step 54/62,  total step 3402/6200, training_loss = 0.1679\n",
      "epoch 55/100, step 55/62,  total step 3403/6200, training_loss = 0.1927\n",
      "epoch 55/100, step 56/62,  total step 3404/6200, training_loss = 0.2642\n",
      "epoch 55/100, step 57/62,  total step 3405/6200, training_loss = 0.2452\n",
      "epoch 55/100, step 58/62,  total step 3406/6200, training_loss = 0.3300\n",
      "epoch 55/100, step 59/62,  total step 3407/6200, training_loss = 0.1939\n",
      "epoch 55/100, step 60/62,  total step 3408/6200, training_loss = 0.3232\n",
      "epoch 55/100, step 61/62,  total step 3409/6200, training_loss = 0.1398\n",
      "epoch 55/100, step 62/62,  total step 3410/6200, training_loss = 0.1992 | avg loss: 0.2313 Dice Metric:   0.641\n",
      "epoch 56/100, step 1/62,  total step 3411/6200, training_loss = 0.2215\n",
      "epoch 56/100, step 2/62,  total step 3412/6200, training_loss = 0.1974\n",
      "epoch 56/100, step 3/62,  total step 3413/6200, training_loss = 0.2449\n",
      "epoch 56/100, step 4/62,  total step 3414/6200, training_loss = 0.3804\n",
      "epoch 56/100, step 5/62,  total step 3415/6200, training_loss = 0.1474\n",
      "epoch 56/100, step 6/62,  total step 3416/6200, training_loss = 0.2043\n",
      "epoch 56/100, step 7/62,  total step 3417/6200, training_loss = 0.1957\n",
      "epoch 56/100, step 8/62,  total step 3418/6200, training_loss = 0.2461\n",
      "epoch 56/100, step 9/62,  total step 3419/6200, training_loss = 0.1831\n",
      "epoch 56/100, step 10/62,  total step 3420/6200, training_loss = 0.2190\n",
      "epoch 56/100, step 11/62,  total step 3421/6200, training_loss = 0.1355\n",
      "epoch 56/100, step 12/62,  total step 3422/6200, training_loss = 0.1746\n",
      "epoch 56/100, step 13/62,  total step 3423/6200, training_loss = 0.2094\n",
      "epoch 56/100, step 14/62,  total step 3424/6200, training_loss = 0.2582\n",
      "epoch 56/100, step 15/62,  total step 3425/6200, training_loss = 0.3613\n",
      "epoch 56/100, step 16/62,  total step 3426/6200, training_loss = 0.3274\n",
      "epoch 56/100, step 17/62,  total step 3427/6200, training_loss = 0.2119\n",
      "epoch 56/100, step 18/62,  total step 3428/6200, training_loss = 0.1951\n",
      "epoch 56/100, step 19/62,  total step 3429/6200, training_loss = 0.2725\n",
      "epoch 56/100, step 20/62,  total step 3430/6200, training_loss = 0.2393\n",
      "epoch 56/100, step 21/62,  total step 3431/6200, training_loss = 0.2679\n",
      "epoch 56/100, step 22/62,  total step 3432/6200, training_loss = 0.3775\n",
      "epoch 56/100, step 23/62,  total step 3433/6200, training_loss = 0.2309\n",
      "epoch 56/100, step 24/62,  total step 3434/6200, training_loss = 0.1675\n",
      "epoch 56/100, step 25/62,  total step 3435/6200, training_loss = 0.2232\n",
      "epoch 56/100, step 26/62,  total step 3436/6200, training_loss = 0.1789\n",
      "epoch 56/100, step 27/62,  total step 3437/6200, training_loss = 0.2744\n",
      "epoch 56/100, step 28/62,  total step 3438/6200, training_loss = 0.3110\n",
      "epoch 56/100, step 29/62,  total step 3439/6200, training_loss = 0.2717\n",
      "epoch 56/100, step 30/62,  total step 3440/6200, training_loss = 0.2728\n",
      "epoch 56/100, step 31/62,  total step 3441/6200, training_loss = 0.1532\n",
      "epoch 56/100, step 32/62,  total step 3442/6200, training_loss = 0.2010\n",
      "epoch 56/100, step 33/62,  total step 3443/6200, training_loss = 0.2827\n",
      "epoch 56/100, step 34/62,  total step 3444/6200, training_loss = 0.2147\n",
      "epoch 56/100, step 35/62,  total step 3445/6200, training_loss = 0.2422\n",
      "epoch 56/100, step 36/62,  total step 3446/6200, training_loss = 0.2670\n",
      "epoch 56/100, step 37/62,  total step 3447/6200, training_loss = 0.2462\n",
      "epoch 56/100, step 38/62,  total step 3448/6200, training_loss = 0.2451\n",
      "epoch 56/100, step 39/62,  total step 3449/6200, training_loss = 0.2987\n",
      "epoch 56/100, step 40/62,  total step 3450/6200, training_loss = 0.2440\n",
      "epoch 56/100, step 41/62,  total step 3451/6200, training_loss = 0.1982\n",
      "epoch 56/100, step 42/62,  total step 3452/6200, training_loss = 0.2201\n",
      "epoch 56/100, step 43/62,  total step 3453/6200, training_loss = 0.1970\n",
      "epoch 56/100, step 44/62,  total step 3454/6200, training_loss = 0.2534\n",
      "epoch 56/100, step 45/62,  total step 3455/6200, training_loss = 0.1426\n",
      "epoch 56/100, step 46/62,  total step 3456/6200, training_loss = 0.1657\n",
      "epoch 56/100, step 47/62,  total step 3457/6200, training_loss = 0.2684\n",
      "epoch 56/100, step 48/62,  total step 3458/6200, training_loss = 0.1804\n",
      "epoch 56/100, step 49/62,  total step 3459/6200, training_loss = 0.1700\n",
      "epoch 56/100, step 50/62,  total step 3460/6200, training_loss = 0.2343\n",
      "epoch 56/100, step 51/62,  total step 3461/6200, training_loss = 0.2370\n",
      "epoch 56/100, step 52/62,  total step 3462/6200, training_loss = 0.1668\n",
      "epoch 56/100, step 53/62,  total step 3463/6200, training_loss = 0.1857\n",
      "epoch 56/100, step 54/62,  total step 3464/6200, training_loss = 0.1532\n",
      "epoch 56/100, step 55/62,  total step 3465/6200, training_loss = 0.1767\n",
      "epoch 56/100, step 56/62,  total step 3466/6200, training_loss = 0.2420\n",
      "epoch 56/100, step 57/62,  total step 3467/6200, training_loss = 0.2486\n",
      "epoch 56/100, step 58/62,  total step 3468/6200, training_loss = 0.3600\n",
      "epoch 56/100, step 59/62,  total step 3469/6200, training_loss = 0.1827\n",
      "epoch 56/100, step 60/62,  total step 3470/6200, training_loss = 0.2863\n",
      "epoch 56/100, step 61/62,  total step 3471/6200, training_loss = 0.1420\n",
      "epoch 56/100, step 62/62,  total step 3472/6200, training_loss = 0.2139 | avg loss: 0.2294 Dice Metric:   0.639\n",
      "epoch 57/100, step 1/62,  total step 3473/6200, training_loss = 0.2158\n",
      "epoch 57/100, step 2/62,  total step 3474/6200, training_loss = 0.2083\n",
      "epoch 57/100, step 3/62,  total step 3475/6200, training_loss = 0.2476\n",
      "epoch 57/100, step 4/62,  total step 3476/6200, training_loss = 0.3419\n",
      "epoch 57/100, step 5/62,  total step 3477/6200, training_loss = 0.1371\n",
      "epoch 57/100, step 6/62,  total step 3478/6200, training_loss = 0.2393\n",
      "epoch 57/100, step 7/62,  total step 3479/6200, training_loss = 0.1761\n",
      "epoch 57/100, step 8/62,  total step 3480/6200, training_loss = 0.2429\n",
      "epoch 57/100, step 9/62,  total step 3481/6200, training_loss = 0.1869\n",
      "epoch 57/100, step 10/62,  total step 3482/6200, training_loss = 0.2316\n",
      "epoch 57/100, step 11/62,  total step 3483/6200, training_loss = 0.1815\n",
      "epoch 57/100, step 12/62,  total step 3484/6200, training_loss = 0.1889\n",
      "epoch 57/100, step 13/62,  total step 3485/6200, training_loss = 0.2258\n",
      "epoch 57/100, step 14/62,  total step 3486/6200, training_loss = 0.2535\n",
      "epoch 57/100, step 15/62,  total step 3487/6200, training_loss = 0.3487\n",
      "epoch 57/100, step 16/62,  total step 3488/6200, training_loss = 0.3310\n",
      "epoch 57/100, step 17/62,  total step 3489/6200, training_loss = 0.1961\n",
      "epoch 57/100, step 18/62,  total step 3490/6200, training_loss = 0.2161\n",
      "epoch 57/100, step 19/62,  total step 3491/6200, training_loss = 0.2762\n",
      "epoch 57/100, step 20/62,  total step 3492/6200, training_loss = 0.2187\n",
      "epoch 57/100, step 21/62,  total step 3493/6200, training_loss = 0.2370\n",
      "epoch 57/100, step 22/62,  total step 3494/6200, training_loss = 0.3848\n",
      "epoch 57/100, step 23/62,  total step 3495/6200, training_loss = 0.2163\n",
      "epoch 57/100, step 24/62,  total step 3496/6200, training_loss = 0.1794\n",
      "epoch 57/100, step 25/62,  total step 3497/6200, training_loss = 0.2094\n",
      "epoch 57/100, step 26/62,  total step 3498/6200, training_loss = 0.1700\n",
      "epoch 57/100, step 27/62,  total step 3499/6200, training_loss = 0.2777\n",
      "epoch 57/100, step 28/62,  total step 3500/6200, training_loss = 0.3549\n",
      "epoch 57/100, step 29/62,  total step 3501/6200, training_loss = 0.3054\n",
      "epoch 57/100, step 30/62,  total step 3502/6200, training_loss = 0.2699\n",
      "epoch 57/100, step 31/62,  total step 3503/6200, training_loss = 0.1555\n",
      "epoch 57/100, step 32/62,  total step 3504/6200, training_loss = 0.1889\n",
      "epoch 57/100, step 33/62,  total step 3505/6200, training_loss = 0.2912\n",
      "epoch 57/100, step 34/62,  total step 3506/6200, training_loss = 0.2164\n",
      "epoch 57/100, step 35/62,  total step 3507/6200, training_loss = 0.2394\n",
      "epoch 57/100, step 36/62,  total step 3508/6200, training_loss = 0.2639\n",
      "epoch 57/100, step 37/62,  total step 3509/6200, training_loss = 0.2047\n",
      "epoch 57/100, step 38/62,  total step 3510/6200, training_loss = 0.2591\n",
      "epoch 57/100, step 39/62,  total step 3511/6200, training_loss = 0.2279\n",
      "epoch 57/100, step 40/62,  total step 3512/6200, training_loss = 0.2639\n",
      "epoch 57/100, step 41/62,  total step 3513/6200, training_loss = 0.1979\n",
      "epoch 57/100, step 42/62,  total step 3514/6200, training_loss = 0.2107\n",
      "epoch 57/100, step 43/62,  total step 3515/6200, training_loss = 0.2176\n",
      "epoch 57/100, step 44/62,  total step 3516/6200, training_loss = 0.2322\n",
      "epoch 57/100, step 45/62,  total step 3517/6200, training_loss = 0.1473\n",
      "epoch 57/100, step 46/62,  total step 3518/6200, training_loss = 0.1846\n",
      "epoch 57/100, step 47/62,  total step 3519/6200, training_loss = 0.2782\n",
      "epoch 57/100, step 48/62,  total step 3520/6200, training_loss = 0.1631\n",
      "epoch 57/100, step 49/62,  total step 3521/6200, training_loss = 0.1806\n",
      "epoch 57/100, step 50/62,  total step 3522/6200, training_loss = 0.2336\n",
      "epoch 57/100, step 51/62,  total step 3523/6200, training_loss = 0.2269\n",
      "epoch 57/100, step 52/62,  total step 3524/6200, training_loss = 0.1525\n",
      "epoch 57/100, step 53/62,  total step 3525/6200, training_loss = 0.2107\n",
      "epoch 57/100, step 54/62,  total step 3526/6200, training_loss = 0.1636\n",
      "epoch 57/100, step 55/62,  total step 3527/6200, training_loss = 0.1934\n",
      "epoch 57/100, step 56/62,  total step 3528/6200, training_loss = 0.2225\n",
      "epoch 57/100, step 57/62,  total step 3529/6200, training_loss = 0.2325\n",
      "epoch 57/100, step 58/62,  total step 3530/6200, training_loss = 0.3389\n",
      "epoch 57/100, step 59/62,  total step 3531/6200, training_loss = 0.2131\n",
      "epoch 57/100, step 60/62,  total step 3532/6200, training_loss = 0.2980\n",
      "epoch 57/100, step 61/62,  total step 3533/6200, training_loss = 0.1455\n",
      "epoch 57/100, step 62/62,  total step 3534/6200, training_loss = 0.1889 | avg loss: 0.2292 Dice Metric:   0.629\n",
      "epoch 58/100, step 1/62,  total step 3535/6200, training_loss = 0.2047\n",
      "epoch 58/100, step 2/62,  total step 3536/6200, training_loss = 0.1863\n",
      "epoch 58/100, step 3/62,  total step 3537/6200, training_loss = 0.2739\n",
      "epoch 58/100, step 4/62,  total step 3538/6200, training_loss = 0.3658\n",
      "epoch 58/100, step 5/62,  total step 3539/6200, training_loss = 0.1524\n",
      "epoch 58/100, step 6/62,  total step 3540/6200, training_loss = 0.2254\n",
      "epoch 58/100, step 7/62,  total step 3541/6200, training_loss = 0.1925\n",
      "epoch 58/100, step 8/62,  total step 3542/6200, training_loss = 0.2461\n",
      "epoch 58/100, step 9/62,  total step 3543/6200, training_loss = 0.1943\n",
      "epoch 58/100, step 10/62,  total step 3544/6200, training_loss = 0.2387\n",
      "epoch 58/100, step 11/62,  total step 3545/6200, training_loss = 0.1748\n",
      "epoch 58/100, step 12/62,  total step 3546/6200, training_loss = 0.1931\n",
      "epoch 58/100, step 13/62,  total step 3547/6200, training_loss = 0.2058\n",
      "epoch 58/100, step 14/62,  total step 3548/6200, training_loss = 0.2183\n",
      "epoch 58/100, step 15/62,  total step 3549/6200, training_loss = 0.3487\n",
      "epoch 58/100, step 16/62,  total step 3550/6200, training_loss = 0.3181\n",
      "epoch 58/100, step 17/62,  total step 3551/6200, training_loss = 0.1680\n",
      "epoch 58/100, step 18/62,  total step 3552/6200, training_loss = 0.2141\n",
      "epoch 58/100, step 19/62,  total step 3553/6200, training_loss = 0.2910\n",
      "epoch 58/100, step 20/62,  total step 3554/6200, training_loss = 0.2467\n",
      "epoch 58/100, step 21/62,  total step 3555/6200, training_loss = 0.2355\n",
      "epoch 58/100, step 22/62,  total step 3556/6200, training_loss = 0.3870\n",
      "epoch 58/100, step 23/62,  total step 3557/6200, training_loss = 0.2227\n",
      "epoch 58/100, step 24/62,  total step 3558/6200, training_loss = 0.1817\n",
      "epoch 58/100, step 25/62,  total step 3559/6200, training_loss = 0.2429\n",
      "epoch 58/100, step 26/62,  total step 3560/6200, training_loss = 0.1851\n",
      "epoch 58/100, step 27/62,  total step 3561/6200, training_loss = 0.2492\n",
      "epoch 58/100, step 28/62,  total step 3562/6200, training_loss = 0.2765\n",
      "epoch 58/100, step 29/62,  total step 3563/6200, training_loss = 0.3081\n",
      "epoch 58/100, step 30/62,  total step 3564/6200, training_loss = 0.2842\n",
      "epoch 58/100, step 31/62,  total step 3565/6200, training_loss = 0.1686\n",
      "epoch 58/100, step 32/62,  total step 3566/6200, training_loss = 0.2089\n",
      "epoch 58/100, step 33/62,  total step 3567/6200, training_loss = 0.2818\n",
      "epoch 58/100, step 34/62,  total step 3568/6200, training_loss = 0.1996\n",
      "epoch 58/100, step 35/62,  total step 3569/6200, training_loss = 0.2406\n",
      "epoch 58/100, step 36/62,  total step 3570/6200, training_loss = 0.2656\n",
      "epoch 58/100, step 37/62,  total step 3571/6200, training_loss = 0.2328\n",
      "epoch 58/100, step 38/62,  total step 3572/6200, training_loss = 0.2529\n",
      "epoch 58/100, step 39/62,  total step 3573/6200, training_loss = 0.2658\n",
      "epoch 58/100, step 40/62,  total step 3574/6200, training_loss = 0.2546\n",
      "epoch 58/100, step 41/62,  total step 3575/6200, training_loss = 0.1876\n",
      "epoch 58/100, step 42/62,  total step 3576/6200, training_loss = 0.1889\n",
      "epoch 58/100, step 43/62,  total step 3577/6200, training_loss = 0.2436\n",
      "epoch 58/100, step 44/62,  total step 3578/6200, training_loss = 0.2529\n",
      "epoch 58/100, step 45/62,  total step 3579/6200, training_loss = 0.1518\n",
      "epoch 58/100, step 46/62,  total step 3580/6200, training_loss = 0.1923\n",
      "epoch 58/100, step 47/62,  total step 3581/6200, training_loss = 0.2873\n",
      "epoch 58/100, step 48/62,  total step 3582/6200, training_loss = 0.1726\n",
      "epoch 58/100, step 49/62,  total step 3583/6200, training_loss = 0.1610\n",
      "epoch 58/100, step 50/62,  total step 3584/6200, training_loss = 0.2325\n",
      "epoch 58/100, step 51/62,  total step 3585/6200, training_loss = 0.2226\n",
      "epoch 58/100, step 52/62,  total step 3586/6200, training_loss = 0.2004\n",
      "epoch 58/100, step 53/62,  total step 3587/6200, training_loss = 0.2229\n",
      "epoch 58/100, step 54/62,  total step 3588/6200, training_loss = 0.1919\n",
      "epoch 58/100, step 55/62,  total step 3589/6200, training_loss = 0.2104\n",
      "epoch 58/100, step 56/62,  total step 3590/6200, training_loss = 0.2323\n",
      "epoch 58/100, step 57/62,  total step 3591/6200, training_loss = 0.2533\n",
      "epoch 58/100, step 58/62,  total step 3592/6200, training_loss = 0.3409\n",
      "epoch 58/100, step 59/62,  total step 3593/6200, training_loss = 0.2090\n",
      "epoch 58/100, step 60/62,  total step 3594/6200, training_loss = 0.3480\n",
      "epoch 58/100, step 61/62,  total step 3595/6200, training_loss = 0.1461\n",
      "epoch 58/100, step 62/62,  total step 3596/6200, training_loss = 0.2146 | avg loss: 0.2333 Dice Metric:   0.623\n",
      "epoch 59/100, step 1/62,  total step 3597/6200, training_loss = 0.2113\n",
      "epoch 59/100, step 2/62,  total step 3598/6200, training_loss = 0.1842\n",
      "epoch 59/100, step 3/62,  total step 3599/6200, training_loss = 0.2439\n",
      "epoch 59/100, step 4/62,  total step 3600/6200, training_loss = 0.3593\n",
      "epoch 59/100, step 5/62,  total step 3601/6200, training_loss = 0.1340\n",
      "epoch 59/100, step 6/62,  total step 3602/6200, training_loss = 0.1956\n",
      "epoch 59/100, step 7/62,  total step 3603/6200, training_loss = 0.1895\n",
      "epoch 59/100, step 8/62,  total step 3604/6200, training_loss = 0.2636\n",
      "epoch 59/100, step 9/62,  total step 3605/6200, training_loss = 0.2382\n",
      "epoch 59/100, step 10/62,  total step 3606/6200, training_loss = 0.2434\n",
      "epoch 59/100, step 11/62,  total step 3607/6200, training_loss = 0.1460\n",
      "epoch 59/100, step 12/62,  total step 3608/6200, training_loss = 0.1557\n",
      "epoch 59/100, step 13/62,  total step 3609/6200, training_loss = 0.2293\n",
      "epoch 59/100, step 14/62,  total step 3610/6200, training_loss = 0.2664\n",
      "epoch 59/100, step 15/62,  total step 3611/6200, training_loss = 0.4100\n",
      "epoch 59/100, step 16/62,  total step 3612/6200, training_loss = 0.3599\n",
      "epoch 59/100, step 17/62,  total step 3613/6200, training_loss = 0.1867\n",
      "epoch 59/100, step 18/62,  total step 3614/6200, training_loss = 0.2295\n",
      "epoch 59/100, step 19/62,  total step 3615/6200, training_loss = 0.2924\n",
      "epoch 59/100, step 20/62,  total step 3616/6200, training_loss = 0.2331\n",
      "epoch 59/100, step 21/62,  total step 3617/6200, training_loss = 0.2480\n",
      "epoch 59/100, step 22/62,  total step 3618/6200, training_loss = 0.3769\n",
      "epoch 59/100, step 23/62,  total step 3619/6200, training_loss = 0.2567\n",
      "epoch 59/100, step 24/62,  total step 3620/6200, training_loss = 0.1917\n",
      "epoch 59/100, step 25/62,  total step 3621/6200, training_loss = 0.2267\n",
      "epoch 59/100, step 26/62,  total step 3622/6200, training_loss = 0.1460\n",
      "epoch 59/100, step 27/62,  total step 3623/6200, training_loss = 0.2645\n",
      "epoch 59/100, step 28/62,  total step 3624/6200, training_loss = 0.3208\n",
      "epoch 59/100, step 29/62,  total step 3625/6200, training_loss = 0.2730\n",
      "epoch 59/100, step 30/62,  total step 3626/6200, training_loss = 0.2507\n",
      "epoch 59/100, step 31/62,  total step 3627/6200, training_loss = 0.1492\n",
      "epoch 59/100, step 32/62,  total step 3628/6200, training_loss = 0.1955\n",
      "epoch 59/100, step 33/62,  total step 3629/6200, training_loss = 0.3630\n",
      "epoch 59/100, step 34/62,  total step 3630/6200, training_loss = 0.2264\n",
      "epoch 59/100, step 35/62,  total step 3631/6200, training_loss = 0.2450\n",
      "epoch 59/100, step 36/62,  total step 3632/6200, training_loss = 0.2575\n",
      "epoch 59/100, step 37/62,  total step 3633/6200, training_loss = 0.2381\n",
      "epoch 59/100, step 38/62,  total step 3634/6200, training_loss = 0.2619\n",
      "epoch 59/100, step 39/62,  total step 3635/6200, training_loss = 0.2461\n",
      "epoch 59/100, step 40/62,  total step 3636/6200, training_loss = 0.2450\n",
      "epoch 59/100, step 41/62,  total step 3637/6200, training_loss = 0.1925\n",
      "epoch 59/100, step 42/62,  total step 3638/6200, training_loss = 0.2253\n",
      "epoch 59/100, step 43/62,  total step 3639/6200, training_loss = 0.2167\n",
      "epoch 59/100, step 44/62,  total step 3640/6200, training_loss = 0.2256\n",
      "epoch 59/100, step 45/62,  total step 3641/6200, training_loss = 0.1262\n",
      "epoch 59/100, step 46/62,  total step 3642/6200, training_loss = 0.1680\n",
      "epoch 59/100, step 47/62,  total step 3643/6200, training_loss = 0.2500\n",
      "epoch 59/100, step 48/62,  total step 3644/6200, training_loss = 0.1669\n",
      "epoch 59/100, step 49/62,  total step 3645/6200, training_loss = 0.1648\n",
      "epoch 59/100, step 50/62,  total step 3646/6200, training_loss = 0.2030\n",
      "epoch 59/100, step 51/62,  total step 3647/6200, training_loss = 0.2269\n",
      "epoch 59/100, step 52/62,  total step 3648/6200, training_loss = 0.1706\n",
      "epoch 59/100, step 53/62,  total step 3649/6200, training_loss = 0.1790\n",
      "epoch 59/100, step 54/62,  total step 3650/6200, training_loss = 0.1647\n",
      "epoch 59/100, step 55/62,  total step 3651/6200, training_loss = 0.1929\n",
      "epoch 59/100, step 56/62,  total step 3652/6200, training_loss = 0.2385\n",
      "epoch 59/100, step 57/62,  total step 3653/6200, training_loss = 0.2762\n",
      "epoch 59/100, step 58/62,  total step 3654/6200, training_loss = 0.3567\n",
      "epoch 59/100, step 59/62,  total step 3655/6200, training_loss = 0.2034\n",
      "epoch 59/100, step 60/62,  total step 3656/6200, training_loss = 0.2848\n",
      "epoch 59/100, step 61/62,  total step 3657/6200, training_loss = 0.1555\n",
      "epoch 59/100, step 62/62,  total step 3658/6200, training_loss = 0.1947 | avg loss: 0.2313 Dice Metric:   0.645\n",
      "epoch 60/100, step 1/62,  total step 3659/6200, training_loss = 0.2185\n",
      "epoch 60/100, step 2/62,  total step 3660/6200, training_loss = 0.2177\n",
      "epoch 60/100, step 3/62,  total step 3661/6200, training_loss = 0.2475\n",
      "epoch 60/100, step 4/62,  total step 3662/6200, training_loss = 0.3888\n",
      "epoch 60/100, step 5/62,  total step 3663/6200, training_loss = 0.1568\n",
      "epoch 60/100, step 6/62,  total step 3664/6200, training_loss = 0.2211\n",
      "epoch 60/100, step 7/62,  total step 3665/6200, training_loss = 0.1848\n",
      "epoch 60/100, step 8/62,  total step 3666/6200, training_loss = 0.2479\n",
      "epoch 60/100, step 9/62,  total step 3667/6200, training_loss = 0.1717\n",
      "epoch 60/100, step 10/62,  total step 3668/6200, training_loss = 0.2047\n",
      "epoch 60/100, step 11/62,  total step 3669/6200, training_loss = 0.1791\n",
      "epoch 60/100, step 12/62,  total step 3670/6200, training_loss = 0.1404\n",
      "epoch 60/100, step 13/62,  total step 3671/6200, training_loss = 0.2187\n",
      "epoch 60/100, step 14/62,  total step 3672/6200, training_loss = 0.2720\n",
      "epoch 60/100, step 15/62,  total step 3673/6200, training_loss = 0.3986\n",
      "epoch 60/100, step 16/62,  total step 3674/6200, training_loss = 0.3370\n",
      "epoch 60/100, step 17/62,  total step 3675/6200, training_loss = 0.1844\n",
      "epoch 60/100, step 18/62,  total step 3676/6200, training_loss = 0.1902\n",
      "epoch 60/100, step 19/62,  total step 3677/6200, training_loss = 0.2768\n",
      "epoch 60/100, step 20/62,  total step 3678/6200, training_loss = 0.2333\n",
      "epoch 60/100, step 21/62,  total step 3679/6200, training_loss = 0.2516\n",
      "epoch 60/100, step 22/62,  total step 3680/6200, training_loss = 0.3849\n",
      "epoch 60/100, step 23/62,  total step 3681/6200, training_loss = 0.2353\n",
      "epoch 60/100, step 24/62,  total step 3682/6200, training_loss = 0.1835\n",
      "epoch 60/100, step 25/62,  total step 3683/6200, training_loss = 0.2296\n",
      "epoch 60/100, step 26/62,  total step 3684/6200, training_loss = 0.1784\n",
      "epoch 60/100, step 27/62,  total step 3685/6200, training_loss = 0.2411\n",
      "epoch 60/100, step 28/62,  total step 3686/6200, training_loss = 0.3360\n",
      "epoch 60/100, step 29/62,  total step 3687/6200, training_loss = 0.2549\n",
      "epoch 60/100, step 30/62,  total step 3688/6200, training_loss = 0.2480\n",
      "epoch 60/100, step 31/62,  total step 3689/6200, training_loss = 0.1556\n",
      "epoch 60/100, step 32/62,  total step 3690/6200, training_loss = 0.2132\n",
      "epoch 60/100, step 33/62,  total step 3691/6200, training_loss = 0.3225\n",
      "epoch 60/100, step 34/62,  total step 3692/6200, training_loss = 0.2077\n",
      "epoch 60/100, step 35/62,  total step 3693/6200, training_loss = 0.2079\n",
      "epoch 60/100, step 36/62,  total step 3694/6200, training_loss = 0.2472\n",
      "epoch 60/100, step 37/62,  total step 3695/6200, training_loss = 0.2188\n",
      "epoch 60/100, step 38/62,  total step 3696/6200, training_loss = 0.2420\n",
      "epoch 60/100, step 39/62,  total step 3697/6200, training_loss = 0.2342\n",
      "epoch 60/100, step 40/62,  total step 3698/6200, training_loss = 0.2483\n",
      "epoch 60/100, step 41/62,  total step 3699/6200, training_loss = 0.2019\n",
      "epoch 60/100, step 42/62,  total step 3700/6200, training_loss = 0.2082\n",
      "epoch 60/100, step 43/62,  total step 3701/6200, training_loss = 0.2129\n",
      "epoch 60/100, step 44/62,  total step 3702/6200, training_loss = 0.2449\n",
      "epoch 60/100, step 45/62,  total step 3703/6200, training_loss = 0.1271\n",
      "epoch 60/100, step 46/62,  total step 3704/6200, training_loss = 0.1890\n",
      "epoch 60/100, step 47/62,  total step 3705/6200, training_loss = 0.2993\n",
      "epoch 60/100, step 48/62,  total step 3706/6200, training_loss = 0.1515\n",
      "epoch 60/100, step 49/62,  total step 3707/6200, training_loss = 0.1530\n",
      "epoch 60/100, step 50/62,  total step 3708/6200, training_loss = 0.1965\n",
      "epoch 60/100, step 51/62,  total step 3709/6200, training_loss = 0.2127\n",
      "epoch 60/100, step 52/62,  total step 3710/6200, training_loss = 0.1895\n",
      "epoch 60/100, step 53/62,  total step 3711/6200, training_loss = 0.2046\n",
      "epoch 60/100, step 54/62,  total step 3712/6200, training_loss = 0.1521\n",
      "epoch 60/100, step 55/62,  total step 3713/6200, training_loss = 0.2063\n",
      "epoch 60/100, step 56/62,  total step 3714/6200, training_loss = 0.2106\n",
      "epoch 60/100, step 57/62,  total step 3715/6200, training_loss = 0.2415\n",
      "epoch 60/100, step 58/62,  total step 3716/6200, training_loss = 0.3252\n",
      "epoch 60/100, step 59/62,  total step 3717/6200, training_loss = 0.1837\n",
      "epoch 60/100, step 60/62,  total step 3718/6200, training_loss = 0.2840\n",
      "epoch 60/100, step 61/62,  total step 3719/6200, training_loss = 0.1499\n",
      "epoch 60/100, step 62/62,  total step 3720/6200, training_loss = 0.1663 | avg loss: 0.2265 Dice Metric:   0.654\n",
      "epoch 61/100, step 1/62,  total step 3721/6200, training_loss = 0.2444\n",
      "epoch 61/100, step 2/62,  total step 3722/6200, training_loss = 0.2131\n",
      "epoch 61/100, step 3/62,  total step 3723/6200, training_loss = 0.2198\n",
      "epoch 61/100, step 4/62,  total step 3724/6200, training_loss = 0.3561\n",
      "epoch 61/100, step 5/62,  total step 3725/6200, training_loss = 0.1413\n",
      "epoch 61/100, step 6/62,  total step 3726/6200, training_loss = 0.2127\n",
      "epoch 61/100, step 7/62,  total step 3727/6200, training_loss = 0.1852\n",
      "epoch 61/100, step 8/62,  total step 3728/6200, training_loss = 0.2481\n",
      "epoch 61/100, step 9/62,  total step 3729/6200, training_loss = 0.2102\n",
      "epoch 61/100, step 10/62,  total step 3730/6200, training_loss = 0.1962\n",
      "epoch 61/100, step 11/62,  total step 3731/6200, training_loss = 0.1639\n",
      "epoch 61/100, step 12/62,  total step 3732/6200, training_loss = 0.1557\n",
      "epoch 61/100, step 13/62,  total step 3733/6200, training_loss = 0.2124\n",
      "epoch 61/100, step 14/62,  total step 3734/6200, training_loss = 0.2570\n",
      "epoch 61/100, step 15/62,  total step 3735/6200, training_loss = 0.3609\n",
      "epoch 61/100, step 16/62,  total step 3736/6200, training_loss = 0.3234\n",
      "epoch 61/100, step 17/62,  total step 3737/6200, training_loss = 0.2001\n",
      "epoch 61/100, step 18/62,  total step 3738/6200, training_loss = 0.2307\n",
      "epoch 61/100, step 19/62,  total step 3739/6200, training_loss = 0.2642\n",
      "epoch 61/100, step 20/62,  total step 3740/6200, training_loss = 0.2500\n",
      "epoch 61/100, step 21/62,  total step 3741/6200, training_loss = 0.2193\n",
      "epoch 61/100, step 22/62,  total step 3742/6200, training_loss = 0.3614\n",
      "epoch 61/100, step 23/62,  total step 3743/6200, training_loss = 0.2134\n",
      "epoch 61/100, step 24/62,  total step 3744/6200, training_loss = 0.1753\n",
      "epoch 61/100, step 25/62,  total step 3745/6200, training_loss = 0.2079\n",
      "epoch 61/100, step 26/62,  total step 3746/6200, training_loss = 0.1730\n",
      "epoch 61/100, step 27/62,  total step 3747/6200, training_loss = 0.2285\n",
      "epoch 61/100, step 28/62,  total step 3748/6200, training_loss = 0.3187\n",
      "epoch 61/100, step 29/62,  total step 3749/6200, training_loss = 0.2850\n",
      "epoch 61/100, step 30/62,  total step 3750/6200, training_loss = 0.2471\n",
      "epoch 61/100, step 31/62,  total step 3751/6200, training_loss = 0.1560\n",
      "epoch 61/100, step 32/62,  total step 3752/6200, training_loss = 0.2260\n",
      "epoch 61/100, step 33/62,  total step 3753/6200, training_loss = 0.3084\n",
      "epoch 61/100, step 34/62,  total step 3754/6200, training_loss = 0.1972\n",
      "epoch 61/100, step 35/62,  total step 3755/6200, training_loss = 0.2147\n",
      "epoch 61/100, step 36/62,  total step 3756/6200, training_loss = 0.2524\n",
      "epoch 61/100, step 37/62,  total step 3757/6200, training_loss = 0.2917\n",
      "epoch 61/100, step 38/62,  total step 3758/6200, training_loss = 0.2641\n",
      "epoch 61/100, step 39/62,  total step 3759/6200, training_loss = 0.2762\n",
      "epoch 61/100, step 40/62,  total step 3760/6200, training_loss = 0.2719\n",
      "epoch 61/100, step 41/62,  total step 3761/6200, training_loss = 0.2293\n",
      "epoch 61/100, step 42/62,  total step 3762/6200, training_loss = 0.2433\n",
      "epoch 61/100, step 43/62,  total step 3763/6200, training_loss = 0.2131\n",
      "epoch 61/100, step 44/62,  total step 3764/6200, training_loss = 0.2171\n",
      "epoch 61/100, step 45/62,  total step 3765/6200, training_loss = 0.1399\n",
      "epoch 61/100, step 46/62,  total step 3766/6200, training_loss = 0.1907\n",
      "epoch 61/100, step 47/62,  total step 3767/6200, training_loss = 0.2735\n",
      "epoch 61/100, step 48/62,  total step 3768/6200, training_loss = 0.1453\n",
      "epoch 61/100, step 49/62,  total step 3769/6200, training_loss = 0.1819\n",
      "epoch 61/100, step 50/62,  total step 3770/6200, training_loss = 0.2082\n",
      "epoch 61/100, step 51/62,  total step 3771/6200, training_loss = 0.1936\n",
      "epoch 61/100, step 52/62,  total step 3772/6200, training_loss = 0.1753\n",
      "epoch 61/100, step 53/62,  total step 3773/6200, training_loss = 0.2181\n",
      "epoch 61/100, step 54/62,  total step 3774/6200, training_loss = 0.1853\n",
      "epoch 61/100, step 55/62,  total step 3775/6200, training_loss = 0.1798\n",
      "epoch 61/100, step 56/62,  total step 3776/6200, training_loss = 0.2284\n",
      "epoch 61/100, step 57/62,  total step 3777/6200, training_loss = 0.2181\n",
      "epoch 61/100, step 58/62,  total step 3778/6200, training_loss = 0.3387\n",
      "epoch 61/100, step 59/62,  total step 3779/6200, training_loss = 0.2077\n",
      "epoch 61/100, step 60/62,  total step 3780/6200, training_loss = 0.2878\n",
      "epoch 61/100, step 61/62,  total step 3781/6200, training_loss = 0.1473\n",
      "epoch 61/100, step 62/62,  total step 3782/6200, training_loss = 0.1833 | avg loss: 0.2281 Dice Metric:   0.656\n",
      "epoch 62/100, step 1/62,  total step 3783/6200, training_loss = 0.2306\n",
      "epoch 62/100, step 2/62,  total step 3784/6200, training_loss = 0.1961\n",
      "epoch 62/100, step 3/62,  total step 3785/6200, training_loss = 0.2357\n",
      "epoch 62/100, step 4/62,  total step 3786/6200, training_loss = 0.3591\n",
      "epoch 62/100, step 5/62,  total step 3787/6200, training_loss = 0.1491\n",
      "epoch 62/100, step 6/62,  total step 3788/6200, training_loss = 0.2339\n",
      "epoch 62/100, step 7/62,  total step 3789/6200, training_loss = 0.1958\n",
      "epoch 62/100, step 8/62,  total step 3790/6200, training_loss = 0.2441\n",
      "epoch 62/100, step 9/62,  total step 3791/6200, training_loss = 0.2042\n",
      "epoch 62/100, step 10/62,  total step 3792/6200, training_loss = 0.2170\n",
      "epoch 62/100, step 11/62,  total step 3793/6200, training_loss = 0.1685\n",
      "epoch 62/100, step 12/62,  total step 3794/6200, training_loss = 0.1670\n",
      "epoch 62/100, step 13/62,  total step 3795/6200, training_loss = 0.2254\n",
      "epoch 62/100, step 14/62,  total step 3796/6200, training_loss = 0.2465\n",
      "epoch 62/100, step 15/62,  total step 3797/6200, training_loss = 0.3577\n",
      "epoch 62/100, step 16/62,  total step 3798/6200, training_loss = 0.3548\n",
      "epoch 62/100, step 17/62,  total step 3799/6200, training_loss = 0.1723\n",
      "epoch 62/100, step 18/62,  total step 3800/6200, training_loss = 0.2171\n",
      "epoch 62/100, step 19/62,  total step 3801/6200, training_loss = 0.2804\n",
      "epoch 62/100, step 20/62,  total step 3802/6200, training_loss = 0.2737\n",
      "epoch 62/100, step 21/62,  total step 3803/6200, training_loss = 0.2273\n",
      "epoch 62/100, step 22/62,  total step 3804/6200, training_loss = 0.3373\n",
      "epoch 62/100, step 23/62,  total step 3805/6200, training_loss = 0.2195\n",
      "epoch 62/100, step 24/62,  total step 3806/6200, training_loss = 0.1733\n",
      "epoch 62/100, step 25/62,  total step 3807/6200, training_loss = 0.1960\n",
      "epoch 62/100, step 26/62,  total step 3808/6200, training_loss = 0.1668\n",
      "epoch 62/100, step 27/62,  total step 3809/6200, training_loss = 0.2315\n",
      "epoch 62/100, step 28/62,  total step 3810/6200, training_loss = 0.3363\n",
      "epoch 62/100, step 29/62,  total step 3811/6200, training_loss = 0.2641\n",
      "epoch 62/100, step 30/62,  total step 3812/6200, training_loss = 0.2668\n",
      "epoch 62/100, step 31/62,  total step 3813/6200, training_loss = 0.1596\n",
      "epoch 62/100, step 32/62,  total step 3814/6200, training_loss = 0.2037\n",
      "epoch 62/100, step 33/62,  total step 3815/6200, training_loss = 0.3157\n",
      "epoch 62/100, step 34/62,  total step 3816/6200, training_loss = 0.1972\n",
      "epoch 62/100, step 35/62,  total step 3817/6200, training_loss = 0.2284\n",
      "epoch 62/100, step 36/62,  total step 3818/6200, training_loss = 0.3108\n",
      "epoch 62/100, step 37/62,  total step 3819/6200, training_loss = 0.2573\n",
      "epoch 62/100, step 38/62,  total step 3820/6200, training_loss = 0.2687\n",
      "epoch 62/100, step 39/62,  total step 3821/6200, training_loss = 0.2938\n",
      "epoch 62/100, step 40/62,  total step 3822/6200, training_loss = 0.2875\n",
      "epoch 62/100, step 41/62,  total step 3823/6200, training_loss = 0.1841\n",
      "epoch 62/100, step 42/62,  total step 3824/6200, training_loss = 0.2202\n",
      "epoch 62/100, step 43/62,  total step 3825/6200, training_loss = 0.2297\n",
      "epoch 62/100, step 44/62,  total step 3826/6200, training_loss = 0.2232\n",
      "epoch 62/100, step 45/62,  total step 3827/6200, training_loss = 0.1539\n",
      "epoch 62/100, step 46/62,  total step 3828/6200, training_loss = 0.1804\n",
      "epoch 62/100, step 47/62,  total step 3829/6200, training_loss = 0.2819\n",
      "epoch 62/100, step 48/62,  total step 3830/6200, training_loss = 0.1757\n",
      "epoch 62/100, step 49/62,  total step 3831/6200, training_loss = 0.1801\n",
      "epoch 62/100, step 50/62,  total step 3832/6200, training_loss = 0.2146\n",
      "epoch 62/100, step 51/62,  total step 3833/6200, training_loss = 0.2495\n",
      "epoch 62/100, step 52/62,  total step 3834/6200, training_loss = 0.2113\n",
      "epoch 62/100, step 53/62,  total step 3835/6200, training_loss = 0.1934\n",
      "epoch 62/100, step 54/62,  total step 3836/6200, training_loss = 0.1698\n",
      "epoch 62/100, step 55/62,  total step 3837/6200, training_loss = 0.1919\n",
      "epoch 62/100, step 56/62,  total step 3838/6200, training_loss = 0.2378\n",
      "epoch 62/100, step 57/62,  total step 3839/6200, training_loss = 0.2555\n",
      "epoch 62/100, step 58/62,  total step 3840/6200, training_loss = 0.3364\n",
      "epoch 62/100, step 59/62,  total step 3841/6200, training_loss = 0.1732\n",
      "epoch 62/100, step 60/62,  total step 3842/6200, training_loss = 0.2866\n",
      "epoch 62/100, step 61/62,  total step 3843/6200, training_loss = 0.1604\n",
      "epoch 62/100, step 62/62,  total step 3844/6200, training_loss = 0.2499 | avg loss: 0.2327 Dice Metric:   0.665\n",
      "epoch 63/100, step 1/62,  total step 3845/6200, training_loss = 0.2151\n",
      "epoch 63/100, step 2/62,  total step 3846/6200, training_loss = 0.2091\n",
      "epoch 63/100, step 3/62,  total step 3847/6200, training_loss = 0.2701\n",
      "epoch 63/100, step 4/62,  total step 3848/6200, training_loss = 0.3468\n",
      "epoch 63/100, step 5/62,  total step 3849/6200, training_loss = 0.1468\n",
      "epoch 63/100, step 6/62,  total step 3850/6200, training_loss = 0.2040\n",
      "epoch 63/100, step 7/62,  total step 3851/6200, training_loss = 0.1834\n",
      "epoch 63/100, step 8/62,  total step 3852/6200, training_loss = 0.2285\n",
      "epoch 63/100, step 9/62,  total step 3853/6200, training_loss = 0.1740\n",
      "epoch 63/100, step 10/62,  total step 3854/6200, training_loss = 0.2226\n",
      "epoch 63/100, step 11/62,  total step 3855/6200, training_loss = 0.1542\n",
      "epoch 63/100, step 12/62,  total step 3856/6200, training_loss = 0.1757\n",
      "epoch 63/100, step 13/62,  total step 3857/6200, training_loss = 0.2185\n",
      "epoch 63/100, step 14/62,  total step 3858/6200, training_loss = 0.2499\n",
      "epoch 63/100, step 15/62,  total step 3859/6200, training_loss = 0.3457\n",
      "epoch 63/100, step 16/62,  total step 3860/6200, training_loss = 0.3146\n",
      "epoch 63/100, step 17/62,  total step 3861/6200, training_loss = 0.1874\n",
      "epoch 63/100, step 18/62,  total step 3862/6200, training_loss = 0.1805\n",
      "epoch 63/100, step 19/62,  total step 3863/6200, training_loss = 0.3035\n",
      "epoch 63/100, step 20/62,  total step 3864/6200, training_loss = 0.2494\n",
      "epoch 63/100, step 21/62,  total step 3865/6200, training_loss = 0.2518\n",
      "epoch 63/100, step 22/62,  total step 3866/6200, training_loss = 0.3344\n",
      "epoch 63/100, step 23/62,  total step 3867/6200, training_loss = 0.2445\n",
      "epoch 63/100, step 24/62,  total step 3868/6200, training_loss = 0.1849\n",
      "epoch 63/100, step 25/62,  total step 3869/6200, training_loss = 0.2238\n",
      "epoch 63/100, step 26/62,  total step 3870/6200, training_loss = 0.1822\n",
      "epoch 63/100, step 27/62,  total step 3871/6200, training_loss = 0.2453\n",
      "epoch 63/100, step 28/62,  total step 3872/6200, training_loss = 0.2996\n",
      "epoch 63/100, step 29/62,  total step 3873/6200, training_loss = 0.2983\n",
      "epoch 63/100, step 30/62,  total step 3874/6200, training_loss = 0.2870\n",
      "epoch 63/100, step 31/62,  total step 3875/6200, training_loss = 0.1796\n",
      "epoch 63/100, step 32/62,  total step 3876/6200, training_loss = 0.2139\n",
      "epoch 63/100, step 33/62,  total step 3877/6200, training_loss = 0.3188\n",
      "epoch 63/100, step 34/62,  total step 3878/6200, training_loss = 0.2040\n",
      "epoch 63/100, step 35/62,  total step 3879/6200, training_loss = 0.2258\n",
      "epoch 63/100, step 36/62,  total step 3880/6200, training_loss = 0.2687\n",
      "epoch 63/100, step 37/62,  total step 3881/6200, training_loss = 0.2283\n",
      "epoch 63/100, step 38/62,  total step 3882/6200, training_loss = 0.2739\n",
      "epoch 63/100, step 39/62,  total step 3883/6200, training_loss = 0.2685\n",
      "epoch 63/100, step 40/62,  total step 3884/6200, training_loss = 0.2475\n",
      "epoch 63/100, step 41/62,  total step 3885/6200, training_loss = 0.1839\n",
      "epoch 63/100, step 42/62,  total step 3886/6200, training_loss = 0.2116\n",
      "epoch 63/100, step 43/62,  total step 3887/6200, training_loss = 0.2235\n",
      "epoch 63/100, step 44/62,  total step 3888/6200, training_loss = 0.2442\n",
      "epoch 63/100, step 45/62,  total step 3889/6200, training_loss = 0.1521\n",
      "epoch 63/100, step 46/62,  total step 3890/6200, training_loss = 0.1823\n",
      "epoch 63/100, step 47/62,  total step 3891/6200, training_loss = 0.2413\n",
      "epoch 63/100, step 48/62,  total step 3892/6200, training_loss = 0.1716\n",
      "epoch 63/100, step 49/62,  total step 3893/6200, training_loss = 0.1577\n",
      "epoch 63/100, step 50/62,  total step 3894/6200, training_loss = 0.2270\n",
      "epoch 63/100, step 51/62,  total step 3895/6200, training_loss = 0.2207\n",
      "epoch 63/100, step 52/62,  total step 3896/6200, training_loss = 0.1542\n",
      "epoch 63/100, step 53/62,  total step 3897/6200, training_loss = 0.1807\n",
      "epoch 63/100, step 54/62,  total step 3898/6200, training_loss = 0.1818\n",
      "epoch 63/100, step 55/62,  total step 3899/6200, training_loss = 0.1937\n",
      "epoch 63/100, step 56/62,  total step 3900/6200, training_loss = 0.2641\n",
      "epoch 63/100, step 57/62,  total step 3901/6200, training_loss = 0.2603\n",
      "epoch 63/100, step 58/62,  total step 3902/6200, training_loss = 0.3564\n",
      "epoch 63/100, step 59/62,  total step 3903/6200, training_loss = 0.1663\n",
      "epoch 63/100, step 60/62,  total step 3904/6200, training_loss = 0.2758\n",
      "epoch 63/100, step 61/62,  total step 3905/6200, training_loss = 0.1585\n",
      "epoch 63/100, step 62/62,  total step 3906/6200, training_loss = 0.2141 | avg loss: 0.2287 Dice Metric:   0.654\n",
      "epoch 64/100, step 1/62,  total step 3907/6200, training_loss = 0.2450\n",
      "epoch 64/100, step 2/62,  total step 3908/6200, training_loss = 0.1900\n",
      "epoch 64/100, step 3/62,  total step 3909/6200, training_loss = 0.2288\n",
      "epoch 64/100, step 4/62,  total step 3910/6200, training_loss = 0.3642\n",
      "epoch 64/100, step 5/62,  total step 3911/6200, training_loss = 0.1490\n",
      "epoch 64/100, step 6/62,  total step 3912/6200, training_loss = 0.1871\n",
      "epoch 64/100, step 7/62,  total step 3913/6200, training_loss = 0.1854\n",
      "epoch 64/100, step 8/62,  total step 3914/6200, training_loss = 0.2118\n",
      "epoch 64/100, step 9/62,  total step 3915/6200, training_loss = 0.2192\n",
      "epoch 64/100, step 10/62,  total step 3916/6200, training_loss = 0.2097\n",
      "epoch 64/100, step 11/62,  total step 3917/6200, training_loss = 0.1573\n",
      "epoch 64/100, step 12/62,  total step 3918/6200, training_loss = 0.1584\n",
      "epoch 64/100, step 13/62,  total step 3919/6200, training_loss = 0.2051\n",
      "epoch 64/100, step 14/62,  total step 3920/6200, training_loss = 0.2469\n",
      "epoch 64/100, step 15/62,  total step 3921/6200, training_loss = 0.3489\n",
      "epoch 64/100, step 16/62,  total step 3922/6200, training_loss = 0.3426\n",
      "epoch 64/100, step 17/62,  total step 3923/6200, training_loss = 0.2055\n",
      "epoch 64/100, step 18/62,  total step 3924/6200, training_loss = 0.2035\n",
      "epoch 64/100, step 19/62,  total step 3925/6200, training_loss = 0.2566\n",
      "epoch 64/100, step 20/62,  total step 3926/6200, training_loss = 0.2382\n",
      "epoch 64/100, step 21/62,  total step 3927/6200, training_loss = 0.2486\n",
      "epoch 64/100, step 22/62,  total step 3928/6200, training_loss = 0.3819\n",
      "epoch 64/100, step 23/62,  total step 3929/6200, training_loss = 0.2414\n",
      "epoch 64/100, step 24/62,  total step 3930/6200, training_loss = 0.1848\n",
      "epoch 64/100, step 25/62,  total step 3931/6200, training_loss = 0.2224\n",
      "epoch 64/100, step 26/62,  total step 3932/6200, training_loss = 0.1617\n",
      "epoch 64/100, step 27/62,  total step 3933/6200, training_loss = 0.2477\n",
      "epoch 64/100, step 28/62,  total step 3934/6200, training_loss = 0.3083\n",
      "epoch 64/100, step 29/62,  total step 3935/6200, training_loss = 0.3150\n",
      "epoch 64/100, step 30/62,  total step 3936/6200, training_loss = 0.2850\n",
      "epoch 64/100, step 31/62,  total step 3937/6200, training_loss = 0.1647\n",
      "epoch 64/100, step 32/62,  total step 3938/6200, training_loss = 0.2128\n",
      "epoch 64/100, step 33/62,  total step 3939/6200, training_loss = 0.3282\n",
      "epoch 64/100, step 34/62,  total step 3940/6200, training_loss = 0.2375\n",
      "epoch 64/100, step 35/62,  total step 3941/6200, training_loss = 0.2371\n",
      "epoch 64/100, step 36/62,  total step 3942/6200, training_loss = 0.2857\n",
      "epoch 64/100, step 37/62,  total step 3943/6200, training_loss = 0.2095\n",
      "epoch 64/100, step 38/62,  total step 3944/6200, training_loss = 0.2571\n",
      "epoch 64/100, step 39/62,  total step 3945/6200, training_loss = 0.2745\n",
      "epoch 64/100, step 40/62,  total step 3946/6200, training_loss = 0.2565\n",
      "epoch 64/100, step 41/62,  total step 3947/6200, training_loss = 0.1909\n",
      "epoch 64/100, step 42/62,  total step 3948/6200, training_loss = 0.2275\n",
      "epoch 64/100, step 43/62,  total step 3949/6200, training_loss = 0.2111\n",
      "epoch 64/100, step 44/62,  total step 3950/6200, training_loss = 0.2215\n",
      "epoch 64/100, step 45/62,  total step 3951/6200, training_loss = 0.1473\n",
      "epoch 64/100, step 46/62,  total step 3952/6200, training_loss = 0.1764\n",
      "epoch 64/100, step 47/62,  total step 3953/6200, training_loss = 0.2600\n",
      "epoch 64/100, step 48/62,  total step 3954/6200, training_loss = 0.1473\n",
      "epoch 64/100, step 49/62,  total step 3955/6200, training_loss = 0.1636\n",
      "epoch 64/100, step 50/62,  total step 3956/6200, training_loss = 0.2287\n",
      "epoch 64/100, step 51/62,  total step 3957/6200, training_loss = 0.2223\n",
      "epoch 64/100, step 52/62,  total step 3958/6200, training_loss = 0.1709\n",
      "epoch 64/100, step 53/62,  total step 3959/6200, training_loss = 0.2387\n",
      "epoch 64/100, step 54/62,  total step 3960/6200, training_loss = 0.1798\n",
      "epoch 64/100, step 55/62,  total step 3961/6200, training_loss = 0.2132\n",
      "epoch 64/100, step 56/62,  total step 3962/6200, training_loss = 0.2283\n",
      "epoch 64/100, step 57/62,  total step 3963/6200, training_loss = 0.2518\n",
      "epoch 64/100, step 58/62,  total step 3964/6200, training_loss = 0.3307\n",
      "epoch 64/100, step 59/62,  total step 3965/6200, training_loss = 0.1849\n",
      "epoch 64/100, step 60/62,  total step 3966/6200, training_loss = 0.2658\n",
      "epoch 64/100, step 61/62,  total step 3967/6200, training_loss = 0.1418\n",
      "epoch 64/100, step 62/62,  total step 3968/6200, training_loss = 0.2184 | avg loss: 0.2296 Dice Metric:   0.663\n",
      "epoch 65/100, step 1/62,  total step 3969/6200, training_loss = 0.2153\n",
      "epoch 65/100, step 2/62,  total step 3970/6200, training_loss = 0.1952\n",
      "epoch 65/100, step 3/62,  total step 3971/6200, training_loss = 0.2351\n",
      "epoch 65/100, step 4/62,  total step 3972/6200, training_loss = 0.3544\n",
      "epoch 65/100, step 5/62,  total step 3973/6200, training_loss = 0.1541\n",
      "epoch 65/100, step 6/62,  total step 3974/6200, training_loss = 0.1949\n",
      "epoch 65/100, step 7/62,  total step 3975/6200, training_loss = 0.1965\n",
      "epoch 65/100, step 8/62,  total step 3976/6200, training_loss = 0.2518\n",
      "epoch 65/100, step 9/62,  total step 3977/6200, training_loss = 0.1930\n",
      "epoch 65/100, step 10/62,  total step 3978/6200, training_loss = 0.2524\n",
      "epoch 65/100, step 11/62,  total step 3979/6200, training_loss = 0.1696\n",
      "epoch 65/100, step 12/62,  total step 3980/6200, training_loss = 0.1354\n",
      "epoch 65/100, step 13/62,  total step 3981/6200, training_loss = 0.2144\n",
      "epoch 65/100, step 14/62,  total step 3982/6200, training_loss = 0.2686\n",
      "epoch 65/100, step 15/62,  total step 3983/6200, training_loss = 0.3963\n",
      "epoch 65/100, step 16/62,  total step 3984/6200, training_loss = 0.3415\n",
      "epoch 65/100, step 17/62,  total step 3985/6200, training_loss = 0.2084\n",
      "epoch 65/100, step 18/62,  total step 3986/6200, training_loss = 0.2187\n",
      "epoch 65/100, step 19/62,  total step 3987/6200, training_loss = 0.2688\n",
      "epoch 65/100, step 20/62,  total step 3988/6200, training_loss = 0.2216\n",
      "epoch 65/100, step 21/62,  total step 3989/6200, training_loss = 0.2607\n",
      "epoch 65/100, step 22/62,  total step 3990/6200, training_loss = 0.3523\n",
      "epoch 65/100, step 23/62,  total step 3991/6200, training_loss = 0.2276\n",
      "epoch 65/100, step 24/62,  total step 3992/6200, training_loss = 0.1876\n",
      "epoch 65/100, step 25/62,  total step 3993/6200, training_loss = 0.2422\n",
      "epoch 65/100, step 26/62,  total step 3994/6200, training_loss = 0.1677\n",
      "epoch 65/100, step 27/62,  total step 3995/6200, training_loss = 0.2616\n",
      "epoch 65/100, step 28/62,  total step 3996/6200, training_loss = 0.3403\n",
      "epoch 65/100, step 29/62,  total step 3997/6200, training_loss = 0.2632\n",
      "epoch 65/100, step 30/62,  total step 3998/6200, training_loss = 0.2455\n",
      "epoch 65/100, step 31/62,  total step 3999/6200, training_loss = 0.1451\n",
      "epoch 65/100, step 32/62,  total step 4000/6200, training_loss = 0.2017\n",
      "epoch 65/100, step 33/62,  total step 4001/6200, training_loss = 0.2731\n",
      "epoch 65/100, step 34/62,  total step 4002/6200, training_loss = 0.2029\n",
      "epoch 65/100, step 35/62,  total step 4003/6200, training_loss = 0.2176\n",
      "epoch 65/100, step 36/62,  total step 4004/6200, training_loss = 0.3029\n",
      "epoch 65/100, step 37/62,  total step 4005/6200, training_loss = 0.2154\n",
      "epoch 65/100, step 38/62,  total step 4006/6200, training_loss = 0.2586\n",
      "epoch 65/100, step 39/62,  total step 4007/6200, training_loss = 0.2628\n",
      "epoch 65/100, step 40/62,  total step 4008/6200, training_loss = 0.2558\n",
      "epoch 65/100, step 41/62,  total step 4009/6200, training_loss = 0.1765\n",
      "epoch 65/100, step 42/62,  total step 4010/6200, training_loss = 0.2311\n",
      "epoch 65/100, step 43/62,  total step 4011/6200, training_loss = 0.2313\n",
      "epoch 65/100, step 44/62,  total step 4012/6200, training_loss = 0.2317\n",
      "epoch 65/100, step 45/62,  total step 4013/6200, training_loss = 0.1417\n",
      "epoch 65/100, step 46/62,  total step 4014/6200, training_loss = 0.2013\n",
      "epoch 65/100, step 47/62,  total step 4015/6200, training_loss = 0.2535\n",
      "epoch 65/100, step 48/62,  total step 4016/6200, training_loss = 0.1538\n",
      "epoch 65/100, step 49/62,  total step 4017/6200, training_loss = 0.2007\n",
      "epoch 65/100, step 50/62,  total step 4018/6200, training_loss = 0.2045\n",
      "epoch 65/100, step 51/62,  total step 4019/6200, training_loss = 0.2264\n",
      "epoch 65/100, step 52/62,  total step 4020/6200, training_loss = 0.2003\n",
      "epoch 65/100, step 53/62,  total step 4021/6200, training_loss = 0.1971\n",
      "epoch 65/100, step 54/62,  total step 4022/6200, training_loss = 0.1483\n",
      "epoch 65/100, step 55/62,  total step 4023/6200, training_loss = 0.1850\n",
      "epoch 65/100, step 56/62,  total step 4024/6200, training_loss = 0.2105\n",
      "epoch 65/100, step 57/62,  total step 4025/6200, training_loss = 0.2586\n",
      "epoch 65/100, step 58/62,  total step 4026/6200, training_loss = 0.3334\n",
      "epoch 65/100, step 59/62,  total step 4027/6200, training_loss = 0.1900\n",
      "epoch 65/100, step 60/62,  total step 4028/6200, training_loss = 0.2815\n",
      "epoch 65/100, step 61/62,  total step 4029/6200, training_loss = 0.1369\n",
      "epoch 65/100, step 62/62,  total step 4030/6200, training_loss = 0.1974 | avg loss: 0.2284 Dice Metric:   0.635\n",
      "epoch 66/100, step 1/62,  total step 4031/6200, training_loss = 0.2297\n",
      "epoch 66/100, step 2/62,  total step 4032/6200, training_loss = 0.1740\n",
      "epoch 66/100, step 3/62,  total step 4033/6200, training_loss = 0.2464\n",
      "epoch 66/100, step 4/62,  total step 4034/6200, training_loss = 0.3397\n",
      "epoch 66/100, step 5/62,  total step 4035/6200, training_loss = 0.1513\n",
      "epoch 66/100, step 6/62,  total step 4036/6200, training_loss = 0.2223\n",
      "epoch 66/100, step 7/62,  total step 4037/6200, training_loss = 0.1911\n",
      "epoch 66/100, step 8/62,  total step 4038/6200, training_loss = 0.2413\n",
      "epoch 66/100, step 9/62,  total step 4039/6200, training_loss = 0.1820\n",
      "epoch 66/100, step 10/62,  total step 4040/6200, training_loss = 0.2152\n",
      "epoch 66/100, step 11/62,  total step 4041/6200, training_loss = 0.1557\n",
      "epoch 66/100, step 12/62,  total step 4042/6200, training_loss = 0.1776\n",
      "epoch 66/100, step 13/62,  total step 4043/6200, training_loss = 0.2272\n",
      "epoch 66/100, step 14/62,  total step 4044/6200, training_loss = 0.2579\n",
      "epoch 66/100, step 15/62,  total step 4045/6200, training_loss = 0.3243\n",
      "epoch 66/100, step 16/62,  total step 4046/6200, training_loss = 0.3456\n",
      "epoch 66/100, step 17/62,  total step 4047/6200, training_loss = 0.1707\n",
      "epoch 66/100, step 18/62,  total step 4048/6200, training_loss = 0.2114\n",
      "epoch 66/100, step 19/62,  total step 4049/6200, training_loss = 0.3286\n",
      "epoch 66/100, step 20/62,  total step 4050/6200, training_loss = 0.2047\n",
      "epoch 66/100, step 21/62,  total step 4051/6200, training_loss = 0.2310\n",
      "epoch 66/100, step 22/62,  total step 4052/6200, training_loss = 0.3296\n",
      "epoch 66/100, step 23/62,  total step 4053/6200, training_loss = 0.2358\n",
      "epoch 66/100, step 24/62,  total step 4054/6200, training_loss = 0.1861\n",
      "epoch 66/100, step 25/62,  total step 4055/6200, training_loss = 0.2335\n",
      "epoch 66/100, step 26/62,  total step 4056/6200, training_loss = 0.1703\n",
      "epoch 66/100, step 27/62,  total step 4057/6200, training_loss = 0.2320\n",
      "epoch 66/100, step 28/62,  total step 4058/6200, training_loss = 0.2951\n",
      "epoch 66/100, step 29/62,  total step 4059/6200, training_loss = 0.2669\n",
      "epoch 66/100, step 30/62,  total step 4060/6200, training_loss = 0.2727\n",
      "epoch 66/100, step 31/62,  total step 4061/6200, training_loss = 0.1620\n",
      "epoch 66/100, step 32/62,  total step 4062/6200, training_loss = 0.2312\n",
      "epoch 66/100, step 33/62,  total step 4063/6200, training_loss = 0.2897\n",
      "epoch 66/100, step 34/62,  total step 4064/6200, training_loss = 0.1870\n",
      "epoch 66/100, step 35/62,  total step 4065/6200, training_loss = 0.2332\n",
      "epoch 66/100, step 36/62,  total step 4066/6200, training_loss = 0.2731\n",
      "epoch 66/100, step 37/62,  total step 4067/6200, training_loss = 0.2134\n",
      "epoch 66/100, step 38/62,  total step 4068/6200, training_loss = 0.2396\n",
      "epoch 66/100, step 39/62,  total step 4069/6200, training_loss = 0.2398\n",
      "epoch 66/100, step 40/62,  total step 4070/6200, training_loss = 0.2685\n",
      "epoch 66/100, step 41/62,  total step 4071/6200, training_loss = 0.1745\n",
      "epoch 66/100, step 42/62,  total step 4072/6200, training_loss = 0.2114\n",
      "epoch 66/100, step 43/62,  total step 4073/6200, training_loss = 0.2078\n",
      "epoch 66/100, step 44/62,  total step 4074/6200, training_loss = 0.2231\n",
      "epoch 66/100, step 45/62,  total step 4075/6200, training_loss = 0.1294\n",
      "epoch 66/100, step 46/62,  total step 4076/6200, training_loss = 0.1626\n",
      "epoch 66/100, step 47/62,  total step 4077/6200, training_loss = 0.2686\n",
      "epoch 66/100, step 48/62,  total step 4078/6200, training_loss = 0.1615\n",
      "epoch 66/100, step 49/62,  total step 4079/6200, training_loss = 0.1660\n",
      "epoch 66/100, step 50/62,  total step 4080/6200, training_loss = 0.2098\n",
      "epoch 66/100, step 51/62,  total step 4081/6200, training_loss = 0.2276\n",
      "epoch 66/100, step 52/62,  total step 4082/6200, training_loss = 0.1735\n",
      "epoch 66/100, step 53/62,  total step 4083/6200, training_loss = 0.2212\n",
      "epoch 66/100, step 54/62,  total step 4084/6200, training_loss = 0.1613\n",
      "epoch 66/100, step 55/62,  total step 4085/6200, training_loss = 0.1944\n",
      "epoch 66/100, step 56/62,  total step 4086/6200, training_loss = 0.2430\n",
      "epoch 66/100, step 57/62,  total step 4087/6200, training_loss = 0.2429\n",
      "epoch 66/100, step 58/62,  total step 4088/6200, training_loss = 0.3354\n",
      "epoch 66/100, step 59/62,  total step 4089/6200, training_loss = 0.2064\n",
      "epoch 66/100, step 60/62,  total step 4090/6200, training_loss = 0.3293\n",
      "epoch 66/100, step 61/62,  total step 4091/6200, training_loss = 0.1466\n",
      "epoch 66/100, step 62/62,  total step 4092/6200, training_loss = 0.1776 | avg loss: 0.2252 Dice Metric:   0.667\n",
      "epoch 67/100, step 1/62,  total step 4093/6200, training_loss = 0.2051\n",
      "epoch 67/100, step 2/62,  total step 4094/6200, training_loss = 0.2124\n",
      "epoch 67/100, step 3/62,  total step 4095/6200, training_loss = 0.2553\n",
      "epoch 67/100, step 4/62,  total step 4096/6200, training_loss = 0.3491\n",
      "epoch 67/100, step 5/62,  total step 4097/6200, training_loss = 0.1522\n",
      "epoch 67/100, step 6/62,  total step 4098/6200, training_loss = 0.2046\n",
      "epoch 67/100, step 7/62,  total step 4099/6200, training_loss = 0.1716\n",
      "epoch 67/100, step 8/62,  total step 4100/6200, training_loss = 0.2649\n",
      "epoch 67/100, step 9/62,  total step 4101/6200, training_loss = 0.1869\n",
      "epoch 67/100, step 10/62,  total step 4102/6200, training_loss = 0.2133\n",
      "epoch 67/100, step 11/62,  total step 4103/6200, training_loss = 0.1703\n",
      "epoch 67/100, step 12/62,  total step 4104/6200, training_loss = 0.2120\n",
      "epoch 67/100, step 13/62,  total step 4105/6200, training_loss = 0.2090\n",
      "epoch 67/100, step 14/62,  total step 4106/6200, training_loss = 0.2683\n",
      "epoch 67/100, step 15/62,  total step 4107/6200, training_loss = 0.3801\n",
      "epoch 67/100, step 16/62,  total step 4108/6200, training_loss = 0.3335\n",
      "epoch 67/100, step 17/62,  total step 4109/6200, training_loss = 0.1944\n",
      "epoch 67/100, step 18/62,  total step 4110/6200, training_loss = 0.2332\n",
      "epoch 67/100, step 19/62,  total step 4111/6200, training_loss = 0.3281\n",
      "epoch 67/100, step 20/62,  total step 4112/6200, training_loss = 0.2075\n",
      "epoch 67/100, step 21/62,  total step 4113/6200, training_loss = 0.2589\n",
      "epoch 67/100, step 22/62,  total step 4114/6200, training_loss = 0.3287\n",
      "epoch 67/100, step 23/62,  total step 4115/6200, training_loss = 0.2554\n",
      "epoch 67/100, step 24/62,  total step 4116/6200, training_loss = 0.1659\n",
      "epoch 67/100, step 25/62,  total step 4117/6200, training_loss = 0.2029\n",
      "epoch 67/100, step 26/62,  total step 4118/6200, training_loss = 0.1595\n",
      "epoch 67/100, step 27/62,  total step 4119/6200, training_loss = 0.2170\n",
      "epoch 67/100, step 28/62,  total step 4120/6200, training_loss = 0.3983\n",
      "epoch 67/100, step 29/62,  total step 4121/6200, training_loss = 0.2541\n",
      "epoch 67/100, step 30/62,  total step 4122/6200, training_loss = 0.2569\n",
      "epoch 67/100, step 31/62,  total step 4123/6200, training_loss = 0.1640\n",
      "epoch 67/100, step 32/62,  total step 4124/6200, training_loss = 0.2102\n",
      "epoch 67/100, step 33/62,  total step 4125/6200, training_loss = 0.3001\n",
      "epoch 67/100, step 34/62,  total step 4126/6200, training_loss = 0.1966\n",
      "epoch 67/100, step 35/62,  total step 4127/6200, training_loss = 0.2239\n",
      "epoch 67/100, step 36/62,  total step 4128/6200, training_loss = 0.2602\n",
      "epoch 67/100, step 37/62,  total step 4129/6200, training_loss = 0.2277\n",
      "epoch 67/100, step 38/62,  total step 4130/6200, training_loss = 0.2898\n",
      "epoch 67/100, step 39/62,  total step 4131/6200, training_loss = 0.2674\n",
      "epoch 67/100, step 40/62,  total step 4132/6200, training_loss = 0.2807\n",
      "epoch 67/100, step 41/62,  total step 4133/6200, training_loss = 0.1838\n",
      "epoch 67/100, step 42/62,  total step 4134/6200, training_loss = 0.2047\n",
      "epoch 67/100, step 43/62,  total step 4135/6200, training_loss = 0.2123\n",
      "epoch 67/100, step 44/62,  total step 4136/6200, training_loss = 0.2358\n",
      "epoch 67/100, step 45/62,  total step 4137/6200, training_loss = 0.1546\n",
      "epoch 67/100, step 46/62,  total step 4138/6200, training_loss = 0.1826\n",
      "epoch 67/100, step 47/62,  total step 4139/6200, training_loss = 0.2564\n",
      "epoch 67/100, step 48/62,  total step 4140/6200, training_loss = 0.1643\n",
      "epoch 67/100, step 49/62,  total step 4141/6200, training_loss = 0.1888\n",
      "epoch 67/100, step 50/62,  total step 4142/6200, training_loss = 0.2159\n",
      "epoch 67/100, step 51/62,  total step 4143/6200, training_loss = 0.2454\n",
      "epoch 67/100, step 52/62,  total step 4144/6200, training_loss = 0.1775\n",
      "epoch 67/100, step 53/62,  total step 4145/6200, training_loss = 0.2372\n",
      "epoch 67/100, step 54/62,  total step 4146/6200, training_loss = 0.1728\n",
      "epoch 67/100, step 55/62,  total step 4147/6200, training_loss = 0.1945\n",
      "epoch 67/100, step 56/62,  total step 4148/6200, training_loss = 0.2244\n",
      "epoch 67/100, step 57/62,  total step 4149/6200, training_loss = 0.2551\n",
      "epoch 67/100, step 58/62,  total step 4150/6200, training_loss = 0.3287\n",
      "epoch 67/100, step 59/62,  total step 4151/6200, training_loss = 0.1718\n",
      "epoch 67/100, step 60/62,  total step 4152/6200, training_loss = 0.3080\n",
      "epoch 67/100, step 61/62,  total step 4153/6200, training_loss = 0.1546\n",
      "epoch 67/100, step 62/62,  total step 4154/6200, training_loss = 0.4129 | avg loss: 0.2347 Dice Metric:   0.639\n",
      "epoch 68/100, step 1/62,  total step 4155/6200, training_loss = 0.2366\n",
      "epoch 68/100, step 2/62,  total step 4156/6200, training_loss = 0.2001\n",
      "epoch 68/100, step 3/62,  total step 4157/6200, training_loss = 0.2675\n",
      "epoch 68/100, step 4/62,  total step 4158/6200, training_loss = 0.3231\n",
      "epoch 68/100, step 5/62,  total step 4159/6200, training_loss = 0.1505\n",
      "epoch 68/100, step 6/62,  total step 4160/6200, training_loss = 0.2120\n",
      "epoch 68/100, step 7/62,  total step 4161/6200, training_loss = 0.1767\n",
      "epoch 68/100, step 8/62,  total step 4162/6200, training_loss = 0.2203\n",
      "epoch 68/100, step 9/62,  total step 4163/6200, training_loss = 0.1959\n",
      "epoch 68/100, step 10/62,  total step 4164/6200, training_loss = 0.1942\n",
      "epoch 68/100, step 11/62,  total step 4165/6200, training_loss = 0.1686\n",
      "epoch 68/100, step 12/62,  total step 4166/6200, training_loss = 0.1695\n",
      "epoch 68/100, step 13/62,  total step 4167/6200, training_loss = 0.2169\n",
      "epoch 68/100, step 14/62,  total step 4168/6200, training_loss = 0.2554\n",
      "epoch 68/100, step 15/62,  total step 4169/6200, training_loss = 0.3317\n",
      "epoch 68/100, step 16/62,  total step 4170/6200, training_loss = 0.2974\n",
      "epoch 68/100, step 17/62,  total step 4171/6200, training_loss = 0.2088\n",
      "epoch 68/100, step 18/62,  total step 4172/6200, training_loss = 0.2078\n",
      "epoch 68/100, step 19/62,  total step 4173/6200, training_loss = 0.2735\n",
      "epoch 68/100, step 20/62,  total step 4174/6200, training_loss = 0.2303\n",
      "epoch 68/100, step 21/62,  total step 4175/6200, training_loss = 0.2449\n",
      "epoch 68/100, step 22/62,  total step 4176/6200, training_loss = 0.3333\n",
      "epoch 68/100, step 23/62,  total step 4177/6200, training_loss = 0.2166\n",
      "epoch 68/100, step 24/62,  total step 4178/6200, training_loss = 0.1868\n",
      "epoch 68/100, step 25/62,  total step 4179/6200, training_loss = 0.2258\n",
      "epoch 68/100, step 26/62,  total step 4180/6200, training_loss = 0.1561\n",
      "epoch 68/100, step 27/62,  total step 4181/6200, training_loss = 0.2540\n",
      "epoch 68/100, step 28/62,  total step 4182/6200, training_loss = 0.2933\n",
      "epoch 68/100, step 29/62,  total step 4183/6200, training_loss = 0.2548\n",
      "epoch 68/100, step 30/62,  total step 4184/6200, training_loss = 0.2793\n",
      "epoch 68/100, step 31/62,  total step 4185/6200, training_loss = 0.1472\n",
      "epoch 68/100, step 32/62,  total step 4186/6200, training_loss = 0.2077\n",
      "epoch 68/100, step 33/62,  total step 4187/6200, training_loss = 0.3095\n",
      "epoch 68/100, step 34/62,  total step 4188/6200, training_loss = 0.2262\n",
      "epoch 68/100, step 35/62,  total step 4189/6200, training_loss = 0.2658\n",
      "epoch 68/100, step 36/62,  total step 4190/6200, training_loss = 0.3055\n",
      "epoch 68/100, step 37/62,  total step 4191/6200, training_loss = 0.2172\n",
      "epoch 68/100, step 38/62,  total step 4192/6200, training_loss = 0.2700\n",
      "epoch 68/100, step 39/62,  total step 4193/6200, training_loss = 0.2678\n",
      "epoch 68/100, step 40/62,  total step 4194/6200, training_loss = 0.2609\n",
      "epoch 68/100, step 41/62,  total step 4195/6200, training_loss = 0.1861\n",
      "epoch 68/100, step 42/62,  total step 4196/6200, training_loss = 0.2482\n",
      "epoch 68/100, step 43/62,  total step 4197/6200, training_loss = 0.1975\n",
      "epoch 68/100, step 44/62,  total step 4198/6200, training_loss = 0.2239\n",
      "epoch 68/100, step 45/62,  total step 4199/6200, training_loss = 0.1376\n",
      "epoch 68/100, step 46/62,  total step 4200/6200, training_loss = 0.1562\n",
      "epoch 68/100, step 47/62,  total step 4201/6200, training_loss = 0.2553\n",
      "epoch 68/100, step 48/62,  total step 4202/6200, training_loss = 0.1545\n",
      "epoch 68/100, step 49/62,  total step 4203/6200, training_loss = 0.1690\n",
      "epoch 68/100, step 50/62,  total step 4204/6200, training_loss = 0.2227\n",
      "epoch 68/100, step 51/62,  total step 4205/6200, training_loss = 0.2090\n",
      "epoch 68/100, step 52/62,  total step 4206/6200, training_loss = 0.1598\n",
      "epoch 68/100, step 53/62,  total step 4207/6200, training_loss = 0.1961\n",
      "epoch 68/100, step 54/62,  total step 4208/6200, training_loss = 0.2062\n",
      "epoch 68/100, step 55/62,  total step 4209/6200, training_loss = 0.1771\n",
      "epoch 68/100, step 56/62,  total step 4210/6200, training_loss = 0.2436\n",
      "epoch 68/100, step 57/62,  total step 4211/6200, training_loss = 0.2202\n",
      "epoch 68/100, step 58/62,  total step 4212/6200, training_loss = 0.3507\n",
      "epoch 68/100, step 59/62,  total step 4213/6200, training_loss = 0.1921\n",
      "epoch 68/100, step 60/62,  total step 4214/6200, training_loss = 0.2914\n",
      "epoch 68/100, step 61/62,  total step 4215/6200, training_loss = 0.1532\n",
      "epoch 68/100, step 62/62,  total step 4216/6200, training_loss = 0.2813 | avg loss: 0.2273 Dice Metric:   0.658\n",
      "epoch 69/100, step 1/62,  total step 4217/6200, training_loss = 0.2154\n",
      "epoch 69/100, step 2/62,  total step 4218/6200, training_loss = 0.2133\n",
      "epoch 69/100, step 3/62,  total step 4219/6200, training_loss = 0.2510\n",
      "epoch 69/100, step 4/62,  total step 4220/6200, training_loss = 0.3458\n",
      "epoch 69/100, step 5/62,  total step 4221/6200, training_loss = 0.1575\n",
      "epoch 69/100, step 6/62,  total step 4222/6200, training_loss = 0.2232\n",
      "epoch 69/100, step 7/62,  total step 4223/6200, training_loss = 0.1971\n",
      "epoch 69/100, step 8/62,  total step 4224/6200, training_loss = 0.2525\n",
      "epoch 69/100, step 9/62,  total step 4225/6200, training_loss = 0.1656\n",
      "epoch 69/100, step 10/62,  total step 4226/6200, training_loss = 0.2130\n",
      "epoch 69/100, step 11/62,  total step 4227/6200, training_loss = 0.1536\n",
      "epoch 69/100, step 12/62,  total step 4228/6200, training_loss = 0.1675\n",
      "epoch 69/100, step 13/62,  total step 4229/6200, training_loss = 0.1954\n",
      "epoch 69/100, step 14/62,  total step 4230/6200, training_loss = 0.2230\n",
      "epoch 69/100, step 15/62,  total step 4231/6200, training_loss = 0.3523\n",
      "epoch 69/100, step 16/62,  total step 4232/6200, training_loss = 0.3577\n",
      "epoch 69/100, step 17/62,  total step 4233/6200, training_loss = 0.1784\n",
      "epoch 69/100, step 18/62,  total step 4234/6200, training_loss = 0.2139\n",
      "epoch 69/100, step 19/62,  total step 4235/6200, training_loss = 0.3267\n",
      "epoch 69/100, step 20/62,  total step 4236/6200, training_loss = 0.2321\n",
      "epoch 69/100, step 21/62,  total step 4237/6200, training_loss = 0.2404\n",
      "epoch 69/100, step 22/62,  total step 4238/6200, training_loss = 0.3517\n",
      "epoch 69/100, step 23/62,  total step 4239/6200, training_loss = 0.2504\n",
      "epoch 69/100, step 24/62,  total step 4240/6200, training_loss = 0.1908\n",
      "epoch 69/100, step 25/62,  total step 4241/6200, training_loss = 0.2157\n",
      "epoch 69/100, step 26/62,  total step 4242/6200, training_loss = 0.1652\n",
      "epoch 69/100, step 27/62,  total step 4243/6200, training_loss = 0.2248\n",
      "epoch 69/100, step 28/62,  total step 4244/6200, training_loss = 0.3601\n",
      "epoch 69/100, step 29/62,  total step 4245/6200, training_loss = 0.2618\n",
      "epoch 69/100, step 30/62,  total step 4246/6200, training_loss = 0.2683\n",
      "epoch 69/100, step 31/62,  total step 4247/6200, training_loss = 0.1692\n",
      "epoch 69/100, step 32/62,  total step 4248/6200, training_loss = 0.2030\n",
      "epoch 69/100, step 33/62,  total step 4249/6200, training_loss = 0.2636\n",
      "epoch 69/100, step 34/62,  total step 4250/6200, training_loss = 0.2043\n",
      "epoch 69/100, step 35/62,  total step 4251/6200, training_loss = 0.2396\n",
      "epoch 69/100, step 36/62,  total step 4252/6200, training_loss = 0.2755\n",
      "epoch 69/100, step 37/62,  total step 4253/6200, training_loss = 0.2103\n",
      "epoch 69/100, step 38/62,  total step 4254/6200, training_loss = 0.2410\n",
      "epoch 69/100, step 39/62,  total step 4255/6200, training_loss = 0.2853\n",
      "epoch 69/100, step 40/62,  total step 4256/6200, training_loss = 0.2739\n",
      "epoch 69/100, step 41/62,  total step 4257/6200, training_loss = 0.2133\n",
      "epoch 69/100, step 42/62,  total step 4258/6200, training_loss = 0.2173\n",
      "epoch 69/100, step 43/62,  total step 4259/6200, training_loss = 0.1959\n",
      "epoch 69/100, step 44/62,  total step 4260/6200, training_loss = 0.2222\n",
      "epoch 69/100, step 45/62,  total step 4261/6200, training_loss = 0.1442\n",
      "epoch 69/100, step 46/62,  total step 4262/6200, training_loss = 0.1778\n",
      "epoch 69/100, step 47/62,  total step 4263/6200, training_loss = 0.2352\n",
      "epoch 69/100, step 48/62,  total step 4264/6200, training_loss = 0.1648\n",
      "epoch 69/100, step 49/62,  total step 4265/6200, training_loss = 0.1672\n",
      "epoch 69/100, step 50/62,  total step 4266/6200, training_loss = 0.2286\n",
      "epoch 69/100, step 51/62,  total step 4267/6200, training_loss = 0.2048\n",
      "epoch 69/100, step 52/62,  total step 4268/6200, training_loss = 0.1792\n",
      "epoch 69/100, step 53/62,  total step 4269/6200, training_loss = 0.2349\n",
      "epoch 69/100, step 54/62,  total step 4270/6200, training_loss = 0.1809\n",
      "epoch 69/100, step 55/62,  total step 4271/6200, training_loss = 0.1960\n",
      "epoch 69/100, step 56/62,  total step 4272/6200, training_loss = 0.2286\n",
      "epoch 69/100, step 57/62,  total step 4273/6200, training_loss = 0.2539\n",
      "epoch 69/100, step 58/62,  total step 4274/6200, training_loss = 0.3178\n",
      "epoch 69/100, step 59/62,  total step 4275/6200, training_loss = 0.2338\n",
      "epoch 69/100, step 60/62,  total step 4276/6200, training_loss = 0.2985\n",
      "epoch 69/100, step 61/62,  total step 4277/6200, training_loss = 0.1479\n",
      "epoch 69/100, step 62/62,  total step 4278/6200, training_loss = 0.3052 | avg loss: 0.2303 Dice Metric:   0.652\n",
      "epoch 70/100, step 1/62,  total step 4279/6200, training_loss = 0.2092\n",
      "epoch 70/100, step 2/62,  total step 4280/6200, training_loss = 0.1747\n",
      "epoch 70/100, step 3/62,  total step 4281/6200, training_loss = 0.2353\n",
      "epoch 70/100, step 4/62,  total step 4282/6200, training_loss = 0.3564\n",
      "epoch 70/100, step 5/62,  total step 4283/6200, training_loss = 0.1410\n",
      "epoch 70/100, step 6/62,  total step 4284/6200, training_loss = 0.2108\n",
      "epoch 70/100, step 7/62,  total step 4285/6200, training_loss = 0.1848\n",
      "epoch 70/100, step 8/62,  total step 4286/6200, training_loss = 0.2244\n",
      "epoch 70/100, step 9/62,  total step 4287/6200, training_loss = 0.1797\n",
      "epoch 70/100, step 10/62,  total step 4288/6200, training_loss = 0.2065\n",
      "epoch 70/100, step 11/62,  total step 4289/6200, training_loss = 0.1550\n",
      "epoch 70/100, step 12/62,  total step 4290/6200, training_loss = 0.1737\n",
      "epoch 70/100, step 13/62,  total step 4291/6200, training_loss = 0.2429\n",
      "epoch 70/100, step 14/62,  total step 4292/6200, training_loss = 0.2379\n",
      "epoch 70/100, step 15/62,  total step 4293/6200, training_loss = 0.3488\n",
      "epoch 70/100, step 16/62,  total step 4294/6200, training_loss = 0.3304\n",
      "epoch 70/100, step 17/62,  total step 4295/6200, training_loss = 0.1730\n",
      "epoch 70/100, step 18/62,  total step 4296/6200, training_loss = 0.1935\n",
      "epoch 70/100, step 19/62,  total step 4297/6200, training_loss = 0.2682\n",
      "epoch 70/100, step 20/62,  total step 4298/6200, training_loss = 0.1943\n",
      "epoch 70/100, step 21/62,  total step 4299/6200, training_loss = 0.2543\n",
      "epoch 70/100, step 22/62,  total step 4300/6200, training_loss = 0.4026\n",
      "epoch 70/100, step 23/62,  total step 4301/6200, training_loss = 0.2553\n",
      "epoch 70/100, step 24/62,  total step 4302/6200, training_loss = 0.1821\n",
      "epoch 70/100, step 25/62,  total step 4303/6200, training_loss = 0.2083\n",
      "epoch 70/100, step 26/62,  total step 4304/6200, training_loss = 0.1637\n",
      "epoch 70/100, step 27/62,  total step 4305/6200, training_loss = 0.2610\n",
      "epoch 70/100, step 28/62,  total step 4306/6200, training_loss = 0.3613\n",
      "epoch 70/100, step 29/62,  total step 4307/6200, training_loss = 0.2590\n",
      "epoch 70/100, step 30/62,  total step 4308/6200, training_loss = 0.2542\n",
      "epoch 70/100, step 31/62,  total step 4309/6200, training_loss = 0.1452\n",
      "epoch 70/100, step 32/62,  total step 4310/6200, training_loss = 0.2456\n",
      "epoch 70/100, step 33/62,  total step 4311/6200, training_loss = 0.2888\n",
      "epoch 70/100, step 34/62,  total step 4312/6200, training_loss = 0.1991\n",
      "epoch 70/100, step 35/62,  total step 4313/6200, training_loss = 0.2223\n",
      "epoch 70/100, step 36/62,  total step 4314/6200, training_loss = 0.2942\n",
      "epoch 70/100, step 37/62,  total step 4315/6200, training_loss = 0.2062\n",
      "epoch 70/100, step 38/62,  total step 4316/6200, training_loss = 0.2392\n",
      "epoch 70/100, step 39/62,  total step 4317/6200, training_loss = 0.2295\n",
      "epoch 70/100, step 40/62,  total step 4318/6200, training_loss = 0.2558\n",
      "epoch 70/100, step 41/62,  total step 4319/6200, training_loss = 0.1892\n",
      "epoch 70/100, step 42/62,  total step 4320/6200, training_loss = 0.2267\n",
      "epoch 70/100, step 43/62,  total step 4321/6200, training_loss = 0.2117\n",
      "epoch 70/100, step 44/62,  total step 4322/6200, training_loss = 0.2269\n",
      "epoch 70/100, step 45/62,  total step 4323/6200, training_loss = 0.1365\n",
      "epoch 70/100, step 46/62,  total step 4324/6200, training_loss = 0.1625\n",
      "epoch 70/100, step 47/62,  total step 4325/6200, training_loss = 0.2736\n",
      "epoch 70/100, step 48/62,  total step 4326/6200, training_loss = 0.1728\n",
      "epoch 70/100, step 49/62,  total step 4327/6200, training_loss = 0.1620\n",
      "epoch 70/100, step 50/62,  total step 4328/6200, training_loss = 0.2027\n",
      "epoch 70/100, step 51/62,  total step 4329/6200, training_loss = 0.2109\n",
      "epoch 70/100, step 52/62,  total step 4330/6200, training_loss = 0.1615\n",
      "epoch 70/100, step 53/62,  total step 4331/6200, training_loss = 0.2145\n",
      "epoch 70/100, step 54/62,  total step 4332/6200, training_loss = 0.1777\n",
      "epoch 70/100, step 55/62,  total step 4333/6200, training_loss = 0.1734\n",
      "epoch 70/100, step 56/62,  total step 4334/6200, training_loss = 0.2663\n",
      "epoch 70/100, step 57/62,  total step 4335/6200, training_loss = 0.2221\n",
      "epoch 70/100, step 58/62,  total step 4336/6200, training_loss = 0.3121\n",
      "epoch 70/100, step 59/62,  total step 4337/6200, training_loss = 0.1874\n",
      "epoch 70/100, step 60/62,  total step 4338/6200, training_loss = 0.2862\n",
      "epoch 70/100, step 61/62,  total step 4339/6200, training_loss = 0.1536\n",
      "epoch 70/100, step 62/62,  total step 4340/6200, training_loss = 0.1510 | avg loss: 0.2234 Dice Metric:   0.648\n",
      "epoch 71/100, step 1/62,  total step 4341/6200, training_loss = 0.2138\n",
      "epoch 71/100, step 2/62,  total step 4342/6200, training_loss = 0.1886\n",
      "epoch 71/100, step 3/62,  total step 4343/6200, training_loss = 0.2606\n",
      "epoch 71/100, step 4/62,  total step 4344/6200, training_loss = 0.3842\n",
      "epoch 71/100, step 5/62,  total step 4345/6200, training_loss = 0.1585\n",
      "epoch 71/100, step 6/62,  total step 4346/6200, training_loss = 0.2074\n",
      "epoch 71/100, step 7/62,  total step 4347/6200, training_loss = 0.1918\n",
      "epoch 71/100, step 8/62,  total step 4348/6200, training_loss = 0.2746\n",
      "epoch 71/100, step 9/62,  total step 4349/6200, training_loss = 0.1810\n",
      "epoch 71/100, step 10/62,  total step 4350/6200, training_loss = 0.1984\n",
      "epoch 71/100, step 11/62,  total step 4351/6200, training_loss = 0.1833\n",
      "epoch 71/100, step 12/62,  total step 4352/6200, training_loss = 0.1791\n",
      "epoch 71/100, step 13/62,  total step 4353/6200, training_loss = 0.1942\n",
      "epoch 71/100, step 14/62,  total step 4354/6200, training_loss = 0.2722\n",
      "epoch 71/100, step 15/62,  total step 4355/6200, training_loss = 0.3416\n",
      "epoch 71/100, step 16/62,  total step 4356/6200, training_loss = 0.2673\n",
      "epoch 71/100, step 17/62,  total step 4357/6200, training_loss = 0.1972\n",
      "epoch 71/100, step 18/62,  total step 4358/6200, training_loss = 0.1989\n",
      "epoch 71/100, step 19/62,  total step 4359/6200, training_loss = 0.3008\n",
      "epoch 71/100, step 20/62,  total step 4360/6200, training_loss = 0.2429\n",
      "epoch 71/100, step 21/62,  total step 4361/6200, training_loss = 0.2640\n",
      "epoch 71/100, step 22/62,  total step 4362/6200, training_loss = 0.4020\n",
      "epoch 71/100, step 23/62,  total step 4363/6200, training_loss = 0.2504\n",
      "epoch 71/100, step 24/62,  total step 4364/6200, training_loss = 0.1788\n",
      "epoch 71/100, step 25/62,  total step 4365/6200, training_loss = 0.2127\n",
      "epoch 71/100, step 26/62,  total step 4366/6200, training_loss = 0.1551\n",
      "epoch 71/100, step 27/62,  total step 4367/6200, training_loss = 0.2424\n",
      "epoch 71/100, step 28/62,  total step 4368/6200, training_loss = 0.3458\n",
      "epoch 71/100, step 29/62,  total step 4369/6200, training_loss = 0.2503\n",
      "epoch 71/100, step 30/62,  total step 4370/6200, training_loss = 0.2300\n",
      "epoch 71/100, step 31/62,  total step 4371/6200, training_loss = 0.1517\n",
      "epoch 71/100, step 32/62,  total step 4372/6200, training_loss = 0.1928\n",
      "epoch 71/100, step 33/62,  total step 4373/6200, training_loss = 0.2559\n",
      "epoch 71/100, step 34/62,  total step 4374/6200, training_loss = 0.1889\n",
      "epoch 71/100, step 35/62,  total step 4375/6200, training_loss = 0.2329\n",
      "epoch 71/100, step 36/62,  total step 4376/6200, training_loss = 0.2921\n",
      "epoch 71/100, step 37/62,  total step 4377/6200, training_loss = 0.2206\n",
      "epoch 71/100, step 38/62,  total step 4378/6200, training_loss = 0.2331\n",
      "epoch 71/100, step 39/62,  total step 4379/6200, training_loss = 0.2336\n",
      "epoch 71/100, step 40/62,  total step 4380/6200, training_loss = 0.2679\n",
      "epoch 71/100, step 41/62,  total step 4381/6200, training_loss = 0.2087\n",
      "epoch 71/100, step 42/62,  total step 4382/6200, training_loss = 0.1956\n",
      "epoch 71/100, step 43/62,  total step 4383/6200, training_loss = 0.2036\n",
      "epoch 71/100, step 44/62,  total step 4384/6200, training_loss = 0.2313\n",
      "epoch 71/100, step 45/62,  total step 4385/6200, training_loss = 0.1547\n",
      "epoch 71/100, step 46/62,  total step 4386/6200, training_loss = 0.1764\n",
      "epoch 71/100, step 47/62,  total step 4387/6200, training_loss = 0.2617\n",
      "epoch 71/100, step 48/62,  total step 4388/6200, training_loss = 0.1834\n",
      "epoch 71/100, step 49/62,  total step 4389/6200, training_loss = 0.1625\n",
      "epoch 71/100, step 50/62,  total step 4390/6200, training_loss = 0.1991\n",
      "epoch 71/100, step 51/62,  total step 4391/6200, training_loss = 0.1984\n",
      "epoch 71/100, step 52/62,  total step 4392/6200, training_loss = 0.1854\n",
      "epoch 71/100, step 53/62,  total step 4393/6200, training_loss = 0.2027\n",
      "epoch 71/100, step 54/62,  total step 4394/6200, training_loss = 0.1551\n",
      "epoch 71/100, step 55/62,  total step 4395/6200, training_loss = 0.1943\n",
      "epoch 71/100, step 56/62,  total step 4396/6200, training_loss = 0.2510\n",
      "epoch 71/100, step 57/62,  total step 4397/6200, training_loss = 0.2151\n",
      "epoch 71/100, step 58/62,  total step 4398/6200, training_loss = 0.3093\n",
      "epoch 71/100, step 59/62,  total step 4399/6200, training_loss = 0.1854\n",
      "epoch 71/100, step 60/62,  total step 4400/6200, training_loss = 0.2718\n",
      "epoch 71/100, step 61/62,  total step 4401/6200, training_loss = 0.1459\n",
      "epoch 71/100, step 62/62,  total step 4402/6200, training_loss = 0.1887 | avg loss: 0.2245 Dice Metric:    0.66\n",
      "epoch 72/100, step 1/62,  total step 4403/6200, training_loss = 0.2013\n",
      "epoch 72/100, step 2/62,  total step 4404/6200, training_loss = 0.2019\n",
      "epoch 72/100, step 3/62,  total step 4405/6200, training_loss = 0.2432\n",
      "epoch 72/100, step 4/62,  total step 4406/6200, training_loss = 0.3605\n",
      "epoch 72/100, step 5/62,  total step 4407/6200, training_loss = 0.1603\n",
      "epoch 72/100, step 6/62,  total step 4408/6200, training_loss = 0.2444\n",
      "epoch 72/100, step 7/62,  total step 4409/6200, training_loss = 0.1834\n",
      "epoch 72/100, step 8/62,  total step 4410/6200, training_loss = 0.2166\n",
      "epoch 72/100, step 9/62,  total step 4411/6200, training_loss = 0.1755\n",
      "epoch 72/100, step 10/62,  total step 4412/6200, training_loss = 0.2158\n",
      "epoch 72/100, step 11/62,  total step 4413/6200, training_loss = 0.1786\n",
      "epoch 72/100, step 12/62,  total step 4414/6200, training_loss = 0.1514\n",
      "epoch 72/100, step 13/62,  total step 4415/6200, training_loss = 0.2006\n",
      "epoch 72/100, step 14/62,  total step 4416/6200, training_loss = 0.2458\n",
      "epoch 72/100, step 15/62,  total step 4417/6200, training_loss = 0.3368\n",
      "epoch 72/100, step 16/62,  total step 4418/6200, training_loss = 0.3243\n",
      "epoch 72/100, step 17/62,  total step 4419/6200, training_loss = 0.1819\n",
      "epoch 72/100, step 18/62,  total step 4420/6200, training_loss = 0.2236\n",
      "epoch 72/100, step 19/62,  total step 4421/6200, training_loss = 0.2719\n",
      "epoch 72/100, step 20/62,  total step 4422/6200, training_loss = 0.2222\n",
      "epoch 72/100, step 21/62,  total step 4423/6200, training_loss = 0.2456\n",
      "epoch 72/100, step 22/62,  total step 4424/6200, training_loss = 0.3802\n",
      "epoch 72/100, step 23/62,  total step 4425/6200, training_loss = 0.2332\n",
      "epoch 72/100, step 24/62,  total step 4426/6200, training_loss = 0.1693\n",
      "epoch 72/100, step 25/62,  total step 4427/6200, training_loss = 0.2751\n",
      "epoch 72/100, step 26/62,  total step 4428/6200, training_loss = 0.1983\n",
      "epoch 72/100, step 27/62,  total step 4429/6200, training_loss = 0.2243\n",
      "epoch 72/100, step 28/62,  total step 4430/6200, training_loss = 0.3528\n",
      "epoch 72/100, step 29/62,  total step 4431/6200, training_loss = 0.2742\n",
      "epoch 72/100, step 30/62,  total step 4432/6200, training_loss = 0.3046\n",
      "epoch 72/100, step 31/62,  total step 4433/6200, training_loss = 0.1641\n",
      "epoch 72/100, step 32/62,  total step 4434/6200, training_loss = 0.2154\n",
      "epoch 72/100, step 33/62,  total step 4435/6200, training_loss = 0.2919\n",
      "epoch 72/100, step 34/62,  total step 4436/6200, training_loss = 0.2240\n",
      "epoch 72/100, step 35/62,  total step 4437/6200, training_loss = 0.2676\n",
      "epoch 72/100, step 36/62,  total step 4438/6200, training_loss = 0.2908\n",
      "epoch 72/100, step 37/62,  total step 4439/6200, training_loss = 0.2224\n",
      "epoch 72/100, step 38/62,  total step 4440/6200, training_loss = 0.2790\n",
      "epoch 72/100, step 39/62,  total step 4441/6200, training_loss = 0.3006\n",
      "epoch 72/100, step 40/62,  total step 4442/6200, training_loss = 0.2486\n",
      "epoch 72/100, step 41/62,  total step 4443/6200, training_loss = 0.2007\n",
      "epoch 72/100, step 42/62,  total step 4444/6200, training_loss = 0.2285\n",
      "epoch 72/100, step 43/62,  total step 4445/6200, training_loss = 0.2157\n",
      "epoch 72/100, step 44/62,  total step 4446/6200, training_loss = 0.2331\n",
      "epoch 72/100, step 45/62,  total step 4447/6200, training_loss = 0.1495\n",
      "epoch 72/100, step 46/62,  total step 4448/6200, training_loss = 0.1670\n",
      "epoch 72/100, step 47/62,  total step 4449/6200, training_loss = 0.2523\n",
      "epoch 72/100, step 48/62,  total step 4450/6200, training_loss = 0.1513\n",
      "epoch 72/100, step 49/62,  total step 4451/6200, training_loss = 0.1609\n",
      "epoch 72/100, step 50/62,  total step 4452/6200, training_loss = 0.2212\n",
      "epoch 72/100, step 51/62,  total step 4453/6200, training_loss = 0.2261\n",
      "epoch 72/100, step 52/62,  total step 4454/6200, training_loss = 0.1798\n",
      "epoch 72/100, step 53/62,  total step 4455/6200, training_loss = 0.2184\n",
      "epoch 72/100, step 54/62,  total step 4456/6200, training_loss = 0.1653\n",
      "epoch 72/100, step 55/62,  total step 4457/6200, training_loss = 0.1937\n",
      "epoch 72/100, step 56/62,  total step 4458/6200, training_loss = 0.2439\n",
      "epoch 72/100, step 57/62,  total step 4459/6200, training_loss = 0.2664\n",
      "epoch 72/100, step 58/62,  total step 4460/6200, training_loss = 0.3261\n",
      "epoch 72/100, step 59/62,  total step 4461/6200, training_loss = 0.1849\n",
      "epoch 72/100, step 60/62,  total step 4462/6200, training_loss = 0.3004\n",
      "epoch 72/100, step 61/62,  total step 4463/6200, training_loss = 0.1509\n",
      "epoch 72/100, step 62/62,  total step 4464/6200, training_loss = 0.2302 | avg loss: 0.2317 Dice Metric:   0.644\n",
      "epoch 73/100, step 1/62,  total step 4465/6200, training_loss = 0.2261\n",
      "epoch 73/100, step 2/62,  total step 4466/6200, training_loss = 0.2124\n",
      "epoch 73/100, step 3/62,  total step 4467/6200, training_loss = 0.2401\n",
      "epoch 73/100, step 4/62,  total step 4468/6200, training_loss = 0.3736\n",
      "epoch 73/100, step 5/62,  total step 4469/6200, training_loss = 0.1456\n",
      "epoch 73/100, step 6/62,  total step 4470/6200, training_loss = 0.2113\n",
      "epoch 73/100, step 7/62,  total step 4471/6200, training_loss = 0.1974\n",
      "epoch 73/100, step 8/62,  total step 4472/6200, training_loss = 0.2616\n",
      "epoch 73/100, step 9/62,  total step 4473/6200, training_loss = 0.1988\n",
      "epoch 73/100, step 10/62,  total step 4474/6200, training_loss = 0.2448\n",
      "epoch 73/100, step 11/62,  total step 4475/6200, training_loss = 0.1532\n",
      "epoch 73/100, step 12/62,  total step 4476/6200, training_loss = 0.1667\n",
      "epoch 73/100, step 13/62,  total step 4477/6200, training_loss = 0.2251\n",
      "epoch 73/100, step 14/62,  total step 4478/6200, training_loss = 0.2491\n",
      "epoch 73/100, step 15/62,  total step 4479/6200, training_loss = 0.3521\n",
      "epoch 73/100, step 16/62,  total step 4480/6200, training_loss = 0.3142\n",
      "epoch 73/100, step 17/62,  total step 4481/6200, training_loss = 0.2265\n",
      "epoch 73/100, step 18/62,  total step 4482/6200, training_loss = 0.2063\n",
      "epoch 73/100, step 19/62,  total step 4483/6200, training_loss = 0.3027\n",
      "epoch 73/100, step 20/62,  total step 4484/6200, training_loss = 0.2243\n",
      "epoch 73/100, step 21/62,  total step 4485/6200, training_loss = 0.2542\n",
      "epoch 73/100, step 22/62,  total step 4486/6200, training_loss = 0.3408\n",
      "epoch 73/100, step 23/62,  total step 4487/6200, training_loss = 0.2067\n",
      "epoch 73/100, step 24/62,  total step 4488/6200, training_loss = 0.1825\n",
      "epoch 73/100, step 25/62,  total step 4489/6200, training_loss = 0.2116\n",
      "epoch 73/100, step 26/62,  total step 4490/6200, training_loss = 0.1585\n",
      "epoch 73/100, step 27/62,  total step 4491/6200, training_loss = 0.2167\n",
      "epoch 73/100, step 28/62,  total step 4492/6200, training_loss = 0.2735\n",
      "epoch 73/100, step 29/62,  total step 4493/6200, training_loss = 0.2753\n",
      "epoch 73/100, step 30/62,  total step 4494/6200, training_loss = 0.2560\n",
      "epoch 73/100, step 31/62,  total step 4495/6200, training_loss = 0.1528\n",
      "epoch 73/100, step 32/62,  total step 4496/6200, training_loss = 0.2064\n",
      "epoch 73/100, step 33/62,  total step 4497/6200, training_loss = 0.3050\n",
      "epoch 73/100, step 34/62,  total step 4498/6200, training_loss = 0.2055\n",
      "epoch 73/100, step 35/62,  total step 4499/6200, training_loss = 0.2385\n",
      "epoch 73/100, step 36/62,  total step 4500/6200, training_loss = 0.2371\n",
      "epoch 73/100, step 37/62,  total step 4501/6200, training_loss = 0.1799\n",
      "epoch 73/100, step 38/62,  total step 4502/6200, training_loss = 0.2049\n",
      "epoch 73/100, step 39/62,  total step 4503/6200, training_loss = 0.2287\n",
      "epoch 73/100, step 40/62,  total step 4504/6200, training_loss = 0.2678\n",
      "epoch 73/100, step 41/62,  total step 4505/6200, training_loss = 0.2072\n",
      "epoch 73/100, step 42/62,  total step 4506/6200, training_loss = 0.2127\n",
      "epoch 73/100, step 43/62,  total step 4507/6200, training_loss = 0.2073\n",
      "epoch 73/100, step 44/62,  total step 4508/6200, training_loss = 0.2132\n",
      "epoch 73/100, step 45/62,  total step 4509/6200, training_loss = 0.1430\n",
      "epoch 73/100, step 46/62,  total step 4510/6200, training_loss = 0.1616\n",
      "epoch 73/100, step 47/62,  total step 4511/6200, training_loss = 0.2579\n",
      "epoch 73/100, step 48/62,  total step 4512/6200, training_loss = 0.1649\n",
      "epoch 73/100, step 49/62,  total step 4513/6200, training_loss = 0.1663\n",
      "epoch 73/100, step 50/62,  total step 4514/6200, training_loss = 0.2095\n",
      "epoch 73/100, step 51/62,  total step 4515/6200, training_loss = 0.2325\n",
      "epoch 73/100, step 52/62,  total step 4516/6200, training_loss = 0.1623\n",
      "epoch 73/100, step 53/62,  total step 4517/6200, training_loss = 0.2588\n",
      "epoch 73/100, step 54/62,  total step 4518/6200, training_loss = 0.1824\n",
      "epoch 73/100, step 55/62,  total step 4519/6200, training_loss = 0.1741\n",
      "epoch 73/100, step 56/62,  total step 4520/6200, training_loss = 0.2234\n",
      "epoch 73/100, step 57/62,  total step 4521/6200, training_loss = 0.2308\n",
      "epoch 73/100, step 58/62,  total step 4522/6200, training_loss = 0.3286\n",
      "epoch 73/100, step 59/62,  total step 4523/6200, training_loss = 0.1805\n",
      "epoch 73/100, step 60/62,  total step 4524/6200, training_loss = 0.2887\n",
      "epoch 73/100, step 61/62,  total step 4525/6200, training_loss = 0.1397\n",
      "epoch 73/100, step 62/62,  total step 4526/6200, training_loss = 0.1697 | avg loss: 0.2234 Dice Metric:   0.641\n",
      "epoch 74/100, step 1/62,  total step 4527/6200, training_loss = 0.2625\n",
      "epoch 74/100, step 2/62,  total step 4528/6200, training_loss = 0.1893\n",
      "epoch 74/100, step 3/62,  total step 4529/6200, training_loss = 0.2454\n",
      "epoch 74/100, step 4/62,  total step 4530/6200, training_loss = 0.3451\n",
      "epoch 74/100, step 5/62,  total step 4531/6200, training_loss = 0.1648\n",
      "epoch 74/100, step 6/62,  total step 4532/6200, training_loss = 0.2186\n",
      "epoch 74/100, step 7/62,  total step 4533/6200, training_loss = 0.1810\n",
      "epoch 74/100, step 8/62,  total step 4534/6200, training_loss = 0.2594\n",
      "epoch 74/100, step 9/62,  total step 4535/6200, training_loss = 0.2000\n",
      "epoch 74/100, step 10/62,  total step 4536/6200, training_loss = 0.1991\n",
      "epoch 74/100, step 11/62,  total step 4537/6200, training_loss = 0.1654\n",
      "epoch 74/100, step 12/62,  total step 4538/6200, training_loss = 0.1686\n",
      "epoch 74/100, step 13/62,  total step 4539/6200, training_loss = 0.1873\n",
      "epoch 74/100, step 14/62,  total step 4540/6200, training_loss = 0.2409\n",
      "epoch 74/100, step 15/62,  total step 4541/6200, training_loss = 0.3374\n",
      "epoch 74/100, step 16/62,  total step 4542/6200, training_loss = 0.2844\n",
      "epoch 74/100, step 17/62,  total step 4543/6200, training_loss = 0.1882\n",
      "epoch 74/100, step 18/62,  total step 4544/6200, training_loss = 0.2052\n",
      "epoch 74/100, step 19/62,  total step 4545/6200, training_loss = 0.2557\n",
      "epoch 74/100, step 20/62,  total step 4546/6200, training_loss = 0.2199\n",
      "epoch 74/100, step 21/62,  total step 4547/6200, training_loss = 0.2120\n",
      "epoch 74/100, step 22/62,  total step 4548/6200, training_loss = 0.3420\n",
      "epoch 74/100, step 23/62,  total step 4549/6200, training_loss = 0.2223\n",
      "epoch 74/100, step 24/62,  total step 4550/6200, training_loss = 0.1815\n",
      "epoch 74/100, step 25/62,  total step 4551/6200, training_loss = 0.2194\n",
      "epoch 74/100, step 26/62,  total step 4552/6200, training_loss = 0.1796\n",
      "epoch 74/100, step 27/62,  total step 4553/6200, training_loss = 0.2772\n",
      "epoch 74/100, step 28/62,  total step 4554/6200, training_loss = 0.4328\n",
      "epoch 74/100, step 29/62,  total step 4555/6200, training_loss = 0.2845\n",
      "epoch 74/100, step 30/62,  total step 4556/6200, training_loss = 0.2604\n",
      "epoch 74/100, step 31/62,  total step 4557/6200, training_loss = 0.1577\n",
      "epoch 74/100, step 32/62,  total step 4558/6200, training_loss = 0.2142\n",
      "epoch 74/100, step 33/62,  total step 4559/6200, training_loss = 0.3348\n",
      "epoch 74/100, step 34/62,  total step 4560/6200, training_loss = 0.1986\n",
      "epoch 74/100, step 35/62,  total step 4561/6200, training_loss = 0.2058\n",
      "epoch 74/100, step 36/62,  total step 4562/6200, training_loss = 0.2645\n",
      "epoch 74/100, step 37/62,  total step 4563/6200, training_loss = 0.2264\n",
      "epoch 74/100, step 38/62,  total step 4564/6200, training_loss = 0.2451\n",
      "epoch 74/100, step 39/62,  total step 4565/6200, training_loss = 0.2909\n",
      "epoch 74/100, step 40/62,  total step 4566/6200, training_loss = 0.2485\n",
      "epoch 74/100, step 41/62,  total step 4567/6200, training_loss = 0.1887\n",
      "epoch 74/100, step 42/62,  total step 4568/6200, training_loss = 0.2107\n",
      "epoch 74/100, step 43/62,  total step 4569/6200, training_loss = 0.2194\n",
      "epoch 74/100, step 44/62,  total step 4570/6200, training_loss = 0.2439\n",
      "epoch 74/100, step 45/62,  total step 4571/6200, training_loss = 0.1526\n",
      "epoch 74/100, step 46/62,  total step 4572/6200, training_loss = 0.1821\n",
      "epoch 74/100, step 47/62,  total step 4573/6200, training_loss = 0.2548\n",
      "epoch 74/100, step 48/62,  total step 4574/6200, training_loss = 0.1581\n",
      "epoch 74/100, step 49/62,  total step 4575/6200, training_loss = 0.1720\n",
      "epoch 74/100, step 50/62,  total step 4576/6200, training_loss = 0.2185\n",
      "epoch 74/100, step 51/62,  total step 4577/6200, training_loss = 0.2215\n",
      "epoch 74/100, step 52/62,  total step 4578/6200, training_loss = 0.1679\n",
      "epoch 74/100, step 53/62,  total step 4579/6200, training_loss = 0.2115\n",
      "epoch 74/100, step 54/62,  total step 4580/6200, training_loss = 0.1526\n",
      "epoch 74/100, step 55/62,  total step 4581/6200, training_loss = 0.1934\n",
      "epoch 74/100, step 56/62,  total step 4582/6200, training_loss = 0.2291\n",
      "epoch 74/100, step 57/62,  total step 4583/6200, training_loss = 0.2295\n",
      "epoch 74/100, step 58/62,  total step 4584/6200, training_loss = 0.3186\n",
      "epoch 74/100, step 59/62,  total step 4585/6200, training_loss = 0.1664\n",
      "epoch 74/100, step 60/62,  total step 4586/6200, training_loss = 0.2630\n",
      "epoch 74/100, step 61/62,  total step 4587/6200, training_loss = 0.1474\n",
      "epoch 74/100, step 62/62,  total step 4588/6200, training_loss = 0.1746 | avg loss: 0.2257 Dice Metric:   0.645\n",
      "epoch 75/100, step 1/62,  total step 4589/6200, training_loss = 0.2004\n",
      "epoch 75/100, step 2/62,  total step 4590/6200, training_loss = 0.1959\n",
      "epoch 75/100, step 3/62,  total step 4591/6200, training_loss = 0.2365\n",
      "epoch 75/100, step 4/62,  total step 4592/6200, training_loss = 0.3449\n",
      "epoch 75/100, step 5/62,  total step 4593/6200, training_loss = 0.1522\n",
      "epoch 75/100, step 6/62,  total step 4594/6200, training_loss = 0.2193\n",
      "epoch 75/100, step 7/62,  total step 4595/6200, training_loss = 0.1814\n",
      "epoch 75/100, step 8/62,  total step 4596/6200, training_loss = 0.2610\n",
      "epoch 75/100, step 9/62,  total step 4597/6200, training_loss = 0.1615\n",
      "epoch 75/100, step 10/62,  total step 4598/6200, training_loss = 0.2135\n",
      "epoch 75/100, step 11/62,  total step 4599/6200, training_loss = 0.1619\n",
      "epoch 75/100, step 12/62,  total step 4600/6200, training_loss = 0.1748\n",
      "epoch 75/100, step 13/62,  total step 4601/6200, training_loss = 0.1955\n",
      "epoch 75/100, step 14/62,  total step 4602/6200, training_loss = 0.2488\n",
      "epoch 75/100, step 15/62,  total step 4603/6200, training_loss = 0.3418\n",
      "epoch 75/100, step 16/62,  total step 4604/6200, training_loss = 0.2914\n",
      "epoch 75/100, step 17/62,  total step 4605/6200, training_loss = 0.1809\n",
      "epoch 75/100, step 18/62,  total step 4606/6200, training_loss = 0.2118\n",
      "epoch 75/100, step 19/62,  total step 4607/6200, training_loss = 0.2623\n",
      "epoch 75/100, step 20/62,  total step 4608/6200, training_loss = 0.2334\n",
      "epoch 75/100, step 21/62,  total step 4609/6200, training_loss = 0.2243\n",
      "epoch 75/100, step 22/62,  total step 4610/6200, training_loss = 0.3777\n",
      "epoch 75/100, step 23/62,  total step 4611/6200, training_loss = 0.1992\n",
      "epoch 75/100, step 24/62,  total step 4612/6200, training_loss = 0.1712\n",
      "epoch 75/100, step 25/62,  total step 4613/6200, training_loss = 0.2066\n",
      "epoch 75/100, step 26/62,  total step 4614/6200, training_loss = 0.1642\n",
      "epoch 75/100, step 27/62,  total step 4615/6200, training_loss = 0.2590\n",
      "epoch 75/100, step 28/62,  total step 4616/6200, training_loss = 0.3253\n",
      "epoch 75/100, step 29/62,  total step 4617/6200, training_loss = 0.2532\n",
      "epoch 75/100, step 30/62,  total step 4618/6200, training_loss = 0.2519\n",
      "epoch 75/100, step 31/62,  total step 4619/6200, training_loss = 0.1430\n",
      "epoch 75/100, step 32/62,  total step 4620/6200, training_loss = 0.2446\n",
      "epoch 75/100, step 33/62,  total step 4621/6200, training_loss = 0.2989\n",
      "epoch 75/100, step 34/62,  total step 4622/6200, training_loss = 0.2101\n",
      "epoch 75/100, step 35/62,  total step 4623/6200, training_loss = 0.2217\n",
      "epoch 75/100, step 36/62,  total step 4624/6200, training_loss = 0.2462\n",
      "epoch 75/100, step 37/62,  total step 4625/6200, training_loss = 0.2352\n",
      "epoch 75/100, step 38/62,  total step 4626/6200, training_loss = 0.2376\n",
      "epoch 75/100, step 39/62,  total step 4627/6200, training_loss = 0.2209\n",
      "epoch 75/100, step 40/62,  total step 4628/6200, training_loss = 0.2546\n",
      "epoch 75/100, step 41/62,  total step 4629/6200, training_loss = 0.1985\n",
      "epoch 75/100, step 42/62,  total step 4630/6200, training_loss = 0.2152\n",
      "epoch 75/100, step 43/62,  total step 4631/6200, training_loss = 0.1897\n",
      "epoch 75/100, step 44/62,  total step 4632/6200, training_loss = 0.2146\n",
      "epoch 75/100, step 45/62,  total step 4633/6200, training_loss = 0.1280\n",
      "epoch 75/100, step 46/62,  total step 4634/6200, training_loss = 0.1624\n",
      "epoch 75/100, step 47/62,  total step 4635/6200, training_loss = 0.2630\n",
      "epoch 75/100, step 48/62,  total step 4636/6200, training_loss = 0.1612\n",
      "epoch 75/100, step 49/62,  total step 4637/6200, training_loss = 0.1613\n",
      "epoch 75/100, step 50/62,  total step 4638/6200, training_loss = 0.1922\n",
      "epoch 75/100, step 51/62,  total step 4639/6200, training_loss = 0.2126\n",
      "epoch 75/100, step 52/62,  total step 4640/6200, training_loss = 0.1534\n",
      "epoch 75/100, step 53/62,  total step 4641/6200, training_loss = 0.2209\n",
      "epoch 75/100, step 54/62,  total step 4642/6200, training_loss = 0.1554\n",
      "epoch 75/100, step 55/62,  total step 4643/6200, training_loss = 0.1956\n",
      "epoch 75/100, step 56/62,  total step 4644/6200, training_loss = 0.2295\n",
      "epoch 75/100, step 57/62,  total step 4645/6200, training_loss = 0.2536\n",
      "epoch 75/100, step 58/62,  total step 4646/6200, training_loss = 0.3034\n",
      "epoch 75/100, step 59/62,  total step 4647/6200, training_loss = 0.1765\n",
      "epoch 75/100, step 60/62,  total step 4648/6200, training_loss = 0.2626\n",
      "epoch 75/100, step 61/62,  total step 4649/6200, training_loss = 0.1614\n",
      "epoch 75/100, step 62/62,  total step 4650/6200, training_loss = 0.1947 | avg loss: 0.2197 Dice Metric:   0.642\n",
      "epoch 76/100, step 1/62,  total step 4651/6200, training_loss = 0.2022\n",
      "epoch 76/100, step 2/62,  total step 4652/6200, training_loss = 0.1825\n",
      "epoch 76/100, step 3/62,  total step 4653/6200, training_loss = 0.2357\n",
      "epoch 76/100, step 4/62,  total step 4654/6200, training_loss = 0.3442\n",
      "epoch 76/100, step 5/62,  total step 4655/6200, training_loss = 0.1324\n",
      "epoch 76/100, step 6/62,  total step 4656/6200, training_loss = 0.1882\n",
      "epoch 76/100, step 7/62,  total step 4657/6200, training_loss = 0.1754\n",
      "epoch 76/100, step 8/62,  total step 4658/6200, training_loss = 0.2729\n",
      "epoch 76/100, step 9/62,  total step 4659/6200, training_loss = 0.1739\n",
      "epoch 76/100, step 10/62,  total step 4660/6200, training_loss = 0.2078\n",
      "epoch 76/100, step 11/62,  total step 4661/6200, training_loss = 0.1590\n",
      "epoch 76/100, step 12/62,  total step 4662/6200, training_loss = 0.1650\n",
      "epoch 76/100, step 13/62,  total step 4663/6200, training_loss = 0.2156\n",
      "epoch 76/100, step 14/62,  total step 4664/6200, training_loss = 0.2329\n",
      "epoch 76/100, step 15/62,  total step 4665/6200, training_loss = 0.3773\n",
      "epoch 76/100, step 16/62,  total step 4666/6200, training_loss = 0.2888\n",
      "epoch 76/100, step 17/62,  total step 4667/6200, training_loss = 0.2214\n",
      "epoch 76/100, step 18/62,  total step 4668/6200, training_loss = 0.1966\n",
      "epoch 76/100, step 19/62,  total step 4669/6200, training_loss = 0.2684\n",
      "epoch 76/100, step 20/62,  total step 4670/6200, training_loss = 0.2309\n",
      "epoch 76/100, step 21/62,  total step 4671/6200, training_loss = 0.2486\n",
      "epoch 76/100, step 22/62,  total step 4672/6200, training_loss = 0.3263\n",
      "epoch 76/100, step 23/62,  total step 4673/6200, training_loss = 0.2303\n",
      "epoch 76/100, step 24/62,  total step 4674/6200, training_loss = 0.2022\n",
      "epoch 76/100, step 25/62,  total step 4675/6200, training_loss = 0.2085\n",
      "epoch 76/100, step 26/62,  total step 4676/6200, training_loss = 0.1597\n",
      "epoch 76/100, step 27/62,  total step 4677/6200, training_loss = 0.2424\n",
      "epoch 76/100, step 28/62,  total step 4678/6200, training_loss = 0.3237\n",
      "epoch 76/100, step 29/62,  total step 4679/6200, training_loss = 0.2730\n",
      "epoch 76/100, step 30/62,  total step 4680/6200, training_loss = 0.2775\n",
      "epoch 76/100, step 31/62,  total step 4681/6200, training_loss = 0.1685\n",
      "epoch 76/100, step 32/62,  total step 4682/6200, training_loss = 0.1855\n",
      "epoch 76/100, step 33/62,  total step 4683/6200, training_loss = 0.2879\n",
      "epoch 76/100, step 34/62,  total step 4684/6200, training_loss = 0.1975\n",
      "epoch 76/100, step 35/62,  total step 4685/6200, training_loss = 0.2478\n",
      "epoch 76/100, step 36/62,  total step 4686/6200, training_loss = 0.3010\n",
      "epoch 76/100, step 37/62,  total step 4687/6200, training_loss = 0.2332\n",
      "epoch 76/100, step 38/62,  total step 4688/6200, training_loss = 0.2510\n",
      "epoch 76/100, step 39/62,  total step 4689/6200, training_loss = 0.2708\n",
      "epoch 76/100, step 40/62,  total step 4690/6200, training_loss = 0.2744\n",
      "epoch 76/100, step 41/62,  total step 4691/6200, training_loss = 0.1908\n",
      "epoch 76/100, step 42/62,  total step 4692/6200, training_loss = 0.2248\n",
      "epoch 76/100, step 43/62,  total step 4693/6200, training_loss = 0.2189\n",
      "epoch 76/100, step 44/62,  total step 4694/6200, training_loss = 0.2468\n",
      "epoch 76/100, step 45/62,  total step 4695/6200, training_loss = 0.1594\n",
      "epoch 76/100, step 46/62,  total step 4696/6200, training_loss = 0.1866\n",
      "epoch 76/100, step 47/62,  total step 4697/6200, training_loss = 0.2272\n",
      "epoch 76/100, step 48/62,  total step 4698/6200, training_loss = 0.1512\n",
      "epoch 76/100, step 49/62,  total step 4699/6200, training_loss = 0.1495\n",
      "epoch 76/100, step 50/62,  total step 4700/6200, training_loss = 0.2365\n",
      "epoch 76/100, step 51/62,  total step 4701/6200, training_loss = 0.2039\n",
      "epoch 76/100, step 52/62,  total step 4702/6200, training_loss = 0.1585\n",
      "epoch 76/100, step 53/62,  total step 4703/6200, training_loss = 0.1831\n",
      "epoch 76/100, step 54/62,  total step 4704/6200, training_loss = 0.1436\n",
      "epoch 76/100, step 55/62,  total step 4705/6200, training_loss = 0.1886\n",
      "epoch 76/100, step 56/62,  total step 4706/6200, training_loss = 0.2527\n",
      "epoch 76/100, step 57/62,  total step 4707/6200, training_loss = 0.2435\n",
      "epoch 76/100, step 58/62,  total step 4708/6200, training_loss = 0.3278\n",
      "epoch 76/100, step 59/62,  total step 4709/6200, training_loss = 0.2086\n",
      "epoch 76/100, step 60/62,  total step 4710/6200, training_loss = 0.2663\n",
      "epoch 76/100, step 61/62,  total step 4711/6200, training_loss = 0.1425\n",
      "epoch 76/100, step 62/62,  total step 4712/6200, training_loss = 0.2004 | avg loss: 0.2241 Dice Metric:    0.64\n",
      "epoch 77/100, step 1/62,  total step 4713/6200, training_loss = 0.1989\n",
      "epoch 77/100, step 2/62,  total step 4714/6200, training_loss = 0.1850\n",
      "epoch 77/100, step 3/62,  total step 4715/6200, training_loss = 0.2356\n",
      "epoch 77/100, step 4/62,  total step 4716/6200, training_loss = 0.3381\n",
      "epoch 77/100, step 5/62,  total step 4717/6200, training_loss = 0.1312\n",
      "epoch 77/100, step 6/62,  total step 4718/6200, training_loss = 0.2184\n",
      "epoch 77/100, step 7/62,  total step 4719/6200, training_loss = 0.1782\n",
      "epoch 77/100, step 8/62,  total step 4720/6200, training_loss = 0.2490\n",
      "epoch 77/100, step 9/62,  total step 4721/6200, training_loss = 0.1970\n",
      "epoch 77/100, step 10/62,  total step 4722/6200, training_loss = 0.2050\n",
      "epoch 77/100, step 11/62,  total step 4723/6200, training_loss = 0.1619\n",
      "epoch 77/100, step 12/62,  total step 4724/6200, training_loss = 0.1851\n",
      "epoch 77/100, step 13/62,  total step 4725/6200, training_loss = 0.2065\n",
      "epoch 77/100, step 14/62,  total step 4726/6200, training_loss = 0.2261\n",
      "epoch 77/100, step 15/62,  total step 4727/6200, training_loss = 0.3781\n",
      "epoch 77/100, step 16/62,  total step 4728/6200, training_loss = 0.3035\n",
      "epoch 77/100, step 17/62,  total step 4729/6200, training_loss = 0.1975\n",
      "epoch 77/100, step 18/62,  total step 4730/6200, training_loss = 0.1998\n",
      "epoch 77/100, step 19/62,  total step 4731/6200, training_loss = 0.2502\n",
      "epoch 77/100, step 20/62,  total step 4732/6200, training_loss = 0.2145\n",
      "epoch 77/100, step 21/62,  total step 4733/6200, training_loss = 0.2281\n",
      "epoch 77/100, step 22/62,  total step 4734/6200, training_loss = 0.3596\n",
      "epoch 77/100, step 23/62,  total step 4735/6200, training_loss = 0.2553\n",
      "epoch 77/100, step 24/62,  total step 4736/6200, training_loss = 0.1639\n",
      "epoch 77/100, step 25/62,  total step 4737/6200, training_loss = 0.1930\n",
      "epoch 77/100, step 26/62,  total step 4738/6200, training_loss = 0.1452\n",
      "epoch 77/100, step 27/62,  total step 4739/6200, training_loss = 0.2482\n",
      "epoch 77/100, step 28/62,  total step 4740/6200, training_loss = 0.3408\n",
      "epoch 77/100, step 29/62,  total step 4741/6200, training_loss = 0.3040\n",
      "epoch 77/100, step 30/62,  total step 4742/6200, training_loss = 0.2577\n",
      "epoch 77/100, step 31/62,  total step 4743/6200, training_loss = 0.1554\n",
      "epoch 77/100, step 32/62,  total step 4744/6200, training_loss = 0.2129\n",
      "epoch 77/100, step 33/62,  total step 4745/6200, training_loss = 0.2586\n",
      "epoch 77/100, step 34/62,  total step 4746/6200, training_loss = 0.1922\n",
      "epoch 77/100, step 35/62,  total step 4747/6200, training_loss = 0.2156\n",
      "epoch 77/100, step 36/62,  total step 4748/6200, training_loss = 0.2623\n",
      "epoch 77/100, step 37/62,  total step 4749/6200, training_loss = 0.2164\n",
      "epoch 77/100, step 38/62,  total step 4750/6200, training_loss = 0.2567\n",
      "epoch 77/100, step 39/62,  total step 4751/6200, training_loss = 0.2426\n",
      "epoch 77/100, step 40/62,  total step 4752/6200, training_loss = 0.2768\n",
      "epoch 77/100, step 41/62,  total step 4753/6200, training_loss = 0.1931\n",
      "epoch 77/100, step 42/62,  total step 4754/6200, training_loss = 0.2442\n",
      "epoch 77/100, step 43/62,  total step 4755/6200, training_loss = 0.2109\n",
      "epoch 77/100, step 44/62,  total step 4756/6200, training_loss = 0.2115\n",
      "epoch 77/100, step 45/62,  total step 4757/6200, training_loss = 0.1553\n",
      "epoch 77/100, step 46/62,  total step 4758/6200, training_loss = 0.1583\n",
      "epoch 77/100, step 47/62,  total step 4759/6200, training_loss = 0.2771\n",
      "epoch 77/100, step 48/62,  total step 4760/6200, training_loss = 0.1485\n",
      "epoch 77/100, step 49/62,  total step 4761/6200, training_loss = 0.1546\n",
      "epoch 77/100, step 50/62,  total step 4762/6200, training_loss = 0.1799\n",
      "epoch 77/100, step 51/62,  total step 4763/6200, training_loss = 0.2024\n",
      "epoch 77/100, step 52/62,  total step 4764/6200, training_loss = 0.1803\n",
      "epoch 77/100, step 53/62,  total step 4765/6200, training_loss = 0.2435\n",
      "epoch 77/100, step 54/62,  total step 4766/6200, training_loss = 0.1758\n",
      "epoch 77/100, step 55/62,  total step 4767/6200, training_loss = 0.1761\n",
      "epoch 77/100, step 56/62,  total step 4768/6200, training_loss = 0.2316\n",
      "epoch 77/100, step 57/62,  total step 4769/6200, training_loss = 0.2434\n",
      "epoch 77/100, step 58/62,  total step 4770/6200, training_loss = 0.3187\n",
      "epoch 77/100, step 59/62,  total step 4771/6200, training_loss = 0.1746\n",
      "epoch 77/100, step 60/62,  total step 4772/6200, training_loss = 0.2919\n",
      "epoch 77/100, step 61/62,  total step 4773/6200, training_loss = 0.1392\n",
      "epoch 77/100, step 62/62,  total step 4774/6200, training_loss = 0.3043 | avg loss: 0.2236 Dice Metric:   0.652\n",
      "epoch 78/100, step 1/62,  total step 4775/6200, training_loss = 0.2176\n",
      "epoch 78/100, step 2/62,  total step 4776/6200, training_loss = 0.1992\n",
      "epoch 78/100, step 3/62,  total step 4777/6200, training_loss = 0.2228\n",
      "epoch 78/100, step 4/62,  total step 4778/6200, training_loss = 0.3357\n",
      "epoch 78/100, step 5/62,  total step 4779/6200, training_loss = 0.1562\n",
      "epoch 78/100, step 6/62,  total step 4780/6200, training_loss = 0.1877\n",
      "epoch 78/100, step 7/62,  total step 4781/6200, training_loss = 0.1786\n",
      "epoch 78/100, step 8/62,  total step 4782/6200, training_loss = 0.2213\n",
      "epoch 78/100, step 9/62,  total step 4783/6200, training_loss = 0.1653\n",
      "epoch 78/100, step 10/62,  total step 4784/6200, training_loss = 0.2459\n",
      "epoch 78/100, step 11/62,  total step 4785/6200, training_loss = 0.1548\n",
      "epoch 78/100, step 12/62,  total step 4786/6200, training_loss = 0.1633\n",
      "epoch 78/100, step 13/62,  total step 4787/6200, training_loss = 0.2191\n",
      "epoch 78/100, step 14/62,  total step 4788/6200, training_loss = 0.2450\n",
      "epoch 78/100, step 15/62,  total step 4789/6200, training_loss = 0.3676\n",
      "epoch 78/100, step 16/62,  total step 4790/6200, training_loss = 0.2780\n",
      "epoch 78/100, step 17/62,  total step 4791/6200, training_loss = 0.1592\n",
      "epoch 78/100, step 18/62,  total step 4792/6200, training_loss = 0.2178\n",
      "epoch 78/100, step 19/62,  total step 4793/6200, training_loss = 0.2753\n",
      "epoch 78/100, step 20/62,  total step 4794/6200, training_loss = 0.2163\n",
      "epoch 78/100, step 21/62,  total step 4795/6200, training_loss = 0.2383\n",
      "epoch 78/100, step 22/62,  total step 4796/6200, training_loss = 0.3794\n",
      "epoch 78/100, step 23/62,  total step 4797/6200, training_loss = 0.2371\n",
      "epoch 78/100, step 24/62,  total step 4798/6200, training_loss = 0.1803\n",
      "epoch 78/100, step 25/62,  total step 4799/6200, training_loss = 0.1930\n",
      "epoch 78/100, step 26/62,  total step 4800/6200, training_loss = 0.1603\n",
      "epoch 78/100, step 27/62,  total step 4801/6200, training_loss = 0.2494\n",
      "epoch 78/100, step 28/62,  total step 4802/6200, training_loss = 0.3234\n",
      "epoch 78/100, step 29/62,  total step 4803/6200, training_loss = 0.2472\n",
      "epoch 78/100, step 30/62,  total step 4804/6200, training_loss = 0.2807\n",
      "epoch 78/100, step 31/62,  total step 4805/6200, training_loss = 0.1631\n",
      "epoch 78/100, step 32/62,  total step 4806/6200, training_loss = 0.2103\n",
      "epoch 78/100, step 33/62,  total step 4807/6200, training_loss = 0.3010\n",
      "epoch 78/100, step 34/62,  total step 4808/6200, training_loss = 0.2029\n",
      "epoch 78/100, step 35/62,  total step 4809/6200, training_loss = 0.2578\n",
      "epoch 78/100, step 36/62,  total step 4810/6200, training_loss = 0.2984\n",
      "epoch 78/100, step 37/62,  total step 4811/6200, training_loss = 0.2156\n",
      "epoch 78/100, step 38/62,  total step 4812/6200, training_loss = 0.2546\n",
      "epoch 78/100, step 39/62,  total step 4813/6200, training_loss = 0.2700\n",
      "epoch 78/100, step 40/62,  total step 4814/6200, training_loss = 0.2865\n",
      "epoch 78/100, step 41/62,  total step 4815/6200, training_loss = 0.1921\n",
      "epoch 78/100, step 42/62,  total step 4816/6200, training_loss = 0.2047\n",
      "epoch 78/100, step 43/62,  total step 4817/6200, training_loss = 0.2171\n",
      "epoch 78/100, step 44/62,  total step 4818/6200, training_loss = 0.2210\n",
      "epoch 78/100, step 45/62,  total step 4819/6200, training_loss = 0.1445\n",
      "epoch 78/100, step 46/62,  total step 4820/6200, training_loss = 0.1747\n",
      "epoch 78/100, step 47/62,  total step 4821/6200, training_loss = 0.2859\n",
      "epoch 78/100, step 48/62,  total step 4822/6200, training_loss = 0.1395\n",
      "epoch 78/100, step 49/62,  total step 4823/6200, training_loss = 0.1755\n",
      "epoch 78/100, step 50/62,  total step 4824/6200, training_loss = 0.2111\n",
      "epoch 78/100, step 51/62,  total step 4825/6200, training_loss = 0.2089\n",
      "epoch 78/100, step 52/62,  total step 4826/6200, training_loss = 0.1992\n",
      "epoch 78/100, step 53/62,  total step 4827/6200, training_loss = 0.2049\n",
      "epoch 78/100, step 54/62,  total step 4828/6200, training_loss = 0.1534\n",
      "epoch 78/100, step 55/62,  total step 4829/6200, training_loss = 0.1681\n",
      "epoch 78/100, step 56/62,  total step 4830/6200, training_loss = 0.2497\n",
      "epoch 78/100, step 57/62,  total step 4831/6200, training_loss = 0.2571\n",
      "epoch 78/100, step 58/62,  total step 4832/6200, training_loss = 0.3322\n",
      "epoch 78/100, step 59/62,  total step 4833/6200, training_loss = 0.1844\n",
      "epoch 78/100, step 60/62,  total step 4834/6200, training_loss = 0.2756\n",
      "epoch 78/100, step 61/62,  total step 4835/6200, training_loss = 0.1398\n",
      "epoch 78/100, step 62/62,  total step 4836/6200, training_loss = 0.1826 | avg loss: 0.2242 Dice Metric:   0.663\n",
      "epoch 79/100, step 1/62,  total step 4837/6200, training_loss = 0.2312\n",
      "epoch 79/100, step 2/62,  total step 4838/6200, training_loss = 0.2229\n",
      "epoch 79/100, step 3/62,  total step 4839/6200, training_loss = 0.2452\n",
      "epoch 79/100, step 4/62,  total step 4840/6200, training_loss = 0.3568\n",
      "epoch 79/100, step 5/62,  total step 4841/6200, training_loss = 0.1544\n",
      "epoch 79/100, step 6/62,  total step 4842/6200, training_loss = 0.1985\n",
      "epoch 79/100, step 7/62,  total step 4843/6200, training_loss = 0.1764\n",
      "epoch 79/100, step 8/62,  total step 4844/6200, training_loss = 0.2384\n",
      "epoch 79/100, step 9/62,  total step 4845/6200, training_loss = 0.1742\n",
      "epoch 79/100, step 10/62,  total step 4846/6200, training_loss = 0.2073\n",
      "epoch 79/100, step 11/62,  total step 4847/6200, training_loss = 0.1670\n",
      "epoch 79/100, step 12/62,  total step 4848/6200, training_loss = 0.1580\n",
      "epoch 79/100, step 13/62,  total step 4849/6200, training_loss = 0.2245\n",
      "epoch 79/100, step 14/62,  total step 4850/6200, training_loss = 0.2286\n",
      "epoch 79/100, step 15/62,  total step 4851/6200, training_loss = 0.3607\n",
      "epoch 79/100, step 16/62,  total step 4852/6200, training_loss = 0.2925\n",
      "epoch 79/100, step 17/62,  total step 4853/6200, training_loss = 0.1924\n",
      "epoch 79/100, step 18/62,  total step 4854/6200, training_loss = 0.2151\n",
      "epoch 79/100, step 19/62,  total step 4855/6200, training_loss = 0.2936\n",
      "epoch 79/100, step 20/62,  total step 4856/6200, training_loss = 0.2128\n",
      "epoch 79/100, step 21/62,  total step 4857/6200, training_loss = 0.2282\n",
      "epoch 79/100, step 22/62,  total step 4858/6200, training_loss = 0.3848\n",
      "epoch 79/100, step 23/62,  total step 4859/6200, training_loss = 0.2460\n",
      "epoch 79/100, step 24/62,  total step 4860/6200, training_loss = 0.1950\n",
      "epoch 79/100, step 25/62,  total step 4861/6200, training_loss = 0.2227\n",
      "epoch 79/100, step 26/62,  total step 4862/6200, training_loss = 0.1537\n",
      "epoch 79/100, step 27/62,  total step 4863/6200, training_loss = 0.2696\n",
      "epoch 79/100, step 28/62,  total step 4864/6200, training_loss = 0.3237\n",
      "epoch 79/100, step 29/62,  total step 4865/6200, training_loss = 0.2789\n",
      "epoch 79/100, step 30/62,  total step 4866/6200, training_loss = 0.2402\n",
      "epoch 79/100, step 31/62,  total step 4867/6200, training_loss = 0.1720\n",
      "epoch 79/100, step 32/62,  total step 4868/6200, training_loss = 0.2046\n",
      "epoch 79/100, step 33/62,  total step 4869/6200, training_loss = 0.3344\n",
      "epoch 79/100, step 34/62,  total step 4870/6200, training_loss = 0.2040\n",
      "epoch 79/100, step 35/62,  total step 4871/6200, training_loss = 0.2242\n",
      "epoch 79/100, step 36/62,  total step 4872/6200, training_loss = 0.2810\n",
      "epoch 79/100, step 37/62,  total step 4873/6200, training_loss = 0.2147\n",
      "epoch 79/100, step 38/62,  total step 4874/6200, training_loss = 0.2373\n",
      "epoch 79/100, step 39/62,  total step 4875/6200, training_loss = 0.2702\n",
      "epoch 79/100, step 40/62,  total step 4876/6200, training_loss = 0.2804\n",
      "epoch 79/100, step 41/62,  total step 4877/6200, training_loss = 0.1706\n",
      "epoch 79/100, step 42/62,  total step 4878/6200, training_loss = 0.2099\n",
      "epoch 79/100, step 43/62,  total step 4879/6200, training_loss = 0.2342\n",
      "epoch 79/100, step 44/62,  total step 4880/6200, training_loss = 0.2185\n",
      "epoch 79/100, step 45/62,  total step 4881/6200, training_loss = 0.1437\n",
      "epoch 79/100, step 46/62,  total step 4882/6200, training_loss = 0.1634\n",
      "epoch 79/100, step 47/62,  total step 4883/6200, training_loss = 0.3062\n",
      "epoch 79/100, step 48/62,  total step 4884/6200, training_loss = 0.1733\n",
      "epoch 79/100, step 49/62,  total step 4885/6200, training_loss = 0.1677\n",
      "epoch 79/100, step 50/62,  total step 4886/6200, training_loss = 0.2288\n",
      "epoch 79/100, step 51/62,  total step 4887/6200, training_loss = 0.2456\n",
      "epoch 79/100, step 52/62,  total step 4888/6200, training_loss = 0.1842\n",
      "epoch 79/100, step 53/62,  total step 4889/6200, training_loss = 0.2585\n",
      "epoch 79/100, step 54/62,  total step 4890/6200, training_loss = 0.2204\n",
      "epoch 79/100, step 55/62,  total step 4891/6200, training_loss = 0.1715\n",
      "epoch 79/100, step 56/62,  total step 4892/6200, training_loss = 0.2639\n",
      "epoch 79/100, step 57/62,  total step 4893/6200, training_loss = 0.2618\n",
      "epoch 79/100, step 58/62,  total step 4894/6200, training_loss = 0.3265\n",
      "epoch 79/100, step 59/62,  total step 4895/6200, training_loss = 0.1879\n",
      "epoch 79/100, step 60/62,  total step 4896/6200, training_loss = 0.2623\n",
      "epoch 79/100, step 61/62,  total step 4897/6200, training_loss = 0.1517\n",
      "epoch 79/100, step 62/62,  total step 4898/6200, training_loss = 0.1897 | avg loss: 0.2299 Dice Metric:   0.687\n",
      "epoch 80/100, step 1/62,  total step 4899/6200, training_loss = 0.2285\n",
      "epoch 80/100, step 2/62,  total step 4900/6200, training_loss = 0.1849\n",
      "epoch 80/100, step 3/62,  total step 4901/6200, training_loss = 0.2596\n",
      "epoch 80/100, step 4/62,  total step 4902/6200, training_loss = 0.3467\n",
      "epoch 80/100, step 5/62,  total step 4903/6200, training_loss = 0.1499\n",
      "epoch 80/100, step 6/62,  total step 4904/6200, training_loss = 0.2061\n",
      "epoch 80/100, step 7/62,  total step 4905/6200, training_loss = 0.1851\n",
      "epoch 80/100, step 8/62,  total step 4906/6200, training_loss = 0.2468\n",
      "epoch 80/100, step 9/62,  total step 4907/6200, training_loss = 0.1754\n",
      "epoch 80/100, step 10/62,  total step 4908/6200, training_loss = 0.2359\n",
      "epoch 80/100, step 11/62,  total step 4909/6200, training_loss = 0.1592\n",
      "epoch 80/100, step 12/62,  total step 4910/6200, training_loss = 0.1736\n",
      "epoch 80/100, step 13/62,  total step 4911/6200, training_loss = 0.2013\n",
      "epoch 80/100, step 14/62,  total step 4912/6200, training_loss = 0.2460\n",
      "epoch 80/100, step 15/62,  total step 4913/6200, training_loss = 0.3669\n",
      "epoch 80/100, step 16/62,  total step 4914/6200, training_loss = 0.2980\n",
      "epoch 80/100, step 17/62,  total step 4915/6200, training_loss = 0.1831\n",
      "epoch 80/100, step 18/62,  total step 4916/6200, training_loss = 0.2040\n",
      "epoch 80/100, step 19/62,  total step 4917/6200, training_loss = 0.2690\n",
      "epoch 80/100, step 20/62,  total step 4918/6200, training_loss = 0.2433\n",
      "epoch 80/100, step 21/62,  total step 4919/6200, training_loss = 0.2441\n",
      "epoch 80/100, step 22/62,  total step 4920/6200, training_loss = 0.3955\n",
      "epoch 80/100, step 23/62,  total step 4921/6200, training_loss = 0.2411\n",
      "epoch 80/100, step 24/62,  total step 4922/6200, training_loss = 0.1758\n",
      "epoch 80/100, step 25/62,  total step 4923/6200, training_loss = 0.2461\n",
      "epoch 80/100, step 26/62,  total step 4924/6200, training_loss = 0.1755\n",
      "epoch 80/100, step 27/62,  total step 4925/6200, training_loss = 0.2312\n",
      "epoch 80/100, step 28/62,  total step 4926/6200, training_loss = 0.3853\n",
      "epoch 80/100, step 29/62,  total step 4927/6200, training_loss = 0.2548\n",
      "epoch 80/100, step 30/62,  total step 4928/6200, training_loss = 0.2588\n",
      "epoch 80/100, step 31/62,  total step 4929/6200, training_loss = 0.1366\n",
      "epoch 80/100, step 32/62,  total step 4930/6200, training_loss = 0.1980\n",
      "epoch 80/100, step 33/62,  total step 4931/6200, training_loss = 0.2901\n",
      "epoch 80/100, step 34/62,  total step 4932/6200, training_loss = 0.1846\n",
      "epoch 80/100, step 35/62,  total step 4933/6200, training_loss = 0.2418\n",
      "epoch 80/100, step 36/62,  total step 4934/6200, training_loss = 0.2732\n",
      "epoch 80/100, step 37/62,  total step 4935/6200, training_loss = 0.2178\n",
      "epoch 80/100, step 38/62,  total step 4936/6200, training_loss = 0.2433\n",
      "epoch 80/100, step 39/62,  total step 4937/6200, training_loss = 0.2462\n",
      "epoch 80/100, step 40/62,  total step 4938/6200, training_loss = 0.2729\n",
      "epoch 80/100, step 41/62,  total step 4939/6200, training_loss = 0.1934\n",
      "epoch 80/100, step 42/62,  total step 4940/6200, training_loss = 0.2032\n",
      "epoch 80/100, step 43/62,  total step 4941/6200, training_loss = 0.2133\n",
      "epoch 80/100, step 44/62,  total step 4942/6200, training_loss = 0.2584\n",
      "epoch 80/100, step 45/62,  total step 4943/6200, training_loss = 0.1355\n",
      "epoch 80/100, step 46/62,  total step 4944/6200, training_loss = 0.1521\n",
      "epoch 80/100, step 47/62,  total step 4945/6200, training_loss = 0.2601\n",
      "epoch 80/100, step 48/62,  total step 4946/6200, training_loss = 0.1473\n",
      "epoch 80/100, step 49/62,  total step 4947/6200, training_loss = 0.1752\n",
      "epoch 80/100, step 50/62,  total step 4948/6200, training_loss = 0.2401\n",
      "epoch 80/100, step 51/62,  total step 4949/6200, training_loss = 0.2289\n",
      "epoch 80/100, step 52/62,  total step 4950/6200, training_loss = 0.1708\n",
      "epoch 80/100, step 53/62,  total step 4951/6200, training_loss = 0.2197\n",
      "epoch 80/100, step 54/62,  total step 4952/6200, training_loss = 0.1630\n",
      "epoch 80/100, step 55/62,  total step 4953/6200, training_loss = 0.1935\n",
      "epoch 80/100, step 56/62,  total step 4954/6200, training_loss = 0.2795\n",
      "epoch 80/100, step 57/62,  total step 4955/6200, training_loss = 0.2418\n",
      "epoch 80/100, step 58/62,  total step 4956/6200, training_loss = 0.3377\n",
      "epoch 80/100, step 59/62,  total step 4957/6200, training_loss = 0.1779\n",
      "epoch 80/100, step 60/62,  total step 4958/6200, training_loss = 0.3296\n",
      "epoch 80/100, step 61/62,  total step 4959/6200, training_loss = 0.1460\n",
      "epoch 80/100, step 62/62,  total step 4960/6200, training_loss = 0.2509 | avg loss: 0.2289 Dice Metric:   0.644\n",
      "epoch 81/100, step 1/62,  total step 4961/6200, training_loss = 0.2067\n",
      "epoch 81/100, step 2/62,  total step 4962/6200, training_loss = 0.1801\n",
      "epoch 81/100, step 3/62,  total step 4963/6200, training_loss = 0.2265\n",
      "epoch 81/100, step 4/62,  total step 4964/6200, training_loss = 0.3335\n",
      "epoch 81/100, step 5/62,  total step 4965/6200, training_loss = 0.1412\n",
      "epoch 81/100, step 6/62,  total step 4966/6200, training_loss = 0.2113\n",
      "epoch 81/100, step 7/62,  total step 4967/6200, training_loss = 0.1750\n",
      "epoch 81/100, step 8/62,  total step 4968/6200, training_loss = 0.2040\n",
      "epoch 81/100, step 9/62,  total step 4969/6200, training_loss = 0.1894\n",
      "epoch 81/100, step 10/62,  total step 4970/6200, training_loss = 0.2297\n",
      "epoch 81/100, step 11/62,  total step 4971/6200, training_loss = 0.1737\n",
      "epoch 81/100, step 12/62,  total step 4972/6200, training_loss = 0.1867\n",
      "epoch 81/100, step 13/62,  total step 4973/6200, training_loss = 0.1821\n",
      "epoch 81/100, step 14/62,  total step 4974/6200, training_loss = 0.2630\n",
      "epoch 81/100, step 15/62,  total step 4975/6200, training_loss = 0.3398\n",
      "epoch 81/100, step 16/62,  total step 4976/6200, training_loss = 0.3332\n",
      "epoch 81/100, step 17/62,  total step 4977/6200, training_loss = 0.1810\n",
      "epoch 81/100, step 18/62,  total step 4978/6200, training_loss = 0.2328\n",
      "epoch 81/100, step 19/62,  total step 4979/6200, training_loss = 0.2848\n",
      "epoch 81/100, step 20/62,  total step 4980/6200, training_loss = 0.2154\n",
      "epoch 81/100, step 21/62,  total step 4981/6200, training_loss = 0.2399\n",
      "epoch 81/100, step 22/62,  total step 4982/6200, training_loss = 0.3210\n",
      "epoch 81/100, step 23/62,  total step 4983/6200, training_loss = 0.2351\n",
      "epoch 81/100, step 24/62,  total step 4984/6200, training_loss = 0.2201\n",
      "epoch 81/100, step 25/62,  total step 4985/6200, training_loss = 0.2406\n",
      "epoch 81/100, step 26/62,  total step 4986/6200, training_loss = 0.1517\n",
      "epoch 81/100, step 27/62,  total step 4987/6200, training_loss = 0.2297\n",
      "epoch 81/100, step 28/62,  total step 4988/6200, training_loss = 0.3427\n",
      "epoch 81/100, step 29/62,  total step 4989/6200, training_loss = 0.2421\n",
      "epoch 81/100, step 30/62,  total step 4990/6200, training_loss = 0.2543\n",
      "epoch 81/100, step 31/62,  total step 4991/6200, training_loss = 0.1660\n",
      "epoch 81/100, step 32/62,  total step 4992/6200, training_loss = 0.2011\n",
      "epoch 81/100, step 33/62,  total step 4993/6200, training_loss = 0.2656\n",
      "epoch 81/100, step 34/62,  total step 4994/6200, training_loss = 0.2027\n",
      "epoch 81/100, step 35/62,  total step 4995/6200, training_loss = 0.2270\n",
      "epoch 81/100, step 36/62,  total step 4996/6200, training_loss = 0.3011\n",
      "epoch 81/100, step 37/62,  total step 4997/6200, training_loss = 0.2341\n",
      "epoch 81/100, step 38/62,  total step 4998/6200, training_loss = 0.2408\n",
      "epoch 81/100, step 39/62,  total step 4999/6200, training_loss = 0.2523\n",
      "epoch 81/100, step 40/62,  total step 5000/6200, training_loss = 0.2529\n",
      "epoch 81/100, step 41/62,  total step 5001/6200, training_loss = 0.1794\n",
      "epoch 81/100, step 42/62,  total step 5002/6200, training_loss = 0.2108\n",
      "epoch 81/100, step 43/62,  total step 5003/6200, training_loss = 0.2214\n",
      "epoch 81/100, step 44/62,  total step 5004/6200, training_loss = 0.2671\n",
      "epoch 81/100, step 45/62,  total step 5005/6200, training_loss = 0.1459\n",
      "epoch 81/100, step 46/62,  total step 5006/6200, training_loss = 0.1508\n",
      "epoch 81/100, step 47/62,  total step 5007/6200, training_loss = 0.2967\n",
      "epoch 81/100, step 48/62,  total step 5008/6200, training_loss = 0.1505\n",
      "epoch 81/100, step 49/62,  total step 5009/6200, training_loss = 0.1590\n",
      "epoch 81/100, step 50/62,  total step 5010/6200, training_loss = 0.2260\n",
      "epoch 81/100, step 51/62,  total step 5011/6200, training_loss = 0.2170\n",
      "epoch 81/100, step 52/62,  total step 5012/6200, training_loss = 0.1546\n",
      "epoch 81/100, step 53/62,  total step 5013/6200, training_loss = 0.2237\n",
      "epoch 81/100, step 54/62,  total step 5014/6200, training_loss = 0.1733\n",
      "epoch 81/100, step 55/62,  total step 5015/6200, training_loss = 0.1805\n",
      "epoch 81/100, step 56/62,  total step 5016/6200, training_loss = 0.2406\n",
      "epoch 81/100, step 57/62,  total step 5017/6200, training_loss = 0.2161\n",
      "epoch 81/100, step 58/62,  total step 5018/6200, training_loss = 0.3238\n",
      "epoch 81/100, step 59/62,  total step 5019/6200, training_loss = 0.1906\n",
      "epoch 81/100, step 60/62,  total step 5020/6200, training_loss = 0.2848\n",
      "epoch 81/100, step 61/62,  total step 5021/6200, training_loss = 0.1333\n",
      "epoch 81/100, step 62/62,  total step 5022/6200, training_loss = 0.1911 | avg loss: 0.2234 Dice Metric:   0.625\n",
      "epoch 82/100, step 1/62,  total step 5023/6200, training_loss = 0.1947\n",
      "epoch 82/100, step 2/62,  total step 5024/6200, training_loss = 0.1767\n",
      "epoch 82/100, step 3/62,  total step 5025/6200, training_loss = 0.2329\n",
      "epoch 82/100, step 4/62,  total step 5026/6200, training_loss = 0.3576\n",
      "epoch 82/100, step 5/62,  total step 5027/6200, training_loss = 0.1402\n",
      "epoch 82/100, step 6/62,  total step 5028/6200, training_loss = 0.2265\n",
      "epoch 82/100, step 7/62,  total step 5029/6200, training_loss = 0.1704\n",
      "epoch 82/100, step 8/62,  total step 5030/6200, training_loss = 0.2297\n",
      "epoch 82/100, step 9/62,  total step 5031/6200, training_loss = 0.1810\n",
      "epoch 82/100, step 10/62,  total step 5032/6200, training_loss = 0.2140\n",
      "epoch 82/100, step 11/62,  total step 5033/6200, training_loss = 0.1646\n",
      "epoch 82/100, step 12/62,  total step 5034/6200, training_loss = 0.1733\n",
      "epoch 82/100, step 13/62,  total step 5035/6200, training_loss = 0.2255\n",
      "epoch 82/100, step 14/62,  total step 5036/6200, training_loss = 0.2445\n",
      "epoch 82/100, step 15/62,  total step 5037/6200, training_loss = 0.3439\n",
      "epoch 82/100, step 16/62,  total step 5038/6200, training_loss = 0.2793\n",
      "epoch 82/100, step 17/62,  total step 5039/6200, training_loss = 0.1905\n",
      "epoch 82/100, step 18/62,  total step 5040/6200, training_loss = 0.2054\n",
      "epoch 82/100, step 19/62,  total step 5041/6200, training_loss = 0.2570\n",
      "epoch 82/100, step 20/62,  total step 5042/6200, training_loss = 0.2180\n",
      "epoch 82/100, step 21/62,  total step 5043/6200, training_loss = 0.2492\n",
      "epoch 82/100, step 22/62,  total step 5044/6200, training_loss = 0.3336\n",
      "epoch 82/100, step 23/62,  total step 5045/6200, training_loss = 0.2458\n",
      "epoch 82/100, step 24/62,  total step 5046/6200, training_loss = 0.1949\n",
      "epoch 82/100, step 25/62,  total step 5047/6200, training_loss = 0.2057\n",
      "epoch 82/100, step 26/62,  total step 5048/6200, training_loss = 0.1494\n",
      "epoch 82/100, step 27/62,  total step 5049/6200, training_loss = 0.2591\n",
      "epoch 82/100, step 28/62,  total step 5050/6200, training_loss = 0.3608\n",
      "epoch 82/100, step 29/62,  total step 5051/6200, training_loss = 0.2836\n",
      "epoch 82/100, step 30/62,  total step 5052/6200, training_loss = 0.2308\n",
      "epoch 82/100, step 31/62,  total step 5053/6200, training_loss = 0.1448\n",
      "epoch 82/100, step 32/62,  total step 5054/6200, training_loss = 0.2637\n",
      "epoch 82/100, step 33/62,  total step 5055/6200, training_loss = 0.3191\n",
      "epoch 82/100, step 34/62,  total step 5056/6200, training_loss = 0.1914\n",
      "epoch 82/100, step 35/62,  total step 5057/6200, training_loss = 0.2375\n",
      "epoch 82/100, step 36/62,  total step 5058/6200, training_loss = 0.2744\n",
      "epoch 82/100, step 37/62,  total step 5059/6200, training_loss = 0.1818\n",
      "epoch 82/100, step 38/62,  total step 5060/6200, training_loss = 0.2433\n",
      "epoch 82/100, step 39/62,  total step 5061/6200, training_loss = 0.2287\n",
      "epoch 82/100, step 40/62,  total step 5062/6200, training_loss = 0.2367\n",
      "epoch 82/100, step 41/62,  total step 5063/6200, training_loss = 0.2014\n",
      "epoch 82/100, step 42/62,  total step 5064/6200, training_loss = 0.2189\n",
      "epoch 82/100, step 43/62,  total step 5065/6200, training_loss = 0.2044\n",
      "epoch 82/100, step 44/62,  total step 5066/6200, training_loss = 0.2042\n",
      "epoch 82/100, step 45/62,  total step 5067/6200, training_loss = 0.1502\n",
      "epoch 82/100, step 46/62,  total step 5068/6200, training_loss = 0.1635\n",
      "epoch 82/100, step 47/62,  total step 5069/6200, training_loss = 0.2220\n",
      "epoch 82/100, step 48/62,  total step 5070/6200, training_loss = 0.1670\n",
      "epoch 82/100, step 49/62,  total step 5071/6200, training_loss = 0.1615\n",
      "epoch 82/100, step 50/62,  total step 5072/6200, training_loss = 0.1977\n",
      "epoch 82/100, step 51/62,  total step 5073/6200, training_loss = 0.2311\n",
      "epoch 82/100, step 52/62,  total step 5074/6200, training_loss = 0.1817\n",
      "epoch 82/100, step 53/62,  total step 5075/6200, training_loss = 0.2180\n",
      "epoch 82/100, step 54/62,  total step 5076/6200, training_loss = 0.1729\n",
      "epoch 82/100, step 55/62,  total step 5077/6200, training_loss = 0.1740\n",
      "epoch 82/100, step 56/62,  total step 5078/6200, training_loss = 0.2275\n",
      "epoch 82/100, step 57/62,  total step 5079/6200, training_loss = 0.2668\n",
      "epoch 82/100, step 58/62,  total step 5080/6200, training_loss = 0.3745\n",
      "epoch 82/100, step 59/62,  total step 5081/6200, training_loss = 0.1787\n",
      "epoch 82/100, step 60/62,  total step 5082/6200, training_loss = 0.2613\n",
      "epoch 82/100, step 61/62,  total step 5083/6200, training_loss = 0.1469\n",
      "epoch 82/100, step 62/62,  total step 5084/6200, training_loss = 0.1988 | avg loss: 0.2223 Dice Metric:   0.674\n",
      "epoch 83/100, step 1/62,  total step 5085/6200, training_loss = 0.1970\n",
      "epoch 83/100, step 2/62,  total step 5086/6200, training_loss = 0.1938\n",
      "epoch 83/100, step 3/62,  total step 5087/6200, training_loss = 0.2409\n",
      "epoch 83/100, step 4/62,  total step 5088/6200, training_loss = 0.3667\n",
      "epoch 83/100, step 5/62,  total step 5089/6200, training_loss = 0.1458\n",
      "epoch 83/100, step 6/62,  total step 5090/6200, training_loss = 0.1918\n",
      "epoch 83/100, step 7/62,  total step 5091/6200, training_loss = 0.1750\n",
      "epoch 83/100, step 8/62,  total step 5092/6200, training_loss = 0.2563\n",
      "epoch 83/100, step 9/62,  total step 5093/6200, training_loss = 0.1639\n",
      "epoch 83/100, step 10/62,  total step 5094/6200, training_loss = 0.1911\n",
      "epoch 83/100, step 11/62,  total step 5095/6200, training_loss = 0.1566\n",
      "epoch 83/100, step 12/62,  total step 5096/6200, training_loss = 0.1635\n",
      "epoch 83/100, step 13/62,  total step 5097/6200, training_loss = 0.1917\n",
      "epoch 83/100, step 14/62,  total step 5098/6200, training_loss = 0.2827\n",
      "epoch 83/100, step 15/62,  total step 5099/6200, training_loss = 0.3457\n",
      "epoch 83/100, step 16/62,  total step 5100/6200, training_loss = 0.3060\n",
      "epoch 83/100, step 17/62,  total step 5101/6200, training_loss = 0.2040\n",
      "epoch 83/100, step 18/62,  total step 5102/6200, training_loss = 0.1845\n",
      "epoch 83/100, step 19/62,  total step 5103/6200, training_loss = 0.2996\n",
      "epoch 83/100, step 20/62,  total step 5104/6200, training_loss = 0.2038\n",
      "epoch 83/100, step 21/62,  total step 5105/6200, training_loss = 0.2332\n",
      "epoch 83/100, step 22/62,  total step 5106/6200, training_loss = 0.3782\n",
      "epoch 83/100, step 23/62,  total step 5107/6200, training_loss = 0.2534\n",
      "epoch 83/100, step 24/62,  total step 5108/6200, training_loss = 0.1671\n",
      "epoch 83/100, step 25/62,  total step 5109/6200, training_loss = 0.2112\n",
      "epoch 83/100, step 26/62,  total step 5110/6200, training_loss = 0.1440\n",
      "epoch 83/100, step 27/62,  total step 5111/6200, training_loss = 0.2480\n",
      "epoch 83/100, step 28/62,  total step 5112/6200, training_loss = 0.3326\n",
      "epoch 83/100, step 29/62,  total step 5113/6200, training_loss = 0.2487\n",
      "epoch 83/100, step 30/62,  total step 5114/6200, training_loss = 0.2589\n",
      "epoch 83/100, step 31/62,  total step 5115/6200, training_loss = 0.1446\n",
      "epoch 83/100, step 32/62,  total step 5116/6200, training_loss = 0.1989\n",
      "epoch 83/100, step 33/62,  total step 5117/6200, training_loss = 0.2781\n",
      "epoch 83/100, step 34/62,  total step 5118/6200, training_loss = 0.2042\n",
      "epoch 83/100, step 35/62,  total step 5119/6200, training_loss = 0.2116\n",
      "epoch 83/100, step 36/62,  total step 5120/6200, training_loss = 0.2587\n",
      "epoch 83/100, step 37/62,  total step 5121/6200, training_loss = 0.1929\n",
      "epoch 83/100, step 38/62,  total step 5122/6200, training_loss = 0.2419\n",
      "epoch 83/100, step 39/62,  total step 5123/6200, training_loss = 0.2263\n",
      "epoch 83/100, step 40/62,  total step 5124/6200, training_loss = 0.2771\n",
      "epoch 83/100, step 41/62,  total step 5125/6200, training_loss = 0.2044\n",
      "epoch 83/100, step 42/62,  total step 5126/6200, training_loss = 0.2186\n",
      "epoch 83/100, step 43/62,  total step 5127/6200, training_loss = 0.1964\n",
      "epoch 83/100, step 44/62,  total step 5128/6200, training_loss = 0.2095\n",
      "epoch 83/100, step 45/62,  total step 5129/6200, training_loss = 0.1234\n",
      "epoch 83/100, step 46/62,  total step 5130/6200, training_loss = 0.1617\n",
      "epoch 83/100, step 47/62,  total step 5131/6200, training_loss = 0.2571\n",
      "epoch 83/100, step 48/62,  total step 5132/6200, training_loss = 0.1718\n",
      "epoch 83/100, step 49/62,  total step 5133/6200, training_loss = 0.1934\n",
      "epoch 83/100, step 50/62,  total step 5134/6200, training_loss = 0.2007\n",
      "epoch 83/100, step 51/62,  total step 5135/6200, training_loss = 0.2092\n",
      "epoch 83/100, step 52/62,  total step 5136/6200, training_loss = 0.1753\n",
      "epoch 83/100, step 53/62,  total step 5137/6200, training_loss = 0.2279\n",
      "epoch 83/100, step 54/62,  total step 5138/6200, training_loss = 0.1761\n",
      "epoch 83/100, step 55/62,  total step 5139/6200, training_loss = 0.1803\n",
      "epoch 83/100, step 56/62,  total step 5140/6200, training_loss = 0.2087\n",
      "epoch 83/100, step 57/62,  total step 5141/6200, training_loss = 0.2291\n",
      "epoch 83/100, step 58/62,  total step 5142/6200, training_loss = 0.3313\n",
      "epoch 83/100, step 59/62,  total step 5143/6200, training_loss = 0.1672\n",
      "epoch 83/100, step 60/62,  total step 5144/6200, training_loss = 0.2747\n",
      "epoch 83/100, step 61/62,  total step 5145/6200, training_loss = 0.1388\n",
      "epoch 83/100, step 62/62,  total step 5146/6200, training_loss = 0.2269 | avg loss: 0.2202 Dice Metric:   0.671\n",
      "epoch 84/100, step 1/62,  total step 5147/6200, training_loss = 0.2132\n",
      "epoch 84/100, step 2/62,  total step 5148/6200, training_loss = 0.1874\n",
      "epoch 84/100, step 3/62,  total step 5149/6200, training_loss = 0.2601\n",
      "epoch 84/100, step 4/62,  total step 5150/6200, training_loss = 0.3372\n",
      "epoch 84/100, step 5/62,  total step 5151/6200, training_loss = 0.1442\n",
      "epoch 84/100, step 6/62,  total step 5152/6200, training_loss = 0.2249\n",
      "epoch 84/100, step 7/62,  total step 5153/6200, training_loss = 0.1721\n",
      "epoch 84/100, step 8/62,  total step 5154/6200, training_loss = 0.2245\n",
      "epoch 84/100, step 9/62,  total step 5155/6200, training_loss = 0.1596\n",
      "epoch 84/100, step 10/62,  total step 5156/6200, training_loss = 0.1965\n",
      "epoch 84/100, step 11/62,  total step 5157/6200, training_loss = 0.1745\n",
      "epoch 84/100, step 12/62,  total step 5158/6200, training_loss = 0.1576\n",
      "epoch 84/100, step 13/62,  total step 5159/6200, training_loss = 0.1993\n",
      "epoch 84/100, step 14/62,  total step 5160/6200, training_loss = 0.2251\n",
      "epoch 84/100, step 15/62,  total step 5161/6200, training_loss = 0.3250\n",
      "epoch 84/100, step 16/62,  total step 5162/6200, training_loss = 0.3338\n",
      "epoch 84/100, step 17/62,  total step 5163/6200, training_loss = 0.1842\n",
      "epoch 84/100, step 18/62,  total step 5164/6200, training_loss = 0.1833\n",
      "epoch 84/100, step 19/62,  total step 5165/6200, training_loss = 0.2766\n",
      "epoch 84/100, step 20/62,  total step 5166/6200, training_loss = 0.2045\n",
      "epoch 84/100, step 21/62,  total step 5167/6200, training_loss = 0.2531\n",
      "epoch 84/100, step 22/62,  total step 5168/6200, training_loss = 0.3769\n",
      "epoch 84/100, step 23/62,  total step 5169/6200, training_loss = 0.2178\n",
      "epoch 84/100, step 24/62,  total step 5170/6200, training_loss = 0.1684\n",
      "epoch 84/100, step 25/62,  total step 5171/6200, training_loss = 0.2177\n",
      "epoch 84/100, step 26/62,  total step 5172/6200, training_loss = 0.1822\n",
      "epoch 84/100, step 27/62,  total step 5173/6200, training_loss = 0.2445\n",
      "epoch 84/100, step 28/62,  total step 5174/6200, training_loss = 0.2507\n",
      "epoch 84/100, step 29/62,  total step 5175/6200, training_loss = 0.2242\n",
      "epoch 84/100, step 30/62,  total step 5176/6200, training_loss = 0.2564\n",
      "epoch 84/100, step 31/62,  total step 5177/6200, training_loss = 0.1630\n",
      "epoch 84/100, step 32/62,  total step 5178/6200, training_loss = 0.2090\n",
      "epoch 84/100, step 33/62,  total step 5179/6200, training_loss = 0.2566\n",
      "epoch 84/100, step 34/62,  total step 5180/6200, training_loss = 0.2119\n",
      "epoch 84/100, step 35/62,  total step 5181/6200, training_loss = 0.2317\n",
      "epoch 84/100, step 36/62,  total step 5182/6200, training_loss = 0.2540\n",
      "epoch 84/100, step 37/62,  total step 5183/6200, training_loss = 0.2147\n",
      "epoch 84/100, step 38/62,  total step 5184/6200, training_loss = 0.2494\n",
      "epoch 84/100, step 39/62,  total step 5185/6200, training_loss = 0.2517\n",
      "epoch 84/100, step 40/62,  total step 5186/6200, training_loss = 0.2719\n",
      "epoch 84/100, step 41/62,  total step 5187/6200, training_loss = 0.1990\n",
      "epoch 84/100, step 42/62,  total step 5188/6200, training_loss = 0.2206\n",
      "epoch 84/100, step 43/62,  total step 5189/6200, training_loss = 0.1901\n",
      "epoch 84/100, step 44/62,  total step 5190/6200, training_loss = 0.2055\n",
      "epoch 84/100, step 45/62,  total step 5191/6200, training_loss = 0.1679\n",
      "epoch 84/100, step 46/62,  total step 5192/6200, training_loss = 0.1845\n",
      "epoch 84/100, step 47/62,  total step 5193/6200, training_loss = 0.2646\n",
      "epoch 84/100, step 48/62,  total step 5194/6200, training_loss = 0.1500\n",
      "epoch 84/100, step 49/62,  total step 5195/6200, training_loss = 0.1657\n",
      "epoch 84/100, step 50/62,  total step 5196/6200, training_loss = 0.2162\n",
      "epoch 84/100, step 51/62,  total step 5197/6200, training_loss = 0.2033\n",
      "epoch 84/100, step 52/62,  total step 5198/6200, training_loss = 0.1566\n",
      "epoch 84/100, step 53/62,  total step 5199/6200, training_loss = 0.2224\n",
      "epoch 84/100, step 54/62,  total step 5200/6200, training_loss = 0.1455\n",
      "epoch 84/100, step 55/62,  total step 5201/6200, training_loss = 0.1729\n",
      "epoch 84/100, step 56/62,  total step 5202/6200, training_loss = 0.2380\n",
      "epoch 84/100, step 57/62,  total step 5203/6200, training_loss = 0.2402\n",
      "epoch 84/100, step 58/62,  total step 5204/6200, training_loss = 0.3193\n",
      "epoch 84/100, step 59/62,  total step 5205/6200, training_loss = 0.1654\n",
      "epoch 84/100, step 60/62,  total step 5206/6200, training_loss = 0.2781\n",
      "epoch 84/100, step 61/62,  total step 5207/6200, training_loss = 0.1376\n",
      "epoch 84/100, step 62/62,  total step 5208/6200, training_loss = 0.1726 | avg loss: 0.2178 Dice Metric:    0.67\n",
      "epoch 85/100, step 1/62,  total step 5209/6200, training_loss = 0.2147\n",
      "epoch 85/100, step 2/62,  total step 5210/6200, training_loss = 0.1927\n",
      "epoch 85/100, step 3/62,  total step 5211/6200, training_loss = 0.2337\n",
      "epoch 85/100, step 4/62,  total step 5212/6200, training_loss = 0.3295\n",
      "epoch 85/100, step 5/62,  total step 5213/6200, training_loss = 0.1516\n",
      "epoch 85/100, step 6/62,  total step 5214/6200, training_loss = 0.2045\n",
      "epoch 85/100, step 7/62,  total step 5215/6200, training_loss = 0.1748\n",
      "epoch 85/100, step 8/62,  total step 5216/6200, training_loss = 0.2265\n",
      "epoch 85/100, step 9/62,  total step 5217/6200, training_loss = 0.1856\n",
      "epoch 85/100, step 10/62,  total step 5218/6200, training_loss = 0.2199\n",
      "epoch 85/100, step 11/62,  total step 5219/6200, training_loss = 0.1536\n",
      "epoch 85/100, step 12/62,  total step 5220/6200, training_loss = 0.1545\n",
      "epoch 85/100, step 13/62,  total step 5221/6200, training_loss = 0.1952\n",
      "epoch 85/100, step 14/62,  total step 5222/6200, training_loss = 0.2283\n",
      "epoch 85/100, step 15/62,  total step 5223/6200, training_loss = 0.3341\n",
      "epoch 85/100, step 16/62,  total step 5224/6200, training_loss = 0.3094\n",
      "epoch 85/100, step 17/62,  total step 5225/6200, training_loss = 0.1968\n",
      "epoch 85/100, step 18/62,  total step 5226/6200, training_loss = 0.1976\n",
      "epoch 85/100, step 19/62,  total step 5227/6200, training_loss = 0.2756\n",
      "epoch 85/100, step 20/62,  total step 5228/6200, training_loss = 0.2357\n",
      "epoch 85/100, step 21/62,  total step 5229/6200, training_loss = 0.2644\n",
      "epoch 85/100, step 22/62,  total step 5230/6200, training_loss = 0.3713\n",
      "epoch 85/100, step 23/62,  total step 5231/6200, training_loss = 0.2760\n",
      "epoch 85/100, step 24/62,  total step 5232/6200, training_loss = 0.1752\n",
      "epoch 85/100, step 25/62,  total step 5233/6200, training_loss = 0.2243\n",
      "epoch 85/100, step 26/62,  total step 5234/6200, training_loss = 0.1824\n",
      "epoch 85/100, step 27/62,  total step 5235/6200, training_loss = 0.1986\n",
      "epoch 85/100, step 28/62,  total step 5236/6200, training_loss = 0.3139\n",
      "epoch 85/100, step 29/62,  total step 5237/6200, training_loss = 0.2604\n",
      "epoch 85/100, step 30/62,  total step 5238/6200, training_loss = 0.2249\n",
      "epoch 85/100, step 31/62,  total step 5239/6200, training_loss = 0.1409\n",
      "epoch 85/100, step 32/62,  total step 5240/6200, training_loss = 0.2041\n",
      "epoch 85/100, step 33/62,  total step 5241/6200, training_loss = 0.2663\n",
      "epoch 85/100, step 34/62,  total step 5242/6200, training_loss = 0.2156\n",
      "epoch 85/100, step 35/62,  total step 5243/6200, training_loss = 0.2344\n",
      "epoch 85/100, step 36/62,  total step 5244/6200, training_loss = 0.2378\n",
      "epoch 85/100, step 37/62,  total step 5245/6200, training_loss = 0.2295\n",
      "epoch 85/100, step 38/62,  total step 5246/6200, training_loss = 0.2244\n",
      "epoch 85/100, step 39/62,  total step 5247/6200, training_loss = 0.2589\n",
      "epoch 85/100, step 40/62,  total step 5248/6200, training_loss = 0.2190\n",
      "epoch 85/100, step 41/62,  total step 5249/6200, training_loss = 0.1663\n",
      "epoch 85/100, step 42/62,  total step 5250/6200, training_loss = 0.2167\n",
      "epoch 85/100, step 43/62,  total step 5251/6200, training_loss = 0.2114\n",
      "epoch 85/100, step 44/62,  total step 5252/6200, training_loss = 0.2127\n",
      "epoch 85/100, step 45/62,  total step 5253/6200, training_loss = 0.1316\n",
      "epoch 85/100, step 46/62,  total step 5254/6200, training_loss = 0.1719\n",
      "epoch 85/100, step 47/62,  total step 5255/6200, training_loss = 0.2274\n",
      "epoch 85/100, step 48/62,  total step 5256/6200, training_loss = 0.1610\n",
      "epoch 85/100, step 49/62,  total step 5257/6200, training_loss = 0.1604\n",
      "epoch 85/100, step 50/62,  total step 5258/6200, training_loss = 0.2071\n",
      "epoch 85/100, step 51/62,  total step 5259/6200, training_loss = 0.1971\n",
      "epoch 85/100, step 52/62,  total step 5260/6200, training_loss = 0.1396\n",
      "epoch 85/100, step 53/62,  total step 5261/6200, training_loss = 0.2191\n",
      "epoch 85/100, step 54/62,  total step 5262/6200, training_loss = 0.1590\n",
      "epoch 85/100, step 55/62,  total step 5263/6200, training_loss = 0.1963\n",
      "epoch 85/100, step 56/62,  total step 5264/6200, training_loss = 0.2310\n",
      "epoch 85/100, step 57/62,  total step 5265/6200, training_loss = 0.2365\n",
      "epoch 85/100, step 58/62,  total step 5266/6200, training_loss = 0.3268\n",
      "epoch 85/100, step 59/62,  total step 5267/6200, training_loss = 0.1899\n",
      "epoch 85/100, step 60/62,  total step 5268/6200, training_loss = 0.2647\n",
      "epoch 85/100, step 61/62,  total step 5269/6200, training_loss = 0.1555\n",
      "epoch 85/100, step 62/62,  total step 5270/6200, training_loss = 0.1833 | avg loss: 0.2178 Dice Metric:   0.661\n",
      "epoch 86/100, step 1/62,  total step 5271/6200, training_loss = 0.2096\n",
      "epoch 86/100, step 2/62,  total step 5272/6200, training_loss = 0.1783\n",
      "epoch 86/100, step 3/62,  total step 5273/6200, training_loss = 0.2500\n",
      "epoch 86/100, step 4/62,  total step 5274/6200, training_loss = 0.3445\n",
      "epoch 86/100, step 5/62,  total step 5275/6200, training_loss = 0.1464\n",
      "epoch 86/100, step 6/62,  total step 5276/6200, training_loss = 0.2338\n",
      "epoch 86/100, step 7/62,  total step 5277/6200, training_loss = 0.1730\n",
      "epoch 86/100, step 8/62,  total step 5278/6200, training_loss = 0.2168\n",
      "epoch 86/100, step 9/62,  total step 5279/6200, training_loss = 0.1806\n",
      "epoch 86/100, step 10/62,  total step 5280/6200, training_loss = 0.2303\n",
      "epoch 86/100, step 11/62,  total step 5281/6200, training_loss = 0.1702\n",
      "epoch 86/100, step 12/62,  total step 5282/6200, training_loss = 0.1729\n",
      "epoch 86/100, step 13/62,  total step 5283/6200, training_loss = 0.1801\n",
      "epoch 86/100, step 14/62,  total step 5284/6200, training_loss = 0.2306\n",
      "epoch 86/100, step 15/62,  total step 5285/6200, training_loss = 0.3487\n",
      "epoch 86/100, step 16/62,  total step 5286/6200, training_loss = 0.3031\n",
      "epoch 86/100, step 17/62,  total step 5287/6200, training_loss = 0.1914\n",
      "epoch 86/100, step 18/62,  total step 5288/6200, training_loss = 0.1976\n",
      "epoch 86/100, step 19/62,  total step 5289/6200, training_loss = 0.3403\n",
      "epoch 86/100, step 20/62,  total step 5290/6200, training_loss = 0.2399\n",
      "epoch 86/100, step 21/62,  total step 5291/6200, training_loss = 0.2549\n",
      "epoch 86/100, step 22/62,  total step 5292/6200, training_loss = 0.3938\n",
      "epoch 86/100, step 23/62,  total step 5293/6200, training_loss = 0.2261\n",
      "epoch 86/100, step 24/62,  total step 5294/6200, training_loss = 0.1666\n",
      "epoch 86/100, step 25/62,  total step 5295/6200, training_loss = 0.2032\n",
      "epoch 86/100, step 26/62,  total step 5296/6200, training_loss = 0.1603\n",
      "epoch 86/100, step 27/62,  total step 5297/6200, training_loss = 0.2504\n",
      "epoch 86/100, step 28/62,  total step 5298/6200, training_loss = 0.2872\n",
      "epoch 86/100, step 29/62,  total step 5299/6200, training_loss = 0.2413\n",
      "epoch 86/100, step 30/62,  total step 5300/6200, training_loss = 0.2627\n",
      "epoch 86/100, step 31/62,  total step 5301/6200, training_loss = 0.1497\n",
      "epoch 86/100, step 32/62,  total step 5302/6200, training_loss = 0.1894\n",
      "epoch 86/100, step 33/62,  total step 5303/6200, training_loss = 0.2527\n",
      "epoch 86/100, step 34/62,  total step 5304/6200, training_loss = 0.1869\n",
      "epoch 86/100, step 35/62,  total step 5305/6200, training_loss = 0.2025\n",
      "epoch 86/100, step 36/62,  total step 5306/6200, training_loss = 0.2884\n",
      "epoch 86/100, step 37/62,  total step 5307/6200, training_loss = 0.1949\n",
      "epoch 86/100, step 38/62,  total step 5308/6200, training_loss = 0.2336\n",
      "epoch 86/100, step 39/62,  total step 5309/6200, training_loss = 0.2922\n",
      "epoch 86/100, step 40/62,  total step 5310/6200, training_loss = 0.2373\n",
      "epoch 86/100, step 41/62,  total step 5311/6200, training_loss = 0.1653\n",
      "epoch 86/100, step 42/62,  total step 5312/6200, training_loss = 0.1981\n",
      "epoch 86/100, step 43/62,  total step 5313/6200, training_loss = 0.2201\n",
      "epoch 86/100, step 44/62,  total step 5314/6200, training_loss = 0.2781\n",
      "epoch 86/100, step 45/62,  total step 5315/6200, training_loss = 0.1419\n",
      "epoch 86/100, step 46/62,  total step 5316/6200, training_loss = 0.1726\n",
      "epoch 86/100, step 47/62,  total step 5317/6200, training_loss = 0.2344\n",
      "epoch 86/100, step 48/62,  total step 5318/6200, training_loss = 0.1613\n",
      "epoch 86/100, step 49/62,  total step 5319/6200, training_loss = 0.1686\n",
      "epoch 86/100, step 50/62,  total step 5320/6200, training_loss = 0.2062\n",
      "epoch 86/100, step 51/62,  total step 5321/6200, training_loss = 0.2494\n",
      "epoch 86/100, step 52/62,  total step 5322/6200, training_loss = 0.1561\n",
      "epoch 86/100, step 53/62,  total step 5323/6200, training_loss = 0.2246\n",
      "epoch 86/100, step 54/62,  total step 5324/6200, training_loss = 0.1557\n",
      "epoch 86/100, step 55/62,  total step 5325/6200, training_loss = 0.1982\n",
      "epoch 86/100, step 56/62,  total step 5326/6200, training_loss = 0.2199\n",
      "epoch 86/100, step 57/62,  total step 5327/6200, training_loss = 0.2353\n",
      "epoch 86/100, step 58/62,  total step 5328/6200, training_loss = 0.3254\n",
      "epoch 86/100, step 59/62,  total step 5329/6200, training_loss = 0.1913\n",
      "epoch 86/100, step 60/62,  total step 5330/6200, training_loss = 0.2644\n",
      "epoch 86/100, step 61/62,  total step 5331/6200, training_loss = 0.1428\n",
      "epoch 86/100, step 62/62,  total step 5332/6200, training_loss = 0.1815 | avg loss: 0.2210 Dice Metric:   0.666\n",
      "epoch 87/100, step 1/62,  total step 5333/6200, training_loss = 0.2139\n",
      "epoch 87/100, step 2/62,  total step 5334/6200, training_loss = 0.1866\n",
      "epoch 87/100, step 3/62,  total step 5335/6200, training_loss = 0.2390\n",
      "epoch 87/100, step 4/62,  total step 5336/6200, training_loss = 0.3433\n",
      "epoch 87/100, step 5/62,  total step 5337/6200, training_loss = 0.1469\n",
      "epoch 87/100, step 6/62,  total step 5338/6200, training_loss = 0.1879\n",
      "epoch 87/100, step 7/62,  total step 5339/6200, training_loss = 0.1867\n",
      "epoch 87/100, step 8/62,  total step 5340/6200, training_loss = 0.2201\n",
      "epoch 87/100, step 9/62,  total step 5341/6200, training_loss = 0.1768\n",
      "epoch 87/100, step 10/62,  total step 5342/6200, training_loss = 0.1881\n",
      "epoch 87/100, step 11/62,  total step 5343/6200, training_loss = 0.1621\n",
      "epoch 87/100, step 12/62,  total step 5344/6200, training_loss = 0.1581\n",
      "epoch 87/100, step 13/62,  total step 5345/6200, training_loss = 0.2029\n",
      "epoch 87/100, step 14/62,  total step 5346/6200, training_loss = 0.2344\n",
      "epoch 87/100, step 15/62,  total step 5347/6200, training_loss = 0.3249\n",
      "epoch 87/100, step 16/62,  total step 5348/6200, training_loss = 0.3135\n",
      "epoch 87/100, step 17/62,  total step 5349/6200, training_loss = 0.1777\n",
      "epoch 87/100, step 18/62,  total step 5350/6200, training_loss = 0.1848\n",
      "epoch 87/100, step 19/62,  total step 5351/6200, training_loss = 0.2612\n",
      "epoch 87/100, step 20/62,  total step 5352/6200, training_loss = 0.2663\n",
      "epoch 87/100, step 21/62,  total step 5353/6200, training_loss = 0.2190\n",
      "epoch 87/100, step 22/62,  total step 5354/6200, training_loss = 0.3342\n",
      "epoch 87/100, step 23/62,  total step 5355/6200, training_loss = 0.2097\n",
      "epoch 87/100, step 24/62,  total step 5356/6200, training_loss = 0.1675\n",
      "epoch 87/100, step 25/62,  total step 5357/6200, training_loss = 0.2114\n",
      "epoch 87/100, step 26/62,  total step 5358/6200, training_loss = 0.1625\n",
      "epoch 87/100, step 27/62,  total step 5359/6200, training_loss = 0.2305\n",
      "epoch 87/100, step 28/62,  total step 5360/6200, training_loss = 0.3548\n",
      "epoch 87/100, step 29/62,  total step 5361/6200, training_loss = 0.2488\n",
      "epoch 87/100, step 30/62,  total step 5362/6200, training_loss = 0.2532\n",
      "epoch 87/100, step 31/62,  total step 5363/6200, training_loss = 0.1381\n",
      "epoch 87/100, step 32/62,  total step 5364/6200, training_loss = 0.1708\n",
      "epoch 87/100, step 33/62,  total step 5365/6200, training_loss = 0.2626\n",
      "epoch 87/100, step 34/62,  total step 5366/6200, training_loss = 0.2010\n",
      "epoch 87/100, step 35/62,  total step 5367/6200, training_loss = 0.1954\n",
      "epoch 87/100, step 36/62,  total step 5368/6200, training_loss = 0.2543\n",
      "epoch 87/100, step 37/62,  total step 5369/6200, training_loss = 0.2230\n",
      "epoch 87/100, step 38/62,  total step 5370/6200, training_loss = 0.2298\n",
      "epoch 87/100, step 39/62,  total step 5371/6200, training_loss = 0.2401\n",
      "epoch 87/100, step 40/62,  total step 5372/6200, training_loss = 0.2539\n",
      "epoch 87/100, step 41/62,  total step 5373/6200, training_loss = 0.1635\n",
      "epoch 87/100, step 42/62,  total step 5374/6200, training_loss = 0.2093\n",
      "epoch 87/100, step 43/62,  total step 5375/6200, training_loss = 0.2070\n",
      "epoch 87/100, step 44/62,  total step 5376/6200, training_loss = 0.2045\n",
      "epoch 87/100, step 45/62,  total step 5377/6200, training_loss = 0.1442\n",
      "epoch 87/100, step 46/62,  total step 5378/6200, training_loss = 0.1644\n",
      "epoch 87/100, step 47/62,  total step 5379/6200, training_loss = 0.2682\n",
      "epoch 87/100, step 48/62,  total step 5380/6200, training_loss = 0.1514\n",
      "epoch 87/100, step 49/62,  total step 5381/6200, training_loss = 0.1710\n",
      "epoch 87/100, step 50/62,  total step 5382/6200, training_loss = 0.1978\n",
      "epoch 87/100, step 51/62,  total step 5383/6200, training_loss = 0.2160\n",
      "epoch 87/100, step 52/62,  total step 5384/6200, training_loss = 0.1544\n",
      "epoch 87/100, step 53/62,  total step 5385/6200, training_loss = 0.1979\n",
      "epoch 87/100, step 54/62,  total step 5386/6200, training_loss = 0.1555\n",
      "epoch 87/100, step 55/62,  total step 5387/6200, training_loss = 0.1580\n",
      "epoch 87/100, step 56/62,  total step 5388/6200, training_loss = 0.2156\n",
      "epoch 87/100, step 57/62,  total step 5389/6200, training_loss = 0.2319\n",
      "epoch 87/100, step 58/62,  total step 5390/6200, training_loss = 0.3110\n",
      "epoch 87/100, step 59/62,  total step 5391/6200, training_loss = 0.2062\n",
      "epoch 87/100, step 60/62,  total step 5392/6200, training_loss = 0.2445\n",
      "epoch 87/100, step 61/62,  total step 5393/6200, training_loss = 0.1555\n",
      "epoch 87/100, step 62/62,  total step 5394/6200, training_loss = 0.2305 | avg loss: 0.2144 Dice Metric:   0.673\n",
      "epoch 88/100, step 1/62,  total step 5395/6200, training_loss = 0.1999\n",
      "epoch 88/100, step 2/62,  total step 5396/6200, training_loss = 0.1893\n",
      "epoch 88/100, step 3/62,  total step 5397/6200, training_loss = 0.2176\n",
      "epoch 88/100, step 4/62,  total step 5398/6200, training_loss = 0.3265\n",
      "epoch 88/100, step 5/62,  total step 5399/6200, training_loss = 0.1436\n",
      "epoch 88/100, step 6/62,  total step 5400/6200, training_loss = 0.1777\n",
      "epoch 88/100, step 7/62,  total step 5401/6200, training_loss = 0.1834\n",
      "epoch 88/100, step 8/62,  total step 5402/6200, training_loss = 0.2321\n",
      "epoch 88/100, step 9/62,  total step 5403/6200, training_loss = 0.1982\n",
      "epoch 88/100, step 10/62,  total step 5404/6200, training_loss = 0.1809\n",
      "epoch 88/100, step 11/62,  total step 5405/6200, training_loss = 0.1857\n",
      "epoch 88/100, step 12/62,  total step 5406/6200, training_loss = 0.1721\n",
      "epoch 88/100, step 13/62,  total step 5407/6200, training_loss = 0.1993\n",
      "epoch 88/100, step 14/62,  total step 5408/6200, training_loss = 0.2349\n",
      "epoch 88/100, step 15/62,  total step 5409/6200, training_loss = 0.3436\n",
      "epoch 88/100, step 16/62,  total step 5410/6200, training_loss = 0.2839\n",
      "epoch 88/100, step 17/62,  total step 5411/6200, training_loss = 0.1974\n",
      "epoch 88/100, step 18/62,  total step 5412/6200, training_loss = 0.2039\n",
      "epoch 88/100, step 19/62,  total step 5413/6200, training_loss = 0.2563\n",
      "epoch 88/100, step 20/62,  total step 5414/6200, training_loss = 0.2234\n",
      "epoch 88/100, step 21/62,  total step 5415/6200, training_loss = 0.2673\n",
      "epoch 88/100, step 22/62,  total step 5416/6200, training_loss = 0.3183\n",
      "epoch 88/100, step 23/62,  total step 5417/6200, training_loss = 0.2052\n",
      "epoch 88/100, step 24/62,  total step 5418/6200, training_loss = 0.1604\n",
      "epoch 88/100, step 25/62,  total step 5419/6200, training_loss = 0.2044\n",
      "epoch 88/100, step 26/62,  total step 5420/6200, training_loss = 0.1651\n",
      "epoch 88/100, step 27/62,  total step 5421/6200, training_loss = 0.2159\n",
      "epoch 88/100, step 28/62,  total step 5422/6200, training_loss = 0.3294\n",
      "epoch 88/100, step 29/62,  total step 5423/6200, training_loss = 0.2381\n",
      "epoch 88/100, step 30/62,  total step 5424/6200, training_loss = 0.2546\n",
      "epoch 88/100, step 31/62,  total step 5425/6200, training_loss = 0.1597\n",
      "epoch 88/100, step 32/62,  total step 5426/6200, training_loss = 0.1737\n",
      "epoch 88/100, step 33/62,  total step 5427/6200, training_loss = 0.2631\n",
      "epoch 88/100, step 34/62,  total step 5428/6200, training_loss = 0.2011\n",
      "epoch 88/100, step 35/62,  total step 5429/6200, training_loss = 0.2090\n",
      "epoch 88/100, step 36/62,  total step 5430/6200, training_loss = 0.2307\n",
      "epoch 88/100, step 37/62,  total step 5431/6200, training_loss = 0.1947\n",
      "epoch 88/100, step 38/62,  total step 5432/6200, training_loss = 0.2276\n",
      "epoch 88/100, step 39/62,  total step 5433/6200, training_loss = 0.2878\n",
      "epoch 88/100, step 40/62,  total step 5434/6200, training_loss = 0.2569\n",
      "epoch 88/100, step 41/62,  total step 5435/6200, training_loss = 0.2001\n",
      "epoch 88/100, step 42/62,  total step 5436/6200, training_loss = 0.2376\n",
      "epoch 88/100, step 43/62,  total step 5437/6200, training_loss = 0.1947\n",
      "epoch 88/100, step 44/62,  total step 5438/6200, training_loss = 0.2285\n",
      "epoch 88/100, step 45/62,  total step 5439/6200, training_loss = 0.1391\n",
      "epoch 88/100, step 46/62,  total step 5440/6200, training_loss = 0.2034\n",
      "epoch 88/100, step 47/62,  total step 5441/6200, training_loss = 0.2870\n",
      "epoch 88/100, step 48/62,  total step 5442/6200, training_loss = 0.1443\n",
      "epoch 88/100, step 49/62,  total step 5443/6200, training_loss = 0.1546\n",
      "epoch 88/100, step 50/62,  total step 5444/6200, training_loss = 0.1970\n",
      "epoch 88/100, step 51/62,  total step 5445/6200, training_loss = 0.1893\n",
      "epoch 88/100, step 52/62,  total step 5446/6200, training_loss = 0.1718\n",
      "epoch 88/100, step 53/62,  total step 5447/6200, training_loss = 0.2391\n",
      "epoch 88/100, step 54/62,  total step 5448/6200, training_loss = 0.1933\n",
      "epoch 88/100, step 55/62,  total step 5449/6200, training_loss = 0.1713\n",
      "epoch 88/100, step 56/62,  total step 5450/6200, training_loss = 0.2007\n",
      "epoch 88/100, step 57/62,  total step 5451/6200, training_loss = 0.2249\n",
      "epoch 88/100, step 58/62,  total step 5452/6200, training_loss = 0.3050\n",
      "epoch 88/100, step 59/62,  total step 5453/6200, training_loss = 0.1719\n",
      "epoch 88/100, step 60/62,  total step 5454/6200, training_loss = 0.2769\n",
      "epoch 88/100, step 61/62,  total step 5455/6200, training_loss = 0.1315\n",
      "epoch 88/100, step 62/62,  total step 5456/6200, training_loss = 0.1525 | avg loss: 0.2149 Dice Metric:   0.648\n",
      "epoch 89/100, step 1/62,  total step 5457/6200, training_loss = 0.2176\n",
      "epoch 89/100, step 2/62,  total step 5458/6200, training_loss = 0.1902\n",
      "epoch 89/100, step 3/62,  total step 5459/6200, training_loss = 0.2354\n",
      "epoch 89/100, step 4/62,  total step 5460/6200, training_loss = 0.3655\n",
      "epoch 89/100, step 5/62,  total step 5461/6200, training_loss = 0.1333\n",
      "epoch 89/100, step 6/62,  total step 5462/6200, training_loss = 0.2094\n",
      "epoch 89/100, step 7/62,  total step 5463/6200, training_loss = 0.1944\n",
      "epoch 89/100, step 8/62,  total step 5464/6200, training_loss = 0.2394\n",
      "epoch 89/100, step 9/62,  total step 5465/6200, training_loss = 0.1695\n",
      "epoch 89/100, step 10/62,  total step 5466/6200, training_loss = 0.2334\n",
      "epoch 89/100, step 11/62,  total step 5467/6200, training_loss = 0.1600\n",
      "epoch 89/100, step 12/62,  total step 5468/6200, training_loss = 0.1602\n",
      "epoch 89/100, step 13/62,  total step 5469/6200, training_loss = 0.2234\n",
      "epoch 89/100, step 14/62,  total step 5470/6200, training_loss = 0.2424\n",
      "epoch 89/100, step 15/62,  total step 5471/6200, training_loss = 0.3494\n",
      "epoch 89/100, step 16/62,  total step 5472/6200, training_loss = 0.3079\n",
      "epoch 89/100, step 17/62,  total step 5473/6200, training_loss = 0.1821\n",
      "epoch 89/100, step 18/62,  total step 5474/6200, training_loss = 0.1944\n",
      "epoch 89/100, step 19/62,  total step 5475/6200, training_loss = 0.2659\n",
      "epoch 89/100, step 20/62,  total step 5476/6200, training_loss = 0.2306\n",
      "epoch 89/100, step 21/62,  total step 5477/6200, training_loss = 0.2451\n",
      "epoch 89/100, step 22/62,  total step 5478/6200, training_loss = 0.3424\n",
      "epoch 89/100, step 23/62,  total step 5479/6200, training_loss = 0.2158\n",
      "epoch 89/100, step 24/62,  total step 5480/6200, training_loss = 0.1763\n",
      "epoch 89/100, step 25/62,  total step 5481/6200, training_loss = 0.2286\n",
      "epoch 89/100, step 26/62,  total step 5482/6200, training_loss = 0.1514\n",
      "epoch 89/100, step 27/62,  total step 5483/6200, training_loss = 0.2355\n",
      "epoch 89/100, step 28/62,  total step 5484/6200, training_loss = 0.2770\n",
      "epoch 89/100, step 29/62,  total step 5485/6200, training_loss = 0.2761\n",
      "epoch 89/100, step 30/62,  total step 5486/6200, training_loss = 0.2698\n",
      "epoch 89/100, step 31/62,  total step 5487/6200, training_loss = 0.1788\n",
      "epoch 89/100, step 32/62,  total step 5488/6200, training_loss = 0.2096\n",
      "epoch 89/100, step 33/62,  total step 5489/6200, training_loss = 0.2998\n",
      "epoch 89/100, step 34/62,  total step 5490/6200, training_loss = 0.2040\n",
      "epoch 89/100, step 35/62,  total step 5491/6200, training_loss = 0.2367\n",
      "epoch 89/100, step 36/62,  total step 5492/6200, training_loss = 0.2368\n",
      "epoch 89/100, step 37/62,  total step 5493/6200, training_loss = 0.2141\n",
      "epoch 89/100, step 38/62,  total step 5494/6200, training_loss = 0.2370\n",
      "epoch 89/100, step 39/62,  total step 5495/6200, training_loss = 0.2245\n",
      "epoch 89/100, step 40/62,  total step 5496/6200, training_loss = 0.2541\n",
      "epoch 89/100, step 41/62,  total step 5497/6200, training_loss = 0.1957\n",
      "epoch 89/100, step 42/62,  total step 5498/6200, training_loss = 0.1923\n",
      "epoch 89/100, step 43/62,  total step 5499/6200, training_loss = 0.2139\n",
      "epoch 89/100, step 44/62,  total step 5500/6200, training_loss = 0.2397\n",
      "epoch 89/100, step 45/62,  total step 5501/6200, training_loss = 0.1627\n",
      "epoch 89/100, step 46/62,  total step 5502/6200, training_loss = 0.1775\n",
      "epoch 89/100, step 47/62,  total step 5503/6200, training_loss = 0.2611\n",
      "epoch 89/100, step 48/62,  total step 5504/6200, training_loss = 0.1607\n",
      "epoch 89/100, step 49/62,  total step 5505/6200, training_loss = 0.1508\n",
      "epoch 89/100, step 50/62,  total step 5506/6200, training_loss = 0.2086\n",
      "epoch 89/100, step 51/62,  total step 5507/6200, training_loss = 0.2016\n",
      "epoch 89/100, step 52/62,  total step 5508/6200, training_loss = 0.1495\n",
      "epoch 89/100, step 53/62,  total step 5509/6200, training_loss = 0.2069\n",
      "epoch 89/100, step 54/62,  total step 5510/6200, training_loss = 0.1609\n",
      "epoch 89/100, step 55/62,  total step 5511/6200, training_loss = 0.1645\n",
      "epoch 89/100, step 56/62,  total step 5512/6200, training_loss = 0.2447\n",
      "epoch 89/100, step 57/62,  total step 5513/6200, training_loss = 0.2517\n",
      "epoch 89/100, step 58/62,  total step 5514/6200, training_loss = 0.3362\n",
      "epoch 89/100, step 59/62,  total step 5515/6200, training_loss = 0.1805\n",
      "epoch 89/100, step 60/62,  total step 5516/6200, training_loss = 0.2748\n",
      "epoch 89/100, step 61/62,  total step 5517/6200, training_loss = 0.1580\n",
      "epoch 89/100, step 62/62,  total step 5518/6200, training_loss = 0.1733 | avg loss: 0.2207 Dice Metric:   0.666\n",
      "epoch 90/100, step 1/62,  total step 5519/6200, training_loss = 0.2069\n",
      "epoch 90/100, step 2/62,  total step 5520/6200, training_loss = 0.1906\n",
      "epoch 90/100, step 3/62,  total step 5521/6200, training_loss = 0.2310\n",
      "epoch 90/100, step 4/62,  total step 5522/6200, training_loss = 0.3625\n",
      "epoch 90/100, step 5/62,  total step 5523/6200, training_loss = 0.1486\n",
      "epoch 90/100, step 6/62,  total step 5524/6200, training_loss = 0.2417\n",
      "epoch 90/100, step 7/62,  total step 5525/6200, training_loss = 0.1798\n",
      "epoch 90/100, step 8/62,  total step 5526/6200, training_loss = 0.2057\n",
      "epoch 90/100, step 9/62,  total step 5527/6200, training_loss = 0.1643\n",
      "epoch 90/100, step 10/62,  total step 5528/6200, training_loss = 0.2046\n",
      "epoch 90/100, step 11/62,  total step 5529/6200, training_loss = 0.1481\n",
      "epoch 90/100, step 12/62,  total step 5530/6200, training_loss = 0.2254\n",
      "epoch 90/100, step 13/62,  total step 5531/6200, training_loss = 0.2062\n",
      "epoch 90/100, step 14/62,  total step 5532/6200, training_loss = 0.2315\n",
      "epoch 90/100, step 15/62,  total step 5533/6200, training_loss = 0.3526\n",
      "epoch 90/100, step 16/62,  total step 5534/6200, training_loss = 0.2902\n",
      "epoch 90/100, step 17/62,  total step 5535/6200, training_loss = 0.1799\n",
      "epoch 90/100, step 18/62,  total step 5536/6200, training_loss = 0.2141\n",
      "epoch 90/100, step 19/62,  total step 5537/6200, training_loss = 0.2642\n",
      "epoch 90/100, step 20/62,  total step 5538/6200, training_loss = 0.2417\n",
      "epoch 90/100, step 21/62,  total step 5539/6200, training_loss = 0.2270\n",
      "epoch 90/100, step 22/62,  total step 5540/6200, training_loss = 0.2993\n",
      "epoch 90/100, step 23/62,  total step 5541/6200, training_loss = 0.2322\n",
      "epoch 90/100, step 24/62,  total step 5542/6200, training_loss = 0.1711\n",
      "epoch 90/100, step 25/62,  total step 5543/6200, training_loss = 0.1895\n",
      "epoch 90/100, step 26/62,  total step 5544/6200, training_loss = 0.1787\n",
      "epoch 90/100, step 27/62,  total step 5545/6200, training_loss = 0.2688\n",
      "epoch 90/100, step 28/62,  total step 5546/6200, training_loss = 0.3257\n",
      "epoch 90/100, step 29/62,  total step 5547/6200, training_loss = 0.2486\n",
      "epoch 90/100, step 30/62,  total step 5548/6200, training_loss = 0.2365\n",
      "epoch 90/100, step 31/62,  total step 5549/6200, training_loss = 0.1398\n",
      "epoch 90/100, step 32/62,  total step 5550/6200, training_loss = 0.2189\n",
      "epoch 90/100, step 33/62,  total step 5551/6200, training_loss = 0.2661\n",
      "epoch 90/100, step 34/62,  total step 5552/6200, training_loss = 0.2018\n",
      "epoch 90/100, step 35/62,  total step 5553/6200, training_loss = 0.1937\n",
      "epoch 90/100, step 36/62,  total step 5554/6200, training_loss = 0.2326\n",
      "epoch 90/100, step 37/62,  total step 5555/6200, training_loss = 0.1999\n",
      "epoch 90/100, step 38/62,  total step 5556/6200, training_loss = 0.2348\n",
      "epoch 90/100, step 39/62,  total step 5557/6200, training_loss = 0.2546\n",
      "epoch 90/100, step 40/62,  total step 5558/6200, training_loss = 0.2606\n",
      "epoch 90/100, step 41/62,  total step 5559/6200, training_loss = 0.1727\n",
      "epoch 90/100, step 42/62,  total step 5560/6200, training_loss = 0.2045\n",
      "epoch 90/100, step 43/62,  total step 5561/6200, training_loss = 0.2043\n",
      "epoch 90/100, step 44/62,  total step 5562/6200, training_loss = 0.2253\n",
      "epoch 90/100, step 45/62,  total step 5563/6200, training_loss = 0.1360\n",
      "epoch 90/100, step 46/62,  total step 5564/6200, training_loss = 0.1643\n",
      "epoch 90/100, step 47/62,  total step 5565/6200, training_loss = 0.2821\n",
      "epoch 90/100, step 48/62,  total step 5566/6200, training_loss = 0.1587\n",
      "epoch 90/100, step 49/62,  total step 5567/6200, training_loss = 0.1579\n",
      "epoch 90/100, step 50/62,  total step 5568/6200, training_loss = 0.2339\n",
      "epoch 90/100, step 51/62,  total step 5569/6200, training_loss = 0.2209\n",
      "epoch 90/100, step 52/62,  total step 5570/6200, training_loss = 0.1848\n",
      "epoch 90/100, step 53/62,  total step 5571/6200, training_loss = 0.2014\n",
      "epoch 90/100, step 54/62,  total step 5572/6200, training_loss = 0.1650\n",
      "epoch 90/100, step 55/62,  total step 5573/6200, training_loss = 0.1902\n",
      "epoch 90/100, step 56/62,  total step 5574/6200, training_loss = 0.2278\n",
      "epoch 90/100, step 57/62,  total step 5575/6200, training_loss = 0.2245\n",
      "epoch 90/100, step 58/62,  total step 5576/6200, training_loss = 0.2946\n",
      "epoch 90/100, step 59/62,  total step 5577/6200, training_loss = 0.1747\n",
      "epoch 90/100, step 60/62,  total step 5578/6200, training_loss = 0.2640\n",
      "epoch 90/100, step 61/62,  total step 5579/6200, training_loss = 0.1472\n",
      "epoch 90/100, step 62/62,  total step 5580/6200, training_loss = 0.1681 | avg loss: 0.2173 Dice Metric:   0.661\n",
      "epoch 91/100, step 1/62,  total step 5581/6200, training_loss = 0.2144\n",
      "epoch 91/100, step 2/62,  total step 5582/6200, training_loss = 0.1885\n",
      "epoch 91/100, step 3/62,  total step 5583/6200, training_loss = 0.2636\n",
      "epoch 91/100, step 4/62,  total step 5584/6200, training_loss = 0.3653\n",
      "epoch 91/100, step 5/62,  total step 5585/6200, training_loss = 0.1509\n",
      "epoch 91/100, step 6/62,  total step 5586/6200, training_loss = 0.1970\n",
      "epoch 91/100, step 7/62,  total step 5587/6200, training_loss = 0.2008\n",
      "epoch 91/100, step 8/62,  total step 5588/6200, training_loss = 0.2263\n",
      "epoch 91/100, step 9/62,  total step 5589/6200, training_loss = 0.1793\n",
      "epoch 91/100, step 10/62,  total step 5590/6200, training_loss = 0.2151\n",
      "epoch 91/100, step 11/62,  total step 5591/6200, training_loss = 0.1802\n",
      "epoch 91/100, step 12/62,  total step 5592/6200, training_loss = 0.1430\n",
      "epoch 91/100, step 13/62,  total step 5593/6200, training_loss = 0.2095\n",
      "epoch 91/100, step 14/62,  total step 5594/6200, training_loss = 0.2612\n",
      "epoch 91/100, step 15/62,  total step 5595/6200, training_loss = 0.3530\n",
      "epoch 91/100, step 16/62,  total step 5596/6200, training_loss = 0.3223\n",
      "epoch 91/100, step 17/62,  total step 5597/6200, training_loss = 0.2265\n",
      "epoch 91/100, step 18/62,  total step 5598/6200, training_loss = 0.1854\n",
      "epoch 91/100, step 19/62,  total step 5599/6200, training_loss = 0.2668\n",
      "epoch 91/100, step 20/62,  total step 5600/6200, training_loss = 0.2135\n",
      "epoch 91/100, step 21/62,  total step 5601/6200, training_loss = 0.2566\n",
      "epoch 91/100, step 22/62,  total step 5602/6200, training_loss = 0.3534\n",
      "epoch 91/100, step 23/62,  total step 5603/6200, training_loss = 0.2693\n",
      "epoch 91/100, step 24/62,  total step 5604/6200, training_loss = 0.1880\n",
      "epoch 91/100, step 25/62,  total step 5605/6200, training_loss = 0.2135\n",
      "epoch 91/100, step 26/62,  total step 5606/6200, training_loss = 0.1561\n",
      "epoch 91/100, step 27/62,  total step 5607/6200, training_loss = 0.2445\n",
      "epoch 91/100, step 28/62,  total step 5608/6200, training_loss = 0.2882\n",
      "epoch 91/100, step 29/62,  total step 5609/6200, training_loss = 0.2673\n",
      "epoch 91/100, step 30/62,  total step 5610/6200, training_loss = 0.2515\n",
      "epoch 91/100, step 31/62,  total step 5611/6200, training_loss = 0.1511\n",
      "epoch 91/100, step 32/62,  total step 5612/6200, training_loss = 0.2328\n",
      "epoch 91/100, step 33/62,  total step 5613/6200, training_loss = 0.2625\n",
      "epoch 91/100, step 34/62,  total step 5614/6200, training_loss = 0.1935\n",
      "epoch 91/100, step 35/62,  total step 5615/6200, training_loss = 0.2389\n",
      "epoch 91/100, step 36/62,  total step 5616/6200, training_loss = 0.2584\n",
      "epoch 91/100, step 37/62,  total step 5617/6200, training_loss = 0.2145\n",
      "epoch 91/100, step 38/62,  total step 5618/6200, training_loss = 0.2372\n",
      "epoch 91/100, step 39/62,  total step 5619/6200, training_loss = 0.2335\n",
      "epoch 91/100, step 40/62,  total step 5620/6200, training_loss = 0.2330\n",
      "epoch 91/100, step 41/62,  total step 5621/6200, training_loss = 0.1809\n",
      "epoch 91/100, step 42/62,  total step 5622/6200, training_loss = 0.2068\n",
      "epoch 91/100, step 43/62,  total step 5623/6200, training_loss = 0.1990\n",
      "epoch 91/100, step 44/62,  total step 5624/6200, training_loss = 0.2147\n",
      "epoch 91/100, step 45/62,  total step 5625/6200, training_loss = 0.1464\n",
      "epoch 91/100, step 46/62,  total step 5626/6200, training_loss = 0.1723\n",
      "epoch 91/100, step 47/62,  total step 5627/6200, training_loss = 0.2837\n",
      "epoch 91/100, step 48/62,  total step 5628/6200, training_loss = 0.1680\n",
      "epoch 91/100, step 49/62,  total step 5629/6200, training_loss = 0.1611\n",
      "epoch 91/100, step 50/62,  total step 5630/6200, training_loss = 0.2131\n",
      "epoch 91/100, step 51/62,  total step 5631/6200, training_loss = 0.2124\n",
      "epoch 91/100, step 52/62,  total step 5632/6200, training_loss = 0.1696\n",
      "epoch 91/100, step 53/62,  total step 5633/6200, training_loss = 0.2021\n",
      "epoch 91/100, step 54/62,  total step 5634/6200, training_loss = 0.1840\n",
      "epoch 91/100, step 55/62,  total step 5635/6200, training_loss = 0.1974\n",
      "epoch 91/100, step 56/62,  total step 5636/6200, training_loss = 0.2203\n",
      "epoch 91/100, step 57/62,  total step 5637/6200, training_loss = 0.2775\n",
      "epoch 91/100, step 58/62,  total step 5638/6200, training_loss = 0.3568\n",
      "epoch 91/100, step 59/62,  total step 5639/6200, training_loss = 0.1706\n",
      "epoch 91/100, step 60/62,  total step 5640/6200, training_loss = 0.2748\n",
      "epoch 91/100, step 61/62,  total step 5641/6200, training_loss = 0.1484\n",
      "epoch 91/100, step 62/62,  total step 5642/6200, training_loss = 0.2029 | avg loss: 0.2237 Dice Metric:    0.68\n",
      "epoch 92/100, step 1/62,  total step 5643/6200, training_loss = 0.2299\n",
      "epoch 92/100, step 2/62,  total step 5644/6200, training_loss = 0.1963\n",
      "epoch 92/100, step 3/62,  total step 5645/6200, training_loss = 0.2359\n",
      "epoch 92/100, step 4/62,  total step 5646/6200, training_loss = 0.3697\n",
      "epoch 92/100, step 5/62,  total step 5647/6200, training_loss = 0.1487\n",
      "epoch 92/100, step 6/62,  total step 5648/6200, training_loss = 0.2156\n",
      "epoch 92/100, step 7/62,  total step 5649/6200, training_loss = 0.1972\n",
      "epoch 92/100, step 8/62,  total step 5650/6200, training_loss = 0.2397\n",
      "epoch 92/100, step 9/62,  total step 5651/6200, training_loss = 0.1631\n",
      "epoch 92/100, step 10/62,  total step 5652/6200, training_loss = 0.2532\n",
      "epoch 92/100, step 11/62,  total step 5653/6200, training_loss = 0.1664\n",
      "epoch 92/100, step 12/62,  total step 5654/6200, training_loss = 0.1429\n",
      "epoch 92/100, step 13/62,  total step 5655/6200, training_loss = 0.2088\n",
      "epoch 92/100, step 14/62,  total step 5656/6200, training_loss = 0.2363\n",
      "epoch 92/100, step 15/62,  total step 5657/6200, training_loss = 0.3500\n",
      "epoch 92/100, step 16/62,  total step 5658/6200, training_loss = 0.3245\n",
      "epoch 92/100, step 17/62,  total step 5659/6200, training_loss = 0.2203\n",
      "epoch 92/100, step 18/62,  total step 5660/6200, training_loss = 0.1946\n",
      "epoch 92/100, step 19/62,  total step 5661/6200, training_loss = 0.2862\n",
      "epoch 92/100, step 20/62,  total step 5662/6200, training_loss = 0.2050\n",
      "epoch 92/100, step 21/62,  total step 5663/6200, training_loss = 0.2403\n",
      "epoch 92/100, step 22/62,  total step 5664/6200, training_loss = 0.3161\n",
      "epoch 92/100, step 23/62,  total step 5665/6200, training_loss = 0.2126\n",
      "epoch 92/100, step 24/62,  total step 5666/6200, training_loss = 0.1650\n",
      "epoch 92/100, step 25/62,  total step 5667/6200, training_loss = 0.1835\n",
      "epoch 92/100, step 26/62,  total step 5668/6200, training_loss = 0.1493\n",
      "epoch 92/100, step 27/62,  total step 5669/6200, training_loss = 0.2367\n",
      "epoch 92/100, step 28/62,  total step 5670/6200, training_loss = 0.3056\n",
      "epoch 92/100, step 29/62,  total step 5671/6200, training_loss = 0.2788\n",
      "epoch 92/100, step 30/62,  total step 5672/6200, training_loss = 0.2238\n",
      "epoch 92/100, step 31/62,  total step 5673/6200, training_loss = 0.1451\n",
      "epoch 92/100, step 32/62,  total step 5674/6200, training_loss = 0.1665\n",
      "epoch 92/100, step 33/62,  total step 5675/6200, training_loss = 0.2551\n",
      "epoch 92/100, step 34/62,  total step 5676/6200, training_loss = 0.1808\n",
      "epoch 92/100, step 35/62,  total step 5677/6200, training_loss = 0.2182\n",
      "epoch 92/100, step 36/62,  total step 5678/6200, training_loss = 0.2420\n",
      "epoch 92/100, step 37/62,  total step 5679/6200, training_loss = 0.1886\n",
      "epoch 92/100, step 38/62,  total step 5680/6200, training_loss = 0.2342\n",
      "epoch 92/100, step 39/62,  total step 5681/6200, training_loss = 0.2104\n",
      "epoch 92/100, step 40/62,  total step 5682/6200, training_loss = 0.3022\n",
      "epoch 92/100, step 41/62,  total step 5683/6200, training_loss = 0.1673\n",
      "epoch 92/100, step 42/62,  total step 5684/6200, training_loss = 0.2164\n",
      "epoch 92/100, step 43/62,  total step 5685/6200, training_loss = 0.1959\n",
      "epoch 92/100, step 44/62,  total step 5686/6200, training_loss = 0.2342\n",
      "epoch 92/100, step 45/62,  total step 5687/6200, training_loss = 0.1255\n",
      "epoch 92/100, step 46/62,  total step 5688/6200, training_loss = 0.1534\n",
      "epoch 92/100, step 47/62,  total step 5689/6200, training_loss = 0.2671\n",
      "epoch 92/100, step 48/62,  total step 5690/6200, training_loss = 0.1660\n",
      "epoch 92/100, step 49/62,  total step 5691/6200, training_loss = 0.1549\n",
      "epoch 92/100, step 50/62,  total step 5692/6200, training_loss = 0.1994\n",
      "epoch 92/100, step 51/62,  total step 5693/6200, training_loss = 0.2075\n",
      "epoch 92/100, step 52/62,  total step 5694/6200, training_loss = 0.1678\n",
      "epoch 92/100, step 53/62,  total step 5695/6200, training_loss = 0.2243\n",
      "epoch 92/100, step 54/62,  total step 5696/6200, training_loss = 0.1340\n",
      "epoch 92/100, step 55/62,  total step 5697/6200, training_loss = 0.1866\n",
      "epoch 92/100, step 56/62,  total step 5698/6200, training_loss = 0.2530\n",
      "epoch 92/100, step 57/62,  total step 5699/6200, training_loss = 0.2525\n",
      "epoch 92/100, step 58/62,  total step 5700/6200, training_loss = 0.3287\n",
      "epoch 92/100, step 59/62,  total step 5701/6200, training_loss = 0.1950\n",
      "epoch 92/100, step 60/62,  total step 5702/6200, training_loss = 0.2847\n",
      "epoch 92/100, step 61/62,  total step 5703/6200, training_loss = 0.1488\n",
      "epoch 92/100, step 62/62,  total step 5704/6200, training_loss = 0.2402 | avg loss: 0.2184 Dice Metric:   0.636\n",
      "epoch 93/100, step 1/62,  total step 5705/6200, training_loss = 0.2386\n",
      "epoch 93/100, step 2/62,  total step 5706/6200, training_loss = 0.2000\n",
      "epoch 93/100, step 3/62,  total step 5707/6200, training_loss = 0.2598\n",
      "epoch 93/100, step 4/62,  total step 5708/6200, training_loss = 0.3910\n",
      "epoch 93/100, step 5/62,  total step 5709/6200, training_loss = 0.1447\n",
      "epoch 93/100, step 6/62,  total step 5710/6200, training_loss = 0.2178\n",
      "epoch 93/100, step 7/62,  total step 5711/6200, training_loss = 0.1717\n",
      "epoch 93/100, step 8/62,  total step 5712/6200, training_loss = 0.2410\n",
      "epoch 93/100, step 9/62,  total step 5713/6200, training_loss = 0.1707\n",
      "epoch 93/100, step 10/62,  total step 5714/6200, training_loss = 0.1979\n",
      "epoch 93/100, step 11/62,  total step 5715/6200, training_loss = 0.1510\n",
      "epoch 93/100, step 12/62,  total step 5716/6200, training_loss = 0.1569\n",
      "epoch 93/100, step 13/62,  total step 5717/6200, training_loss = 0.2076\n",
      "epoch 93/100, step 14/62,  total step 5718/6200, training_loss = 0.2438\n",
      "epoch 93/100, step 15/62,  total step 5719/6200, training_loss = 0.3402\n",
      "epoch 93/100, step 16/62,  total step 5720/6200, training_loss = 0.3018\n",
      "epoch 93/100, step 17/62,  total step 5721/6200, training_loss = 0.1812\n",
      "epoch 93/100, step 18/62,  total step 5722/6200, training_loss = 0.1838\n",
      "epoch 93/100, step 19/62,  total step 5723/6200, training_loss = 0.2645\n",
      "epoch 93/100, step 20/62,  total step 5724/6200, training_loss = 0.2077\n",
      "epoch 93/100, step 21/62,  total step 5725/6200, training_loss = 0.2576\n",
      "epoch 93/100, step 22/62,  total step 5726/6200, training_loss = 0.3702\n",
      "epoch 93/100, step 23/62,  total step 5727/6200, training_loss = 0.2038\n",
      "epoch 93/100, step 24/62,  total step 5728/6200, training_loss = 0.1798\n",
      "epoch 93/100, step 25/62,  total step 5729/6200, training_loss = 0.2043\n",
      "epoch 93/100, step 26/62,  total step 5730/6200, training_loss = 0.1639\n",
      "epoch 93/100, step 27/62,  total step 5731/6200, training_loss = 0.2224\n",
      "epoch 93/100, step 28/62,  total step 5732/6200, training_loss = 0.3181\n",
      "epoch 93/100, step 29/62,  total step 5733/6200, training_loss = 0.2400\n",
      "epoch 93/100, step 30/62,  total step 5734/6200, training_loss = 0.2318\n",
      "epoch 93/100, step 31/62,  total step 5735/6200, training_loss = 0.1560\n",
      "epoch 93/100, step 32/62,  total step 5736/6200, training_loss = 0.2000\n",
      "epoch 93/100, step 33/62,  total step 5737/6200, training_loss = 0.2675\n",
      "epoch 93/100, step 34/62,  total step 5738/6200, training_loss = 0.1944\n",
      "epoch 93/100, step 35/62,  total step 5739/6200, training_loss = 0.2562\n",
      "epoch 93/100, step 36/62,  total step 5740/6200, training_loss = 0.2599\n",
      "epoch 93/100, step 37/62,  total step 5741/6200, training_loss = 0.1908\n",
      "epoch 93/100, step 38/62,  total step 5742/6200, training_loss = 0.2359\n",
      "epoch 93/100, step 39/62,  total step 5743/6200, training_loss = 0.2274\n",
      "epoch 93/100, step 40/62,  total step 5744/6200, training_loss = 0.2449\n",
      "epoch 93/100, step 41/62,  total step 5745/6200, training_loss = 0.1595\n",
      "epoch 93/100, step 42/62,  total step 5746/6200, training_loss = 0.2159\n",
      "epoch 93/100, step 43/62,  total step 5747/6200, training_loss = 0.1933\n",
      "epoch 93/100, step 44/62,  total step 5748/6200, training_loss = 0.2086\n",
      "epoch 93/100, step 45/62,  total step 5749/6200, training_loss = 0.1375\n",
      "epoch 93/100, step 46/62,  total step 5750/6200, training_loss = 0.1600\n",
      "epoch 93/100, step 47/62,  total step 5751/6200, training_loss = 0.2452\n",
      "epoch 93/100, step 48/62,  total step 5752/6200, training_loss = 0.1362\n",
      "epoch 93/100, step 49/62,  total step 5753/6200, training_loss = 0.1529\n",
      "epoch 93/100, step 50/62,  total step 5754/6200, training_loss = 0.2054\n",
      "epoch 93/100, step 51/62,  total step 5755/6200, training_loss = 0.1921\n",
      "epoch 93/100, step 52/62,  total step 5756/6200, training_loss = 0.1647\n",
      "epoch 93/100, step 53/62,  total step 5757/6200, training_loss = 0.2016\n",
      "epoch 93/100, step 54/62,  total step 5758/6200, training_loss = 0.1804\n",
      "epoch 93/100, step 55/62,  total step 5759/6200, training_loss = 0.1529\n",
      "epoch 93/100, step 56/62,  total step 5760/6200, training_loss = 0.2235\n",
      "epoch 93/100, step 57/62,  total step 5761/6200, training_loss = 0.2127\n",
      "epoch 93/100, step 58/62,  total step 5762/6200, training_loss = 0.3027\n",
      "epoch 93/100, step 59/62,  total step 5763/6200, training_loss = 0.1777\n",
      "epoch 93/100, step 60/62,  total step 5764/6200, training_loss = 0.2515\n",
      "epoch 93/100, step 61/62,  total step 5765/6200, training_loss = 0.1400\n",
      "epoch 93/100, step 62/62,  total step 5766/6200, training_loss = 0.1851 | avg loss: 0.2144 Dice Metric:   0.665\n",
      "epoch 94/100, step 1/62,  total step 5767/6200, training_loss = 0.1918\n",
      "epoch 94/100, step 2/62,  total step 5768/6200, training_loss = 0.1749\n",
      "epoch 94/100, step 3/62,  total step 5769/6200, training_loss = 0.2199\n",
      "epoch 94/100, step 4/62,  total step 5770/6200, training_loss = 0.3231\n",
      "epoch 94/100, step 5/62,  total step 5771/6200, training_loss = 0.1364\n",
      "epoch 94/100, step 6/62,  total step 5772/6200, training_loss = 0.2220\n",
      "epoch 94/100, step 7/62,  total step 5773/6200, training_loss = 0.1881\n",
      "epoch 94/100, step 8/62,  total step 5774/6200, training_loss = 0.2390\n",
      "epoch 94/100, step 9/62,  total step 5775/6200, training_loss = 0.1756\n",
      "epoch 94/100, step 10/62,  total step 5776/6200, training_loss = 0.2243\n",
      "epoch 94/100, step 11/62,  total step 5777/6200, training_loss = 0.1594\n",
      "epoch 94/100, step 12/62,  total step 5778/6200, training_loss = 0.1699\n",
      "epoch 94/100, step 13/62,  total step 5779/6200, training_loss = 0.2081\n",
      "epoch 94/100, step 14/62,  total step 5780/6200, training_loss = 0.2077\n",
      "epoch 94/100, step 15/62,  total step 5781/6200, training_loss = 0.3252\n",
      "epoch 94/100, step 16/62,  total step 5782/6200, training_loss = 0.2936\n",
      "epoch 94/100, step 17/62,  total step 5783/6200, training_loss = 0.1823\n",
      "epoch 94/100, step 18/62,  total step 5784/6200, training_loss = 0.1829\n",
      "epoch 94/100, step 19/62,  total step 5785/6200, training_loss = 0.3044\n",
      "epoch 94/100, step 20/62,  total step 5786/6200, training_loss = 0.2132\n",
      "epoch 94/100, step 21/62,  total step 5787/6200, training_loss = 0.2479\n",
      "epoch 94/100, step 22/62,  total step 5788/6200, training_loss = 0.3007\n",
      "epoch 94/100, step 23/62,  total step 5789/6200, training_loss = 0.2315\n",
      "epoch 94/100, step 24/62,  total step 5790/6200, training_loss = 0.1609\n",
      "epoch 94/100, step 25/62,  total step 5791/6200, training_loss = 0.2146\n",
      "epoch 94/100, step 26/62,  total step 5792/6200, training_loss = 0.1798\n",
      "epoch 94/100, step 27/62,  total step 5793/6200, training_loss = 0.2264\n",
      "epoch 94/100, step 28/62,  total step 5794/6200, training_loss = 0.2796\n",
      "epoch 94/100, step 29/62,  total step 5795/6200, training_loss = 0.2385\n",
      "epoch 94/100, step 30/62,  total step 5796/6200, training_loss = 0.2425\n",
      "epoch 94/100, step 31/62,  total step 5797/6200, training_loss = 0.1647\n",
      "epoch 94/100, step 32/62,  total step 5798/6200, training_loss = 0.1661\n",
      "epoch 94/100, step 33/62,  total step 5799/6200, training_loss = 0.2763\n",
      "epoch 94/100, step 34/62,  total step 5800/6200, training_loss = 0.1872\n",
      "epoch 94/100, step 35/62,  total step 5801/6200, training_loss = 0.2268\n",
      "epoch 94/100, step 36/62,  total step 5802/6200, training_loss = 0.2803\n",
      "epoch 94/100, step 37/62,  total step 5803/6200, training_loss = 0.2256\n",
      "epoch 94/100, step 38/62,  total step 5804/6200, training_loss = 0.2479\n",
      "epoch 94/100, step 39/62,  total step 5805/6200, training_loss = 0.2517\n",
      "epoch 94/100, step 40/62,  total step 5806/6200, training_loss = 0.2618\n",
      "epoch 94/100, step 41/62,  total step 5807/6200, training_loss = 0.1676\n",
      "epoch 94/100, step 42/62,  total step 5808/6200, training_loss = 0.2187\n",
      "epoch 94/100, step 43/62,  total step 5809/6200, training_loss = 0.2132\n",
      "epoch 94/100, step 44/62,  total step 5810/6200, training_loss = 0.2073\n",
      "epoch 94/100, step 45/62,  total step 5811/6200, training_loss = 0.1284\n",
      "epoch 94/100, step 46/62,  total step 5812/6200, training_loss = 0.1671\n",
      "epoch 94/100, step 47/62,  total step 5813/6200, training_loss = 0.2898\n",
      "epoch 94/100, step 48/62,  total step 5814/6200, training_loss = 0.1497\n",
      "epoch 94/100, step 49/62,  total step 5815/6200, training_loss = 0.1434\n",
      "epoch 94/100, step 50/62,  total step 5816/6200, training_loss = 0.2016\n",
      "epoch 94/100, step 51/62,  total step 5817/6200, training_loss = 0.2164\n",
      "epoch 94/100, step 52/62,  total step 5818/6200, training_loss = 0.1565\n",
      "epoch 94/100, step 53/62,  total step 5819/6200, training_loss = 0.1921\n",
      "epoch 94/100, step 54/62,  total step 5820/6200, training_loss = 0.1480\n",
      "epoch 94/100, step 55/62,  total step 5821/6200, training_loss = 0.1778\n",
      "epoch 94/100, step 56/62,  total step 5822/6200, training_loss = 0.2204\n",
      "epoch 94/100, step 57/62,  total step 5823/6200, training_loss = 0.2184\n",
      "epoch 94/100, step 58/62,  total step 5824/6200, training_loss = 0.3048\n",
      "epoch 94/100, step 59/62,  total step 5825/6200, training_loss = 0.2071\n",
      "epoch 94/100, step 60/62,  total step 5826/6200, training_loss = 0.2801\n",
      "epoch 94/100, step 61/62,  total step 5827/6200, training_loss = 0.1323\n",
      "epoch 94/100, step 62/62,  total step 5828/6200, training_loss = 0.2832 | avg loss: 0.2158 Dice Metric:   0.669\n",
      "epoch 95/100, step 1/62,  total step 5829/6200, training_loss = 0.2069\n",
      "epoch 95/100, step 2/62,  total step 5830/6200, training_loss = 0.1745\n",
      "epoch 95/100, step 3/62,  total step 5831/6200, training_loss = 0.2291\n",
      "epoch 95/100, step 4/62,  total step 5832/6200, training_loss = 0.3434\n",
      "epoch 95/100, step 5/62,  total step 5833/6200, training_loss = 0.1446\n",
      "epoch 95/100, step 6/62,  total step 5834/6200, training_loss = 0.2135\n",
      "epoch 95/100, step 7/62,  total step 5835/6200, training_loss = 0.1688\n",
      "epoch 95/100, step 8/62,  total step 5836/6200, training_loss = 0.2245\n",
      "epoch 95/100, step 9/62,  total step 5837/6200, training_loss = 0.1636\n",
      "epoch 95/100, step 10/62,  total step 5838/6200, training_loss = 0.1892\n",
      "epoch 95/100, step 11/62,  total step 5839/6200, training_loss = 0.1502\n",
      "epoch 95/100, step 12/62,  total step 5840/6200, training_loss = 0.1512\n",
      "epoch 95/100, step 13/62,  total step 5841/6200, training_loss = 0.2046\n",
      "epoch 95/100, step 14/62,  total step 5842/6200, training_loss = 0.2517\n",
      "epoch 95/100, step 15/62,  total step 5843/6200, training_loss = 0.3442\n",
      "epoch 95/100, step 16/62,  total step 5844/6200, training_loss = 0.3126\n",
      "epoch 95/100, step 17/62,  total step 5845/6200, training_loss = 0.2003\n",
      "epoch 95/100, step 18/62,  total step 5846/6200, training_loss = 0.2232\n",
      "epoch 95/100, step 19/62,  total step 5847/6200, training_loss = 0.2592\n",
      "epoch 95/100, step 20/62,  total step 5848/6200, training_loss = 0.2221\n",
      "epoch 95/100, step 21/62,  total step 5849/6200, training_loss = 0.2270\n",
      "epoch 95/100, step 22/62,  total step 5850/6200, training_loss = 0.3413\n",
      "epoch 95/100, step 23/62,  total step 5851/6200, training_loss = 0.2434\n",
      "epoch 95/100, step 24/62,  total step 5852/6200, training_loss = 0.1692\n",
      "epoch 95/100, step 25/62,  total step 5853/6200, training_loss = 0.2007\n",
      "epoch 95/100, step 26/62,  total step 5854/6200, training_loss = 0.1518\n",
      "epoch 95/100, step 27/62,  total step 5855/6200, training_loss = 0.2746\n",
      "epoch 95/100, step 28/62,  total step 5856/6200, training_loss = 0.2958\n",
      "epoch 95/100, step 29/62,  total step 5857/6200, training_loss = 0.2432\n",
      "epoch 95/100, step 30/62,  total step 5858/6200, training_loss = 0.2237\n",
      "epoch 95/100, step 31/62,  total step 5859/6200, training_loss = 0.1417\n",
      "epoch 95/100, step 32/62,  total step 5860/6200, training_loss = 0.2093\n",
      "epoch 95/100, step 33/62,  total step 5861/6200, training_loss = 0.2499\n",
      "epoch 95/100, step 34/62,  total step 5862/6200, training_loss = 0.1774\n",
      "epoch 95/100, step 35/62,  total step 5863/6200, training_loss = 0.1919\n",
      "epoch 95/100, step 36/62,  total step 5864/6200, training_loss = 0.2371\n",
      "epoch 95/100, step 37/62,  total step 5865/6200, training_loss = 0.2145\n",
      "epoch 95/100, step 38/62,  total step 5866/6200, training_loss = 0.2262\n",
      "epoch 95/100, step 39/62,  total step 5867/6200, training_loss = 0.2419\n",
      "epoch 95/100, step 40/62,  total step 5868/6200, training_loss = 0.2460\n",
      "epoch 95/100, step 41/62,  total step 5869/6200, training_loss = 0.1678\n",
      "epoch 95/100, step 42/62,  total step 5870/6200, training_loss = 0.2191\n",
      "epoch 95/100, step 43/62,  total step 5871/6200, training_loss = 0.2062\n",
      "epoch 95/100, step 44/62,  total step 5872/6200, training_loss = 0.2473\n",
      "epoch 95/100, step 45/62,  total step 5873/6200, training_loss = 0.1497\n",
      "epoch 95/100, step 46/62,  total step 5874/6200, training_loss = 0.1727\n",
      "epoch 95/100, step 47/62,  total step 5875/6200, training_loss = 0.1925\n",
      "epoch 95/100, step 48/62,  total step 5876/6200, training_loss = 0.1320\n",
      "epoch 95/100, step 49/62,  total step 5877/6200, training_loss = 0.1500\n",
      "epoch 95/100, step 50/62,  total step 5878/6200, training_loss = 0.2072\n",
      "epoch 95/100, step 51/62,  total step 5879/6200, training_loss = 0.2413\n",
      "epoch 95/100, step 52/62,  total step 5880/6200, training_loss = 0.1753\n",
      "epoch 95/100, step 53/62,  total step 5881/6200, training_loss = 0.1935\n",
      "epoch 95/100, step 54/62,  total step 5882/6200, training_loss = 0.1548\n",
      "epoch 95/100, step 55/62,  total step 5883/6200, training_loss = 0.1861\n",
      "epoch 95/100, step 56/62,  total step 5884/6200, training_loss = 0.2351\n",
      "epoch 95/100, step 57/62,  total step 5885/6200, training_loss = 0.2566\n",
      "epoch 95/100, step 58/62,  total step 5886/6200, training_loss = 0.3365\n",
      "epoch 95/100, step 59/62,  total step 5887/6200, training_loss = 0.1863\n",
      "epoch 95/100, step 60/62,  total step 5888/6200, training_loss = 0.2467\n",
      "epoch 95/100, step 61/62,  total step 5889/6200, training_loss = 0.1415\n",
      "epoch 95/100, step 62/62,  total step 5890/6200, training_loss = 0.1826 | avg loss: 0.2140 Dice Metric:   0.635\n",
      "epoch 96/100, step 1/62,  total step 5891/6200, training_loss = 0.2229\n",
      "epoch 96/100, step 2/62,  total step 5892/6200, training_loss = 0.2059\n",
      "epoch 96/100, step 3/62,  total step 5893/6200, training_loss = 0.2271\n",
      "epoch 96/100, step 4/62,  total step 5894/6200, training_loss = 0.3681\n",
      "epoch 96/100, step 5/62,  total step 5895/6200, training_loss = 0.1391\n",
      "epoch 96/100, step 6/62,  total step 5896/6200, training_loss = 0.2053\n",
      "epoch 96/100, step 7/62,  total step 5897/6200, training_loss = 0.1829\n",
      "epoch 96/100, step 8/62,  total step 5898/6200, training_loss = 0.2456\n",
      "epoch 96/100, step 9/62,  total step 5899/6200, training_loss = 0.1799\n",
      "epoch 96/100, step 10/62,  total step 5900/6200, training_loss = 0.1955\n",
      "epoch 96/100, step 11/62,  total step 5901/6200, training_loss = 0.1634\n",
      "epoch 96/100, step 12/62,  total step 5902/6200, training_loss = 0.1682\n",
      "epoch 96/100, step 13/62,  total step 5903/6200, training_loss = 0.1939\n",
      "epoch 96/100, step 14/62,  total step 5904/6200, training_loss = 0.2524\n",
      "epoch 96/100, step 15/62,  total step 5905/6200, training_loss = 0.3500\n",
      "epoch 96/100, step 16/62,  total step 5906/6200, training_loss = 0.3025\n",
      "epoch 96/100, step 17/62,  total step 5907/6200, training_loss = 0.1948\n",
      "epoch 96/100, step 18/62,  total step 5908/6200, training_loss = 0.2234\n",
      "epoch 96/100, step 19/62,  total step 5909/6200, training_loss = 0.2852\n",
      "epoch 96/100, step 20/62,  total step 5910/6200, training_loss = 0.2001\n",
      "epoch 96/100, step 21/62,  total step 5911/6200, training_loss = 0.2372\n",
      "epoch 96/100, step 22/62,  total step 5912/6200, training_loss = 0.3301\n",
      "epoch 96/100, step 23/62,  total step 5913/6200, training_loss = 0.2379\n",
      "epoch 96/100, step 24/62,  total step 5914/6200, training_loss = 0.1718\n",
      "epoch 96/100, step 25/62,  total step 5915/6200, training_loss = 0.2036\n",
      "epoch 96/100, step 26/62,  total step 5916/6200, training_loss = 0.1560\n",
      "epoch 96/100, step 27/62,  total step 5917/6200, training_loss = 0.2589\n",
      "epoch 96/100, step 28/62,  total step 5918/6200, training_loss = 0.3209\n",
      "epoch 96/100, step 29/62,  total step 5919/6200, training_loss = 0.2484\n",
      "epoch 96/100, step 30/62,  total step 5920/6200, training_loss = 0.2361\n",
      "epoch 96/100, step 31/62,  total step 5921/6200, training_loss = 0.1496\n",
      "epoch 96/100, step 32/62,  total step 5922/6200, training_loss = 0.1716\n",
      "epoch 96/100, step 33/62,  total step 5923/6200, training_loss = 0.2313\n",
      "epoch 96/100, step 34/62,  total step 5924/6200, training_loss = 0.1935\n",
      "epoch 96/100, step 35/62,  total step 5925/6200, training_loss = 0.1999\n",
      "epoch 96/100, step 36/62,  total step 5926/6200, training_loss = 0.2703\n",
      "epoch 96/100, step 37/62,  total step 5927/6200, training_loss = 0.1804\n",
      "epoch 96/100, step 38/62,  total step 5928/6200, training_loss = 0.2291\n",
      "epoch 96/100, step 39/62,  total step 5929/6200, training_loss = 0.2187\n",
      "epoch 96/100, step 40/62,  total step 5930/6200, training_loss = 0.2813\n",
      "epoch 96/100, step 41/62,  total step 5931/6200, training_loss = 0.2067\n",
      "epoch 96/100, step 42/62,  total step 5932/6200, training_loss = 0.2106\n",
      "epoch 96/100, step 43/62,  total step 5933/6200, training_loss = 0.1953\n",
      "epoch 96/100, step 44/62,  total step 5934/6200, training_loss = 0.2273\n",
      "epoch 96/100, step 45/62,  total step 5935/6200, training_loss = 0.1537\n",
      "epoch 96/100, step 46/62,  total step 5936/6200, training_loss = 0.1711\n",
      "epoch 96/100, step 47/62,  total step 5937/6200, training_loss = 0.2818\n",
      "epoch 96/100, step 48/62,  total step 5938/6200, training_loss = 0.1403\n",
      "epoch 96/100, step 49/62,  total step 5939/6200, training_loss = 0.1646\n",
      "epoch 96/100, step 50/62,  total step 5940/6200, training_loss = 0.2077\n",
      "epoch 96/100, step 51/62,  total step 5941/6200, training_loss = 0.1886\n",
      "epoch 96/100, step 52/62,  total step 5942/6200, training_loss = 0.1402\n",
      "epoch 96/100, step 53/62,  total step 5943/6200, training_loss = 0.2434\n",
      "epoch 96/100, step 54/62,  total step 5944/6200, training_loss = 0.1432\n",
      "epoch 96/100, step 55/62,  total step 5945/6200, training_loss = 0.1570\n",
      "epoch 96/100, step 56/62,  total step 5946/6200, training_loss = 0.2393\n",
      "epoch 96/100, step 57/62,  total step 5947/6200, training_loss = 0.2214\n",
      "epoch 96/100, step 58/62,  total step 5948/6200, training_loss = 0.3057\n",
      "epoch 96/100, step 59/62,  total step 5949/6200, training_loss = 0.1699\n",
      "epoch 96/100, step 60/62,  total step 5950/6200, training_loss = 0.2805\n",
      "epoch 96/100, step 61/62,  total step 5951/6200, training_loss = 0.1393\n",
      "epoch 96/100, step 62/62,  total step 5952/6200, training_loss = 0.1980 | avg loss: 0.2164 Dice Metric:    0.64\n",
      "epoch 97/100, step 1/62,  total step 5953/6200, training_loss = 0.1910\n",
      "epoch 97/100, step 2/62,  total step 5954/6200, training_loss = 0.1934\n",
      "epoch 97/100, step 3/62,  total step 5955/6200, training_loss = 0.2580\n",
      "epoch 97/100, step 4/62,  total step 5956/6200, training_loss = 0.3615\n",
      "epoch 97/100, step 5/62,  total step 5957/6200, training_loss = 0.1396\n",
      "epoch 97/100, step 6/62,  total step 5958/6200, training_loss = 0.1953\n",
      "epoch 97/100, step 7/62,  total step 5959/6200, training_loss = 0.1906\n",
      "epoch 97/100, step 8/62,  total step 5960/6200, training_loss = 0.2099\n",
      "epoch 97/100, step 9/62,  total step 5961/6200, training_loss = 0.1806\n",
      "epoch 97/100, step 10/62,  total step 5962/6200, training_loss = 0.2375\n",
      "epoch 97/100, step 11/62,  total step 5963/6200, training_loss = 0.1703\n",
      "epoch 97/100, step 12/62,  total step 5964/6200, training_loss = 0.2113\n",
      "epoch 97/100, step 13/62,  total step 5965/6200, training_loss = 0.2128\n",
      "epoch 97/100, step 14/62,  total step 5966/6200, training_loss = 0.2548\n",
      "epoch 97/100, step 15/62,  total step 5967/6200, training_loss = 0.3470\n",
      "epoch 97/100, step 16/62,  total step 5968/6200, training_loss = 0.2806\n",
      "epoch 97/100, step 17/62,  total step 5969/6200, training_loss = 0.1937\n",
      "epoch 97/100, step 18/62,  total step 5970/6200, training_loss = 0.1897\n",
      "epoch 97/100, step 19/62,  total step 5971/6200, training_loss = 0.2675\n",
      "epoch 97/100, step 20/62,  total step 5972/6200, training_loss = 0.2187\n",
      "epoch 97/100, step 21/62,  total step 5973/6200, training_loss = 0.2614\n",
      "epoch 97/100, step 22/62,  total step 5974/6200, training_loss = 0.3782\n",
      "epoch 97/100, step 23/62,  total step 5975/6200, training_loss = 0.2385\n",
      "epoch 97/100, step 24/62,  total step 5976/6200, training_loss = 0.1619\n",
      "epoch 97/100, step 25/62,  total step 5977/6200, training_loss = 0.2035\n",
      "epoch 97/100, step 26/62,  total step 5978/6200, training_loss = 0.1790\n",
      "epoch 97/100, step 27/62,  total step 5979/6200, training_loss = 0.2460\n",
      "epoch 97/100, step 28/62,  total step 5980/6200, training_loss = 0.2711\n",
      "epoch 97/100, step 29/62,  total step 5981/6200, training_loss = 0.2481\n",
      "epoch 97/100, step 30/62,  total step 5982/6200, training_loss = 0.2488\n",
      "epoch 97/100, step 31/62,  total step 5983/6200, training_loss = 0.1427\n",
      "epoch 97/100, step 32/62,  total step 5984/6200, training_loss = 0.1820\n",
      "epoch 97/100, step 33/62,  total step 5985/6200, training_loss = 0.2314\n",
      "epoch 97/100, step 34/62,  total step 5986/6200, training_loss = 0.1938\n",
      "epoch 97/100, step 35/62,  total step 5987/6200, training_loss = 0.1965\n",
      "epoch 97/100, step 36/62,  total step 5988/6200, training_loss = 0.2231\n",
      "epoch 97/100, step 37/62,  total step 5989/6200, training_loss = 0.2141\n",
      "epoch 97/100, step 38/62,  total step 5990/6200, training_loss = 0.2179\n",
      "epoch 97/100, step 39/62,  total step 5991/6200, training_loss = 0.2266\n",
      "epoch 97/100, step 40/62,  total step 5992/6200, training_loss = 0.2432\n",
      "epoch 97/100, step 41/62,  total step 5993/6200, training_loss = 0.1735\n",
      "epoch 97/100, step 42/62,  total step 5994/6200, training_loss = 0.2152\n",
      "epoch 97/100, step 43/62,  total step 5995/6200, training_loss = 0.2001\n",
      "epoch 97/100, step 44/62,  total step 5996/6200, training_loss = 0.1960\n",
      "epoch 97/100, step 45/62,  total step 5997/6200, training_loss = 0.1264\n",
      "epoch 97/100, step 46/62,  total step 5998/6200, training_loss = 0.1665\n",
      "epoch 97/100, step 47/62,  total step 5999/6200, training_loss = 0.2538\n",
      "epoch 97/100, step 48/62,  total step 6000/6200, training_loss = 0.1477\n",
      "epoch 97/100, step 49/62,  total step 6001/6200, training_loss = 0.1477\n",
      "epoch 97/100, step 50/62,  total step 6002/6200, training_loss = 0.2193\n",
      "epoch 97/100, step 51/62,  total step 6003/6200, training_loss = 0.2029\n",
      "epoch 97/100, step 52/62,  total step 6004/6200, training_loss = 0.1706\n",
      "epoch 97/100, step 53/62,  total step 6005/6200, training_loss = 0.2216\n",
      "epoch 97/100, step 54/62,  total step 6006/6200, training_loss = 0.1535\n",
      "epoch 97/100, step 55/62,  total step 6007/6200, training_loss = 0.1805\n",
      "epoch 97/100, step 56/62,  total step 6008/6200, training_loss = 0.2067\n",
      "epoch 97/100, step 57/62,  total step 6009/6200, training_loss = 0.2273\n",
      "epoch 97/100, step 58/62,  total step 6010/6200, training_loss = 0.3610\n",
      "epoch 97/100, step 59/62,  total step 6011/6200, training_loss = 0.1897\n",
      "epoch 97/100, step 60/62,  total step 6012/6200, training_loss = 0.2466\n",
      "epoch 97/100, step 61/62,  total step 6013/6200, training_loss = 0.1387\n",
      "epoch 97/100, step 62/62,  total step 6014/6200, training_loss = 0.2073 | avg loss: 0.2155 Dice Metric:   0.668\n",
      "epoch 98/100, step 1/62,  total step 6015/6200, training_loss = 0.1908\n",
      "epoch 98/100, step 2/62,  total step 6016/6200, training_loss = 0.1927\n",
      "epoch 98/100, step 3/62,  total step 6017/6200, training_loss = 0.2450\n",
      "epoch 98/100, step 4/62,  total step 6018/6200, training_loss = 0.3322\n",
      "epoch 98/100, step 5/62,  total step 6019/6200, training_loss = 0.1353\n",
      "epoch 98/100, step 6/62,  total step 6020/6200, training_loss = 0.1967\n",
      "epoch 98/100, step 7/62,  total step 6021/6200, training_loss = 0.1805\n",
      "epoch 98/100, step 8/62,  total step 6022/6200, training_loss = 0.2287\n",
      "epoch 98/100, step 9/62,  total step 6023/6200, training_loss = 0.1656\n",
      "epoch 98/100, step 10/62,  total step 6024/6200, training_loss = 0.1819\n",
      "epoch 98/100, step 11/62,  total step 6025/6200, training_loss = 0.1604\n",
      "epoch 98/100, step 12/62,  total step 6026/6200, training_loss = 0.1559\n",
      "epoch 98/100, step 13/62,  total step 6027/6200, training_loss = 0.2219\n",
      "epoch 98/100, step 14/62,  total step 6028/6200, training_loss = 0.2339\n",
      "epoch 98/100, step 15/62,  total step 6029/6200, training_loss = 0.3284\n",
      "epoch 98/100, step 16/62,  total step 6030/6200, training_loss = 0.2962\n",
      "epoch 98/100, step 17/62,  total step 6031/6200, training_loss = 0.1814\n",
      "epoch 98/100, step 18/62,  total step 6032/6200, training_loss = 0.1847\n",
      "epoch 98/100, step 19/62,  total step 6033/6200, training_loss = 0.2936\n",
      "epoch 98/100, step 20/62,  total step 6034/6200, training_loss = 0.1927\n",
      "epoch 98/100, step 21/62,  total step 6035/6200, training_loss = 0.2386\n",
      "epoch 98/100, step 22/62,  total step 6036/6200, training_loss = 0.3328\n",
      "epoch 98/100, step 23/62,  total step 6037/6200, training_loss = 0.2138\n",
      "epoch 98/100, step 24/62,  total step 6038/6200, training_loss = 0.1853\n",
      "epoch 98/100, step 25/62,  total step 6039/6200, training_loss = 0.2102\n",
      "epoch 98/100, step 26/62,  total step 6040/6200, training_loss = 0.1753\n",
      "epoch 98/100, step 27/62,  total step 6041/6200, training_loss = 0.2128\n",
      "epoch 98/100, step 28/62,  total step 6042/6200, training_loss = 0.3095\n",
      "epoch 98/100, step 29/62,  total step 6043/6200, training_loss = 0.2989\n",
      "epoch 98/100, step 30/62,  total step 6044/6200, training_loss = 0.2584\n",
      "epoch 98/100, step 31/62,  total step 6045/6200, training_loss = 0.1635\n",
      "epoch 98/100, step 32/62,  total step 6046/6200, training_loss = 0.2142\n",
      "epoch 98/100, step 33/62,  total step 6047/6200, training_loss = 0.2812\n",
      "epoch 98/100, step 34/62,  total step 6048/6200, training_loss = 0.1987\n",
      "epoch 98/100, step 35/62,  total step 6049/6200, training_loss = 0.2068\n",
      "epoch 98/100, step 36/62,  total step 6050/6200, training_loss = 0.2586\n",
      "epoch 98/100, step 37/62,  total step 6051/6200, training_loss = 0.2220\n",
      "epoch 98/100, step 38/62,  total step 6052/6200, training_loss = 0.2339\n",
      "epoch 98/100, step 39/62,  total step 6053/6200, training_loss = 0.2297\n",
      "epoch 98/100, step 40/62,  total step 6054/6200, training_loss = 0.2581\n",
      "epoch 98/100, step 41/62,  total step 6055/6200, training_loss = 0.1951\n",
      "epoch 98/100, step 42/62,  total step 6056/6200, training_loss = 0.1989\n",
      "epoch 98/100, step 43/62,  total step 6057/6200, training_loss = 0.2014\n",
      "epoch 98/100, step 44/62,  total step 6058/6200, training_loss = 0.2002\n",
      "epoch 98/100, step 45/62,  total step 6059/6200, training_loss = 0.1411\n",
      "epoch 98/100, step 46/62,  total step 6060/6200, training_loss = 0.1866\n",
      "epoch 98/100, step 47/62,  total step 6061/6200, training_loss = 0.2836\n",
      "epoch 98/100, step 48/62,  total step 6062/6200, training_loss = 0.1539\n",
      "epoch 98/100, step 49/62,  total step 6063/6200, training_loss = 0.1614\n",
      "epoch 98/100, step 50/62,  total step 6064/6200, training_loss = 0.1887\n",
      "epoch 98/100, step 51/62,  total step 6065/6200, training_loss = 0.2234\n",
      "epoch 98/100, step 52/62,  total step 6066/6200, training_loss = 0.1740\n",
      "epoch 98/100, step 53/62,  total step 6067/6200, training_loss = 0.2004\n",
      "epoch 98/100, step 54/62,  total step 6068/6200, training_loss = 0.1436\n",
      "epoch 98/100, step 55/62,  total step 6069/6200, training_loss = 0.2031\n",
      "epoch 98/100, step 56/62,  total step 6070/6200, training_loss = 0.2318\n",
      "epoch 98/100, step 57/62,  total step 6071/6200, training_loss = 0.2257\n",
      "epoch 98/100, step 58/62,  total step 6072/6200, training_loss = 0.3446\n",
      "epoch 98/100, step 59/62,  total step 6073/6200, training_loss = 0.1653\n",
      "epoch 98/100, step 60/62,  total step 6074/6200, training_loss = 0.2719\n",
      "epoch 98/100, step 61/62,  total step 6075/6200, training_loss = 0.1313\n",
      "epoch 98/100, step 62/62,  total step 6076/6200, training_loss = 0.1969 | avg loss: 0.2165 Dice Metric:   0.661\n",
      "epoch 99/100, step 1/62,  total step 6077/6200, training_loss = 0.2030\n",
      "epoch 99/100, step 2/62,  total step 6078/6200, training_loss = 0.1874\n",
      "epoch 99/100, step 3/62,  total step 6079/6200, training_loss = 0.2368\n",
      "epoch 99/100, step 4/62,  total step 6080/6200, training_loss = 0.3356\n",
      "epoch 99/100, step 5/62,  total step 6081/6200, training_loss = 0.1633\n",
      "epoch 99/100, step 6/62,  total step 6082/6200, training_loss = 0.1787\n",
      "epoch 99/100, step 7/62,  total step 6083/6200, training_loss = 0.1863\n",
      "epoch 99/100, step 8/62,  total step 6084/6200, training_loss = 0.2415\n",
      "epoch 99/100, step 9/62,  total step 6085/6200, training_loss = 0.1819\n",
      "epoch 99/100, step 10/62,  total step 6086/6200, training_loss = 0.2020\n",
      "epoch 99/100, step 11/62,  total step 6087/6200, training_loss = 0.1765\n",
      "epoch 99/100, step 12/62,  total step 6088/6200, training_loss = 0.1527\n",
      "epoch 99/100, step 13/62,  total step 6089/6200, training_loss = 0.1842\n",
      "epoch 99/100, step 14/62,  total step 6090/6200, training_loss = 0.2391\n",
      "epoch 99/100, step 15/62,  total step 6091/6200, training_loss = 0.3284\n",
      "epoch 99/100, step 16/62,  total step 6092/6200, training_loss = 0.3131\n",
      "epoch 99/100, step 17/62,  total step 6093/6200, training_loss = 0.2034\n",
      "epoch 99/100, step 18/62,  total step 6094/6200, training_loss = 0.1770\n",
      "epoch 99/100, step 19/62,  total step 6095/6200, training_loss = 0.2635\n",
      "epoch 99/100, step 20/62,  total step 6096/6200, training_loss = 0.2050\n",
      "epoch 99/100, step 21/62,  total step 6097/6200, training_loss = 0.2437\n",
      "epoch 99/100, step 22/62,  total step 6098/6200, training_loss = 0.3823\n",
      "epoch 99/100, step 23/62,  total step 6099/6200, training_loss = 0.2421\n",
      "epoch 99/100, step 24/62,  total step 6100/6200, training_loss = 0.1820\n",
      "epoch 99/100, step 25/62,  total step 6101/6200, training_loss = 0.2030\n",
      "epoch 99/100, step 26/62,  total step 6102/6200, training_loss = 0.1654\n",
      "epoch 99/100, step 27/62,  total step 6103/6200, training_loss = 0.2576\n",
      "epoch 99/100, step 28/62,  total step 6104/6200, training_loss = 0.3568\n",
      "epoch 99/100, step 29/62,  total step 6105/6200, training_loss = 0.2357\n",
      "epoch 99/100, step 30/62,  total step 6106/6200, training_loss = 0.2509\n",
      "epoch 99/100, step 31/62,  total step 6107/6200, training_loss = 0.1475\n",
      "epoch 99/100, step 32/62,  total step 6108/6200, training_loss = 0.1960\n",
      "epoch 99/100, step 33/62,  total step 6109/6200, training_loss = 0.2597\n",
      "epoch 99/100, step 34/62,  total step 6110/6200, training_loss = 0.1913\n",
      "epoch 99/100, step 35/62,  total step 6111/6200, training_loss = 0.2013\n",
      "epoch 99/100, step 36/62,  total step 6112/6200, training_loss = 0.2517\n",
      "epoch 99/100, step 37/62,  total step 6113/6200, training_loss = 0.2168\n",
      "epoch 99/100, step 38/62,  total step 6114/6200, training_loss = 0.2094\n",
      "epoch 99/100, step 39/62,  total step 6115/6200, training_loss = 0.2722\n",
      "epoch 99/100, step 40/62,  total step 6116/6200, training_loss = 0.2467\n",
      "epoch 99/100, step 41/62,  total step 6117/6200, training_loss = 0.1772\n",
      "epoch 99/100, step 42/62,  total step 6118/6200, training_loss = 0.2062\n",
      "epoch 99/100, step 43/62,  total step 6119/6200, training_loss = 0.1893\n",
      "epoch 99/100, step 44/62,  total step 6120/6200, training_loss = 0.2155\n",
      "epoch 99/100, step 45/62,  total step 6121/6200, training_loss = 0.1241\n",
      "epoch 99/100, step 46/62,  total step 6122/6200, training_loss = 0.1750\n",
      "epoch 99/100, step 47/62,  total step 6123/6200, training_loss = 0.2999\n",
      "epoch 99/100, step 48/62,  total step 6124/6200, training_loss = 0.1475\n",
      "epoch 99/100, step 49/62,  total step 6125/6200, training_loss = 0.1545\n",
      "epoch 99/100, step 50/62,  total step 6126/6200, training_loss = 0.1918\n",
      "epoch 99/100, step 51/62,  total step 6127/6200, training_loss = 0.2139\n",
      "epoch 99/100, step 52/62,  total step 6128/6200, training_loss = 0.1750\n",
      "epoch 99/100, step 53/62,  total step 6129/6200, training_loss = 0.2299\n",
      "epoch 99/100, step 54/62,  total step 6130/6200, training_loss = 0.1395\n",
      "epoch 99/100, step 55/62,  total step 6131/6200, training_loss = 0.1631\n",
      "epoch 99/100, step 56/62,  total step 6132/6200, training_loss = 0.2431\n",
      "epoch 99/100, step 57/62,  total step 6133/6200, training_loss = 0.2676\n",
      "epoch 99/100, step 58/62,  total step 6134/6200, training_loss = 0.3301\n",
      "epoch 99/100, step 59/62,  total step 6135/6200, training_loss = 0.2044\n",
      "epoch 99/100, step 60/62,  total step 6136/6200, training_loss = 0.2644\n",
      "epoch 99/100, step 61/62,  total step 6137/6200, training_loss = 0.1373\n",
      "epoch 99/100, step 62/62,  total step 6138/6200, training_loss = 0.1975 | avg loss: 0.2180 Dice Metric:    0.66\n",
      "epoch 100/100, step 1/62,  total step 6139/6200, training_loss = 0.2181\n",
      "epoch 100/100, step 2/62,  total step 6140/6200, training_loss = 0.1803\n",
      "epoch 100/100, step 3/62,  total step 6141/6200, training_loss = 0.2353\n",
      "epoch 100/100, step 4/62,  total step 6142/6200, training_loss = 0.3556\n",
      "epoch 100/100, step 5/62,  total step 6143/6200, training_loss = 0.1320\n",
      "epoch 100/100, step 6/62,  total step 6144/6200, training_loss = 0.1950\n",
      "epoch 100/100, step 7/62,  total step 6145/6200, training_loss = 0.1848\n",
      "epoch 100/100, step 8/62,  total step 6146/6200, training_loss = 0.2359\n",
      "epoch 100/100, step 9/62,  total step 6147/6200, training_loss = 0.1450\n",
      "epoch 100/100, step 10/62,  total step 6148/6200, training_loss = 0.2056\n",
      "epoch 100/100, step 11/62,  total step 6149/6200, training_loss = 0.1676\n",
      "epoch 100/100, step 12/62,  total step 6150/6200, training_loss = 0.1597\n",
      "epoch 100/100, step 13/62,  total step 6151/6200, training_loss = 0.2108\n",
      "epoch 100/100, step 14/62,  total step 6152/6200, training_loss = 0.2492\n",
      "epoch 100/100, step 15/62,  total step 6153/6200, training_loss = 0.3441\n",
      "epoch 100/100, step 16/62,  total step 6154/6200, training_loss = 0.2838\n",
      "epoch 100/100, step 17/62,  total step 6155/6200, training_loss = 0.1757\n",
      "epoch 100/100, step 18/62,  total step 6156/6200, training_loss = 0.1765\n",
      "epoch 100/100, step 19/62,  total step 6157/6200, training_loss = 0.2445\n",
      "epoch 100/100, step 20/62,  total step 6158/6200, training_loss = 0.1950\n",
      "epoch 100/100, step 21/62,  total step 6159/6200, training_loss = 0.2586\n",
      "epoch 100/100, step 22/62,  total step 6160/6200, training_loss = 0.3861\n",
      "epoch 100/100, step 23/62,  total step 6161/6200, training_loss = 0.2283\n",
      "epoch 100/100, step 24/62,  total step 6162/6200, training_loss = 0.1763\n",
      "epoch 100/100, step 25/62,  total step 6163/6200, training_loss = 0.1935\n",
      "epoch 100/100, step 26/62,  total step 6164/6200, training_loss = 0.1721\n",
      "epoch 100/100, step 27/62,  total step 6165/6200, training_loss = 0.2053\n",
      "epoch 100/100, step 28/62,  total step 6166/6200, training_loss = 0.2798\n",
      "epoch 100/100, step 29/62,  total step 6167/6200, training_loss = 0.2495\n",
      "epoch 100/100, step 30/62,  total step 6168/6200, training_loss = 0.2496\n",
      "epoch 100/100, step 31/62,  total step 6169/6200, training_loss = 0.1518\n",
      "epoch 100/100, step 32/62,  total step 6170/6200, training_loss = 0.2262\n",
      "epoch 100/100, step 33/62,  total step 6171/6200, training_loss = 0.2768\n",
      "epoch 100/100, step 34/62,  total step 6172/6200, training_loss = 0.1854\n",
      "epoch 100/100, step 35/62,  total step 6173/6200, training_loss = 0.2041\n",
      "epoch 100/100, step 36/62,  total step 6174/6200, training_loss = 0.2611\n",
      "epoch 100/100, step 37/62,  total step 6175/6200, training_loss = 0.1911\n",
      "epoch 100/100, step 38/62,  total step 6176/6200, training_loss = 0.2028\n",
      "epoch 100/100, step 39/62,  total step 6177/6200, training_loss = 0.2226\n",
      "epoch 100/100, step 40/62,  total step 6178/6200, training_loss = 0.2572\n",
      "epoch 100/100, step 41/62,  total step 6179/6200, training_loss = 0.1746\n",
      "epoch 100/100, step 42/62,  total step 6180/6200, training_loss = 0.2087\n",
      "epoch 100/100, step 43/62,  total step 6181/6200, training_loss = 0.1959\n",
      "epoch 100/100, step 44/62,  total step 6182/6200, training_loss = 0.1991\n",
      "epoch 100/100, step 45/62,  total step 6183/6200, training_loss = 0.1442\n",
      "epoch 100/100, step 46/62,  total step 6184/6200, training_loss = 0.1755\n",
      "epoch 100/100, step 47/62,  total step 6185/6200, training_loss = 0.2477\n",
      "epoch 100/100, step 48/62,  total step 6186/6200, training_loss = 0.1827\n",
      "epoch 100/100, step 49/62,  total step 6187/6200, training_loss = 0.1475\n",
      "epoch 100/100, step 50/62,  total step 6188/6200, training_loss = 0.2473\n",
      "epoch 100/100, step 51/62,  total step 6189/6200, training_loss = 0.2222\n",
      "epoch 100/100, step 52/62,  total step 6190/6200, training_loss = 0.1845\n",
      "epoch 100/100, step 53/62,  total step 6191/6200, training_loss = 0.2064\n",
      "epoch 100/100, step 54/62,  total step 6192/6200, training_loss = 0.1601\n",
      "epoch 100/100, step 55/62,  total step 6193/6200, training_loss = 0.1659\n",
      "epoch 100/100, step 56/62,  total step 6194/6200, training_loss = 0.2092\n",
      "epoch 100/100, step 57/62,  total step 6195/6200, training_loss = 0.2190\n",
      "epoch 100/100, step 58/62,  total step 6196/6200, training_loss = 0.2860\n",
      "epoch 100/100, step 59/62,  total step 6197/6200, training_loss = 0.1495\n",
      "epoch 100/100, step 60/62,  total step 6198/6200, training_loss = 0.2816\n",
      "epoch 100/100, step 61/62,  total step 6199/6200, training_loss = 0.1440\n",
      "epoch 100/100, step 62/62,  total step 6200/6200, training_loss = 0.1440 | avg loss: 0.2124 Dice Metric:   0.662"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    " \n",
    "ax[0].plot(epoch_loss_values)\n",
    "ax[0].set_title(\"epoch Loss\")\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].plot(metric_values)\n",
    "ax[1].set_title(\"epoch matric\")\n",
    "ax[1].set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat trials \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02 ](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04 Next ](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "moani_bootcamp_2_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
